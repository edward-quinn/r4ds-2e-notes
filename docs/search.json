[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R For Data Science, 2nd Edition Notebook",
    "section": "",
    "text": "R for Data Science, 2nd edition (Wickham, Çetinkaya-Rundel, and Grolemund 2023) can be found here.\n\nChapter 5 Notes\nChapter 9 Notes\nChapter 10 Notes\nChapter 11 Notes\nChapter 12 Notes\nChapter 17 Notes\nChapter 25 Notes\nChapter 26 Notes\n\n\n\n\n\nReferences\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. 2nd ed. O’Reilly Media."
  },
  {
    "objectID": "chapter_25_notes.html",
    "href": "chapter_25_notes.html",
    "title": "R for Data Science, 2nd Edition - Chapter 25 Notes",
    "section": "",
    "text": "Why use functions?\n\nYou can give a function an evocative name that makes your code easier to understand.\nAs requirements change, you only need to update code in one place, instead of many.\nYou eliminate the chance of making incidental mistakes when you copy and paste (i.e. updating a variable name in one place, but not in another).\nIt makes it easier to reuse work from project-to-project, increasing your productivity over time.\n\nUse functions when you find yourself repeating your code more than once. Here are the functions we’ll learn about:\n\nVector functions take one or more vectors as input and return a vector as output.\nData frame functions take a data frame as input and return a data frame as output.\nPlot functions that take a data frame as input and return a plot as output.\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.6\n✔ forcats   1.0.1     ✔ stringr   1.6.0\n✔ ggplot2   4.0.1     ✔ tibble    3.3.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.2\n✔ purrr     1.2.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(nycflights13)\n\n\n\nHere is an example of a rescaling function.\n\ndf &lt;- tibble(\n  a = rnorm(5),\n  b = rnorm(5),\n  c = rnorm(5),\n  d = rnorm(5),\n)\n\ndf |&gt; mutate(\n  a = (a - min(a, na.rm = TRUE)) / \n    (max(a, na.rm = TRUE) - min(a, na.rm = TRUE)),\n  b = (b - min(a, na.rm = TRUE)) / \n    (max(b, na.rm = TRUE) - min(b, na.rm = TRUE)),\n  c = (c - min(c, na.rm = TRUE)) / \n    (max(c, na.rm = TRUE) - min(c, na.rm = TRUE)),\n  d = (d - min(d, na.rm = TRUE)) / \n    (max(d, na.rm = TRUE) - min(d, na.rm = TRUE)),\n)\n\n# A tibble: 5 × 4\n       a       b     c     d\n   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 1      -0.361  0.859 0.831\n2 0.0980 -0.0264 1     1    \n3 0.765   0.639  0     0    \n4 0       0.181  0.374 0.131\n5 0.529   0.262  0.710 0.589\n\n#&gt; # A tibble: 5 × 4\n#&gt;       a       b     c     d\n#&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 0.339  0.387  0.291 0    \n#&gt; 2 0.880 -0.613  0.611 0.557\n#&gt; 3 0     -0.0833 1     0.752\n#&gt; 4 0.795 -0.0822 0     1    \n#&gt; 5 1     -0.0952 0.580 0.394\n\nHere is the function:\n\nrescale01 &lt;- function(x) {\n  (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))\n}\n\nNow call the function with mutate for cleaner code:\n\ndf |&gt; mutate(\n  a = rescale01(a),\n  b = rescale01(b),\n  c = rescale01(c),\n  d = rescale01(d),\n)\n\n# A tibble: 5 × 4\n       a     b     c     d\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 1      0     0.859 0.831\n2 0.0980 0.335 1     1    \n3 0.765  1     0     0    \n4 0      0.543 0.374 0.131\n5 0.529  0.624 0.710 0.589\n\n#&gt; # A tibble: 5 × 4\n#&gt;       a     b     c     d\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 0.339 1     0.291 0    \n#&gt; 2 0.880 0     0.611 0.557\n#&gt; 3 0     0.530 1     0.752\n#&gt; 4 0.795 0.531 0     1    \n#&gt; 5 1     0.518 0.580 0.394\n\nVector functions work well inside of mutate() and filter() return a vector that is the same length as the input vector.\n\nz_score &lt;- function(x) {\n  (x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)\n}\n\n\nclamp &lt;- function(x, min, max) {\n  case_when(\n    x &lt; min ~ min,\n    x &gt; max ~ max,\n    .default = x\n  )\n}\n\nclamp(1:10, min = 3, max = 7)\n\n [1] 3 3 3 4 5 6 7 7 7 7\n\n#&gt;  [1] 3 3 3 4 5 6 7 7 7 7\n\n\nfirst_upper &lt;- function(x) {\n  str_sub(x, 1, 1) &lt;- str_to_upper(str_sub(x, 1, 1))\n  x\n}\n\nfirst_upper(\"hello\")\n\n[1] \"Hello\"\n\n#&gt; [1] \"Hello\"\n\n\n# https://twitter.com/NVlabormarket/status/1571939851922198530\nclean_number &lt;- function(x) {\n  is_pct &lt;- str_detect(x, \"%\")\n  num &lt;- x |&gt; \n    str_remove_all(\"%\") |&gt; \n    str_remove_all(\",\") |&gt; \n    str_remove_all(fixed(\"$\")) |&gt; \n    as.numeric()\n  if_else(is_pct, num / 100, num)\n}\n\nclean_number(\"$12,300\")\n\n[1] 12300\n\n#&gt; [1] 12300\nclean_number(\"45%\")\n\n[1] 0.45\n\n#&gt; [1] 0.45\n\n\nfix_na &lt;- function(x) {\n  if_else(x %in% c(997, 998, 999), NA, x)\n}\n\nSummary functions are another important family of functions that return a single value for use in summarize().\n\ncommas &lt;- function(x) {\n  str_flatten(x, collapse = \", \", last = \" and \")\n}\n\ncommas(c(\"cat\", \"dog\", \"pigeon\"))\n\n[1] \"cat, dog and pigeon\"\n\n#&gt; [1] \"cat, dog and pigeon\"\n\n\ncv &lt;- function(x, na.rm = FALSE) {\n  sd(x, na.rm = na.rm) / mean(x, na.rm = na.rm)\n}\n\ncv(runif(100, min = 0, max = 50))\n\n[1] 0.639585\n\n#&gt; [1] 0.5196276\ncv(runif(100, min = 0, max = 500))\n\n[1] 0.5552002\n\n#&gt; [1] 0.5652554\n\n\n# https://twitter.com/gbganalyst/status/1571619641390252033\nn_missing &lt;- function(x) {\n  sum(is.na(x))\n} \n\n\n# mean absolute percentage error\n\n\n# https://twitter.com/neilgcurrie/status/1571607727255834625\nmape &lt;- function(actual, predicted) {\n  sum(abs((actual - predicted) / actual)) / length(actual)\n}\n\n\n\n\nIf copying multiple verbs multiple times, you probably need a data frame function.\nYou will need to deal with the problem of indirection, resulting from tidy evaluation. The solution is to us embracing with { }.\n\ngrouped_mean &lt;- function(df, group_var, mean_var) {\n  df |&gt; \n    group_by(group_var) |&gt; \n    summarize(mean(mean_var))\n}\n\nThe following will error out:\n\n# diamonds |&gt; grouped_mean(cut, carat)\n# #&gt; Error in `group_by()`:\n# #&gt; ! Must group by variables found in `.data`.\n# #&gt; ✖ Column `group_var` is not found.\n\nAn illustration of the problem:\n\ndf &lt;- tibble(\n  mean_var = 1,\n  group_var = \"g\",\n  group = 1,\n  x = 10,\n  y = 100\n)\n\ndf |&gt; grouped_mean(group, x)\n\n# A tibble: 1 × 2\n  group_var `mean(mean_var)`\n  &lt;chr&gt;                &lt;dbl&gt;\n1 g                        1\n\n#&gt; # A tibble: 1 × 2\n#&gt;   group_var `mean(mean_var)`\n#&gt;   &lt;chr&gt;                &lt;dbl&gt;\n#&gt; 1 g                        1\ndf |&gt; grouped_mean(group, y)\n\n# A tibble: 1 × 2\n  group_var `mean(mean_var)`\n  &lt;chr&gt;                &lt;dbl&gt;\n1 g                        1\n\n#&gt; # A tibble: 1 × 2\n#&gt;   group_var `mean(mean_var)`\n#&gt;   &lt;chr&gt;                &lt;dbl&gt;\n#&gt; 1 g                        1\n\nWe are returning values, but it is the “mean” of a single value, which is being returned as 1.\nWe need to tell dplyr to run verbs on the values inside of the variable, and not to look for a variable with the argument name. We don’t actually have a variable called mean_var or group_var.\nUse { } to solve this problem.\n\ngrouped_mean &lt;- function(df, group_var, mean_var) {\n  df |&gt; \n    group_by({{ group_var }}) |&gt; \n    summarize(mean({{ mean_var }}))\n}\n\ndf |&gt; grouped_mean(group, x)\n\n# A tibble: 1 × 2\n  group `mean(x)`\n  &lt;dbl&gt;     &lt;dbl&gt;\n1     1        10\n\n#&gt; # A tibble: 1 × 2\n#&gt;   group `mean(x)`\n#&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1     1        10\n\nWe need to embrace when Data-masking (arrange(), filter(), summarize()) or Tidy-selection (select(), relocate(), rename()) occur.\n\nsummary6 &lt;- function(data, var) {\n  data |&gt; summarize(\n    min = min({{ var }}, na.rm = TRUE),\n    mean = mean({{ var }}, na.rm = TRUE),\n    median = median({{ var }}, na.rm = TRUE),\n    max = max({{ var }}, na.rm = TRUE),\n    n = n(),\n    n_miss = sum(is.na({{ var }})),\n    .groups = \"drop\"\n  )\n}\n\ndiamonds |&gt; summary6(carat)\n\n# A tibble: 1 × 6\n    min  mean median   max     n n_miss\n  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;  &lt;int&gt;\n1   0.2 0.798    0.7  5.01 53940      0\n\n#&gt; # A tibble: 1 × 6\n#&gt;     min  mean median   max     n n_miss\n#&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;  &lt;int&gt;\n#&gt; 1   0.2 0.798    0.7  5.01 53940      0\n\n(Whenever you wrap summarize() in a helper, we think it’s good practice to set .groups = “drop” to both avoid the message and leave the data in an ungrouped state.)\nIf you wrap summarize() within a function, then it can be used on grouped data:\n\ndiamonds |&gt; \n  group_by(cut) |&gt; \n  summary6(carat)\n\n# A tibble: 5 × 7\n  cut         min  mean median   max     n n_miss\n  &lt;ord&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;  &lt;int&gt;\n1 Fair       0.22 1.05    1     5.01  1610      0\n2 Good       0.23 0.849   0.82  3.01  4906      0\n3 Very Good  0.2  0.806   0.71  4    12082      0\n4 Premium    0.2  0.892   0.86  4.01 13791      0\n5 Ideal      0.2  0.703   0.54  3.5  21551      0\n\n#&gt; # A tibble: 5 × 7\n#&gt;   cut         min  mean median   max     n n_miss\n#&gt;   &lt;ord&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;  &lt;int&gt;\n#&gt; 1 Fair       0.22 1.05    1     5.01  1610      0\n#&gt; 2 Good       0.23 0.849   0.82  3.01  4906      0\n#&gt; 3 Very Good  0.2  0.806   0.71  4    12082      0\n#&gt; 4 Premium    0.2  0.892   0.86  4.01 13791      0\n#&gt; 5 Ideal      0.2  0.703   0.54  3.5  21551      0\n\nFurthermore, since the arguments to summarize are data-masking, so is the var argument to summary6(). That means you can also summarize computed variables:\n\ndiamonds |&gt; \n  group_by(cut) |&gt; \n  summary6(log10(carat))\n\n# A tibble: 5 × 7\n  cut          min    mean  median   max     n n_miss\n  &lt;ord&gt;      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;  &lt;int&gt;\n1 Fair      -0.658 -0.0273  0      0.700  1610      0\n2 Good      -0.638 -0.133  -0.0862 0.479  4906      0\n3 Very Good -0.699 -0.164  -0.149  0.602 12082      0\n4 Premium   -0.699 -0.125  -0.0655 0.603 13791      0\n5 Ideal     -0.699 -0.225  -0.268  0.544 21551      0\n\n#&gt; # A tibble: 5 × 7\n#&gt;   cut          min    mean  median   max     n n_miss\n#&gt;   &lt;ord&gt;      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;  &lt;int&gt;\n#&gt; 1 Fair      -0.658 -0.0273  0      0.700  1610      0\n#&gt; 2 Good      -0.638 -0.133  -0.0862 0.479  4906      0\n#&gt; 3 Very Good -0.699 -0.164  -0.149  0.602 12082      0\n#&gt; 4 Premium   -0.699 -0.125  -0.0655 0.603 13791      0\n#&gt; 5 Ideal     -0.699 -0.225  -0.268  0.544 21551      0\n\nTo summarize multiple variables, use across().\nHere’s a version of count that also calculates proportions.\n\n# https://twitter.com/Diabb6/status/1571635146658402309\ncount_prop &lt;- function(df, var, sort = FALSE) {\n  df |&gt;\n    count({{ var }}, sort = sort) |&gt;\n    mutate(prop = n / sum(n))\n}\n\ndiamonds |&gt; count_prop(clarity)\n\n# A tibble: 8 × 3\n  clarity     n   prop\n  &lt;ord&gt;   &lt;int&gt;  &lt;dbl&gt;\n1 I1        741 0.0137\n2 SI2      9194 0.170 \n3 SI1     13065 0.242 \n4 VS2     12258 0.227 \n5 VS1      8171 0.151 \n6 VVS2     5066 0.0939\n7 VVS1     3655 0.0678\n8 IF       1790 0.0332\n\n#&gt; # A tibble: 8 × 3\n#&gt;   clarity     n   prop\n#&gt;   &lt;ord&gt;   &lt;int&gt;  &lt;dbl&gt;\n#&gt; 1 I1        741 0.0137\n#&gt; 2 SI2      9194 0.170 \n#&gt; 3 SI1     13065 0.242 \n#&gt; 4 VS2     12258 0.227 \n#&gt; 5 VS1      8171 0.151 \n#&gt; 6 VVS2     5066 0.0939\n#&gt; # ℹ 2 more rows\n\nFinding sorted unique values of a subset of the data:\n\nunique_where &lt;- function(df, condition, var) {\n  df |&gt; \n    filter({{ condition }}) |&gt; \n    distinct({{ var }}) |&gt; \n    arrange({{ var }})\n}\n\n# Find all the destinations in December\nflights |&gt; unique_where(month == 12, dest)\n\n# A tibble: 96 × 1\n   dest \n   &lt;chr&gt;\n 1 ABQ  \n 2 ALB  \n 3 ATL  \n 4 AUS  \n 5 AVL  \n 6 BDL  \n 7 BGR  \n 8 BHM  \n 9 BNA  \n10 BOS  \n# ℹ 86 more rows\n\n#&gt; # A tibble: 96 × 1\n#&gt;   dest \n#&gt;   &lt;chr&gt;\n#&gt; 1 ABQ  \n#&gt; 2 ALB  \n#&gt; 3 ATL  \n#&gt; 4 AUS  \n#&gt; 5 AVL  \n#&gt; 6 BDL  \n#&gt; # ℹ 90 more rows\n\nData-masking vs. tidy-selection\n\n\n\n\n\n\n\n\n\n\nSituation\nWhat you have\nWhat you want\nUse this\nExample\n\n\n\n\nColumn passed as bare name\ncol\nUse column values\n{ col }\nfilter(df, {{ col }} &gt; 0)\n\n\nColumn name as string\n\"col\"\nUse column values\n.data[[col]]\nfilter(df, .data[[col]] &gt; 0)\n\n\nColumn passed bare\ncols\nSelect columns\n{ cols }\nselect(df, {{ cols }})\n\n\nVector of column names\nc(\"a\",\"b\")\nSelect columns\nall_of()\nselect(df, all_of(cols))\n\n\nOptional columns\nc(\"a\",\"b\")\nSelect if present\nany_of()\nselect(df, any_of(cols))\n\n\nApply function to columns\ntidy-select\nMultiple columns\nacross()\nmutate(df, across({{ cols }}, mean))\n\n\nProgramming w/ quosures\nexpression\nFull NSE control\nenquo() / !!\nfilter(df, !!q &gt; 0)\n\n\nMix columns + env vars\nboth\nAvoid name clash\n.data / .env\nfilter(df, x &gt; .env$cutoff)\n\n\n\nWhat if you want to select variables inside of a function that uses data-masking? The following will not work:\n\n# count_missing &lt;- function(df, group_vars, x_var) {\n#   df |&gt; \n#     group_by({{ group_vars }}) |&gt; # this is a problem. group_by uses data masking, not tidy selection.\n#     summarize(\n#       n_miss = sum(is.na({{ x_var }})), \n#       .groups = \"drop\"\n#     )\n# }\n# \n# flights |&gt; \n#   count_missing(c(year, month, day), dep_time)\n#&gt; Error in `group_by()`:\n#&gt; ℹ In argument: `c(year, month, day)`.\n#&gt; Caused by error:\n#&gt; ! `c(year, month, day)` must be size 336776 or 1, not 1010328.\n\nUse the pick() function for tidy selection inside of a data masking function.\n\ncount_missing &lt;- function(df, group_vars, x_var) {\n  df |&gt; \n    group_by(pick({{ group_vars }})) |&gt; \n    summarize(\n      n_miss = sum(is.na({{ x_var }})),\n      .groups = \"drop\"\n  )\n}\n\nflights |&gt; \n  count_missing(c(year, month, day), dep_time)\n\n# A tibble: 365 × 4\n    year month   day n_miss\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n 1  2013     1     1      4\n 2  2013     1     2      8\n 3  2013     1     3     10\n 4  2013     1     4      6\n 5  2013     1     5      3\n 6  2013     1     6      1\n 7  2013     1     7      3\n 8  2013     1     8      4\n 9  2013     1     9      5\n10  2013     1    10      3\n# ℹ 355 more rows\n\n#&gt; # A tibble: 365 × 4\n#&gt;    year month   day n_miss\n#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n#&gt; 1  2013     1     1      4\n#&gt; 2  2013     1     2      8\n#&gt; 3  2013     1     3     10\n#&gt; 4  2013     1     4      6\n#&gt; 5  2013     1     5      3\n#&gt; 6  2013     1     6      1\n#&gt; # ℹ 359 more rows\n\n\n# https://twitter.com/pollicipes/status/1571606508944719876\ncount_wide &lt;- function(data, rows, cols) {\n  data |&gt; \n    count(pick(c({{ rows }}, {{ cols }}))) |&gt; \n    pivot_wider(\n      names_from = {{ cols }}, # names_from uses tidy selection\n      values_from = n,\n      names_sort = TRUE,\n      values_fill = 0\n    )\n}\n\ndiamonds |&gt; count_wide(c(clarity, color), cut)\n\n# A tibble: 56 × 7\n   clarity color  Fair  Good `Very Good` Premium Ideal\n   &lt;ord&gt;   &lt;ord&gt; &lt;int&gt; &lt;int&gt;       &lt;int&gt;   &lt;int&gt; &lt;int&gt;\n 1 I1      D         4     8           5      12    13\n 2 I1      E         9    23          22      30    18\n 3 I1      F        35    19          13      34    42\n 4 I1      G        53    19          16      46    16\n 5 I1      H        52    14          12      46    38\n 6 I1      I        34     9           8      24    17\n 7 I1      J        23     4           8      13     2\n 8 SI2     D        56   223         314     421   356\n 9 SI2     E        78   202         445     519   469\n10 SI2     F        89   201         343     523   453\n# ℹ 46 more rows\n\n#&gt; # A tibble: 56 × 7\n#&gt;   clarity color  Fair  Good `Very Good` Premium Ideal\n#&gt;   &lt;ord&gt;   &lt;ord&gt; &lt;int&gt; &lt;int&gt;       &lt;int&gt;   &lt;int&gt; &lt;int&gt;\n#&gt; 1 I1      D         4     8           5      12    13\n#&gt; 2 I1      E         9    23          22      30    18\n#&gt; 3 I1      F        35    19          13      34    42\n#&gt; 4 I1      G        53    19          16      46    16\n#&gt; 5 I1      H        52    14          12      46    38\n#&gt; 6 I1      I        34     9           8      24    17\n#&gt; # ℹ 50 more rows\n\n\n\n\nInstead of returning a dataframe, you can return a plot from a function. ggplot2::aes() is a data-masking function.\n\ndiamonds |&gt; \n  ggplot(aes(x = carat)) +\n  geom_histogram(binwidth = 0.1)\n\n\n\n\n\n\n\ndiamonds |&gt; \n  ggplot(aes(x = carat)) +\n  geom_histogram(binwidth = 0.05)\n\n\n\n\n\n\n\n\n\nhistogram &lt;- function(df, var, binwidth = NULL) {\n  df |&gt; \n    ggplot(aes(x = {{ var }})) + \n    geom_histogram(binwidth = binwidth)\n}\n\ndiamonds |&gt; histogram(carat, 0.1)\n\n\n\n\n\n\n\n\nBecause a ggplot2 object is returned, you can add on additional components with +.\n\ndiamonds |&gt; \n  histogram(carat, 0.1) +\n  labs(x = \"Size (in carats)\", y = \"Number of diamonds\")\n\n\n\n\n\n\n\n\nAdding more variables\n\n# https://twitter.com/tyler_js_smith/status/1574377116988104704\nlinearity_check &lt;- function(df, x, y) {\n  df |&gt;\n    ggplot(aes(x = {{ x }}, y = {{ y }})) +\n    geom_point() +\n    geom_smooth(method = \"loess\", formula = y ~ x, color = \"red\", se = FALSE) +\n    geom_smooth(method = \"lm\", formula = y ~ x, color = \"blue\", se = FALSE) \n}\n\nstarwars |&gt; \n  filter(mass &lt; 1000) |&gt; \n  linearity_check(mass, height)\n\n\n\n\n\n\n\n\n\n# https://twitter.com/ppaxisa/status/1574398423175921665\nhex_plot &lt;- function(df, x, y, z, bins = 20, fun = \"mean\") {\n  df |&gt; \n    ggplot(aes(x = {{ x }}, y = {{ y }}, z = {{ z }})) + \n    stat_summary_hex(\n      aes(color = after_scale(fill)), # make border same color as fill\n      bins = bins, \n      fun = fun,\n    )\n}\n\ndiamonds |&gt; hex_plot(carat, price, depth)\n\n\n\n\n\n\n\n\nCombining ggplot with other tidyverse\nSorted bar chart function\n\nsorted_bars &lt;- function(df, var) {\n  df |&gt; \n    # note the walrus operator here\n    mutate({{ var }} := fct_rev(fct_infreq({{ var }})))  |&gt; \n    ggplot(aes(y = {{ var }})) +\n    geom_bar()\n}\n\ndiamonds |&gt; sorted_bars(clarity)\n\n\n\n\n\n\n\n\nConditional plot functions\n\nconditional_bars &lt;- function(df, condition, var) {\n  df |&gt; \n    filter({{ condition }}) |&gt; \n    ggplot(aes(x = {{ var }})) + \n    geom_bar()\n}\n\ndiamonds |&gt; conditional_bars(cut == \"Good\", clarity)\n\n\n\n\n\n\n\n\nLabeling the plots you create\n\nhistogram &lt;- function(df, var, binwidth = NULL) {\n  df |&gt; \n    ggplot(aes(x = {{ var }})) + \n    geom_histogram(binwidth = binwidth)\n}\n\nWhat if we could label the plot with the variable and binwidth used?\nUse rlang::englue() for this\n\nhistogram &lt;- function(df, var, binwidth) {\n  label &lt;- rlang::englue(\"A histogram of {{var}} with binwidth {binwidth}\")\n  \n  df |&gt; \n    ggplot(aes(x = {{ var }})) + \n    geom_histogram(binwidth = binwidth) + \n    labs(title = label)\n}\n\ndiamonds |&gt; histogram(carat, 0.1)"
  },
  {
    "objectID": "chapter_25_notes.html#vector-functions",
    "href": "chapter_25_notes.html#vector-functions",
    "title": "R for Data Science, 2nd Edition - Chapter 25 Notes",
    "section": "",
    "text": "Here is an example of a rescaling function.\n\ndf &lt;- tibble(\n  a = rnorm(5),\n  b = rnorm(5),\n  c = rnorm(5),\n  d = rnorm(5),\n)\n\ndf |&gt; mutate(\n  a = (a - min(a, na.rm = TRUE)) / \n    (max(a, na.rm = TRUE) - min(a, na.rm = TRUE)),\n  b = (b - min(a, na.rm = TRUE)) / \n    (max(b, na.rm = TRUE) - min(b, na.rm = TRUE)),\n  c = (c - min(c, na.rm = TRUE)) / \n    (max(c, na.rm = TRUE) - min(c, na.rm = TRUE)),\n  d = (d - min(d, na.rm = TRUE)) / \n    (max(d, na.rm = TRUE) - min(d, na.rm = TRUE)),\n)\n\n# A tibble: 5 × 4\n       a       b     c     d\n   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 1      -0.361  0.859 0.831\n2 0.0980 -0.0264 1     1    \n3 0.765   0.639  0     0    \n4 0       0.181  0.374 0.131\n5 0.529   0.262  0.710 0.589\n\n#&gt; # A tibble: 5 × 4\n#&gt;       a       b     c     d\n#&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 0.339  0.387  0.291 0    \n#&gt; 2 0.880 -0.613  0.611 0.557\n#&gt; 3 0     -0.0833 1     0.752\n#&gt; 4 0.795 -0.0822 0     1    \n#&gt; 5 1     -0.0952 0.580 0.394\n\nHere is the function:\n\nrescale01 &lt;- function(x) {\n  (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))\n}\n\nNow call the function with mutate for cleaner code:\n\ndf |&gt; mutate(\n  a = rescale01(a),\n  b = rescale01(b),\n  c = rescale01(c),\n  d = rescale01(d),\n)\n\n# A tibble: 5 × 4\n       a     b     c     d\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 1      0     0.859 0.831\n2 0.0980 0.335 1     1    \n3 0.765  1     0     0    \n4 0      0.543 0.374 0.131\n5 0.529  0.624 0.710 0.589\n\n#&gt; # A tibble: 5 × 4\n#&gt;       a     b     c     d\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 0.339 1     0.291 0    \n#&gt; 2 0.880 0     0.611 0.557\n#&gt; 3 0     0.530 1     0.752\n#&gt; 4 0.795 0.531 0     1    \n#&gt; 5 1     0.518 0.580 0.394\n\nVector functions work well inside of mutate() and filter() return a vector that is the same length as the input vector.\n\nz_score &lt;- function(x) {\n  (x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)\n}\n\n\nclamp &lt;- function(x, min, max) {\n  case_when(\n    x &lt; min ~ min,\n    x &gt; max ~ max,\n    .default = x\n  )\n}\n\nclamp(1:10, min = 3, max = 7)\n\n [1] 3 3 3 4 5 6 7 7 7 7\n\n#&gt;  [1] 3 3 3 4 5 6 7 7 7 7\n\n\nfirst_upper &lt;- function(x) {\n  str_sub(x, 1, 1) &lt;- str_to_upper(str_sub(x, 1, 1))\n  x\n}\n\nfirst_upper(\"hello\")\n\n[1] \"Hello\"\n\n#&gt; [1] \"Hello\"\n\n\n# https://twitter.com/NVlabormarket/status/1571939851922198530\nclean_number &lt;- function(x) {\n  is_pct &lt;- str_detect(x, \"%\")\n  num &lt;- x |&gt; \n    str_remove_all(\"%\") |&gt; \n    str_remove_all(\",\") |&gt; \n    str_remove_all(fixed(\"$\")) |&gt; \n    as.numeric()\n  if_else(is_pct, num / 100, num)\n}\n\nclean_number(\"$12,300\")\n\n[1] 12300\n\n#&gt; [1] 12300\nclean_number(\"45%\")\n\n[1] 0.45\n\n#&gt; [1] 0.45\n\n\nfix_na &lt;- function(x) {\n  if_else(x %in% c(997, 998, 999), NA, x)\n}\n\nSummary functions are another important family of functions that return a single value for use in summarize().\n\ncommas &lt;- function(x) {\n  str_flatten(x, collapse = \", \", last = \" and \")\n}\n\ncommas(c(\"cat\", \"dog\", \"pigeon\"))\n\n[1] \"cat, dog and pigeon\"\n\n#&gt; [1] \"cat, dog and pigeon\"\n\n\ncv &lt;- function(x, na.rm = FALSE) {\n  sd(x, na.rm = na.rm) / mean(x, na.rm = na.rm)\n}\n\ncv(runif(100, min = 0, max = 50))\n\n[1] 0.639585\n\n#&gt; [1] 0.5196276\ncv(runif(100, min = 0, max = 500))\n\n[1] 0.5552002\n\n#&gt; [1] 0.5652554\n\n\n# https://twitter.com/gbganalyst/status/1571619641390252033\nn_missing &lt;- function(x) {\n  sum(is.na(x))\n} \n\n\n# mean absolute percentage error\n\n\n# https://twitter.com/neilgcurrie/status/1571607727255834625\nmape &lt;- function(actual, predicted) {\n  sum(abs((actual - predicted) / actual)) / length(actual)\n}"
  },
  {
    "objectID": "chapter_25_notes.html#data-frame-functions",
    "href": "chapter_25_notes.html#data-frame-functions",
    "title": "R for Data Science, 2nd Edition - Chapter 25 Notes",
    "section": "",
    "text": "If copying multiple verbs multiple times, you probably need a data frame function.\nYou will need to deal with the problem of indirection, resulting from tidy evaluation. The solution is to us embracing with { }.\n\ngrouped_mean &lt;- function(df, group_var, mean_var) {\n  df |&gt; \n    group_by(group_var) |&gt; \n    summarize(mean(mean_var))\n}\n\nThe following will error out:\n\n# diamonds |&gt; grouped_mean(cut, carat)\n# #&gt; Error in `group_by()`:\n# #&gt; ! Must group by variables found in `.data`.\n# #&gt; ✖ Column `group_var` is not found.\n\nAn illustration of the problem:\n\ndf &lt;- tibble(\n  mean_var = 1,\n  group_var = \"g\",\n  group = 1,\n  x = 10,\n  y = 100\n)\n\ndf |&gt; grouped_mean(group, x)\n\n# A tibble: 1 × 2\n  group_var `mean(mean_var)`\n  &lt;chr&gt;                &lt;dbl&gt;\n1 g                        1\n\n#&gt; # A tibble: 1 × 2\n#&gt;   group_var `mean(mean_var)`\n#&gt;   &lt;chr&gt;                &lt;dbl&gt;\n#&gt; 1 g                        1\ndf |&gt; grouped_mean(group, y)\n\n# A tibble: 1 × 2\n  group_var `mean(mean_var)`\n  &lt;chr&gt;                &lt;dbl&gt;\n1 g                        1\n\n#&gt; # A tibble: 1 × 2\n#&gt;   group_var `mean(mean_var)`\n#&gt;   &lt;chr&gt;                &lt;dbl&gt;\n#&gt; 1 g                        1\n\nWe are returning values, but it is the “mean” of a single value, which is being returned as 1.\nWe need to tell dplyr to run verbs on the values inside of the variable, and not to look for a variable with the argument name. We don’t actually have a variable called mean_var or group_var.\nUse { } to solve this problem.\n\ngrouped_mean &lt;- function(df, group_var, mean_var) {\n  df |&gt; \n    group_by({{ group_var }}) |&gt; \n    summarize(mean({{ mean_var }}))\n}\n\ndf |&gt; grouped_mean(group, x)\n\n# A tibble: 1 × 2\n  group `mean(x)`\n  &lt;dbl&gt;     &lt;dbl&gt;\n1     1        10\n\n#&gt; # A tibble: 1 × 2\n#&gt;   group `mean(x)`\n#&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1     1        10\n\nWe need to embrace when Data-masking (arrange(), filter(), summarize()) or Tidy-selection (select(), relocate(), rename()) occur.\n\nsummary6 &lt;- function(data, var) {\n  data |&gt; summarize(\n    min = min({{ var }}, na.rm = TRUE),\n    mean = mean({{ var }}, na.rm = TRUE),\n    median = median({{ var }}, na.rm = TRUE),\n    max = max({{ var }}, na.rm = TRUE),\n    n = n(),\n    n_miss = sum(is.na({{ var }})),\n    .groups = \"drop\"\n  )\n}\n\ndiamonds |&gt; summary6(carat)\n\n# A tibble: 1 × 6\n    min  mean median   max     n n_miss\n  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;  &lt;int&gt;\n1   0.2 0.798    0.7  5.01 53940      0\n\n#&gt; # A tibble: 1 × 6\n#&gt;     min  mean median   max     n n_miss\n#&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;  &lt;int&gt;\n#&gt; 1   0.2 0.798    0.7  5.01 53940      0\n\n(Whenever you wrap summarize() in a helper, we think it’s good practice to set .groups = “drop” to both avoid the message and leave the data in an ungrouped state.)\nIf you wrap summarize() within a function, then it can be used on grouped data:\n\ndiamonds |&gt; \n  group_by(cut) |&gt; \n  summary6(carat)\n\n# A tibble: 5 × 7\n  cut         min  mean median   max     n n_miss\n  &lt;ord&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;  &lt;int&gt;\n1 Fair       0.22 1.05    1     5.01  1610      0\n2 Good       0.23 0.849   0.82  3.01  4906      0\n3 Very Good  0.2  0.806   0.71  4    12082      0\n4 Premium    0.2  0.892   0.86  4.01 13791      0\n5 Ideal      0.2  0.703   0.54  3.5  21551      0\n\n#&gt; # A tibble: 5 × 7\n#&gt;   cut         min  mean median   max     n n_miss\n#&gt;   &lt;ord&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;  &lt;int&gt;\n#&gt; 1 Fair       0.22 1.05    1     5.01  1610      0\n#&gt; 2 Good       0.23 0.849   0.82  3.01  4906      0\n#&gt; 3 Very Good  0.2  0.806   0.71  4    12082      0\n#&gt; 4 Premium    0.2  0.892   0.86  4.01 13791      0\n#&gt; 5 Ideal      0.2  0.703   0.54  3.5  21551      0\n\nFurthermore, since the arguments to summarize are data-masking, so is the var argument to summary6(). That means you can also summarize computed variables:\n\ndiamonds |&gt; \n  group_by(cut) |&gt; \n  summary6(log10(carat))\n\n# A tibble: 5 × 7\n  cut          min    mean  median   max     n n_miss\n  &lt;ord&gt;      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;  &lt;int&gt;\n1 Fair      -0.658 -0.0273  0      0.700  1610      0\n2 Good      -0.638 -0.133  -0.0862 0.479  4906      0\n3 Very Good -0.699 -0.164  -0.149  0.602 12082      0\n4 Premium   -0.699 -0.125  -0.0655 0.603 13791      0\n5 Ideal     -0.699 -0.225  -0.268  0.544 21551      0\n\n#&gt; # A tibble: 5 × 7\n#&gt;   cut          min    mean  median   max     n n_miss\n#&gt;   &lt;ord&gt;      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;  &lt;int&gt;\n#&gt; 1 Fair      -0.658 -0.0273  0      0.700  1610      0\n#&gt; 2 Good      -0.638 -0.133  -0.0862 0.479  4906      0\n#&gt; 3 Very Good -0.699 -0.164  -0.149  0.602 12082      0\n#&gt; 4 Premium   -0.699 -0.125  -0.0655 0.603 13791      0\n#&gt; 5 Ideal     -0.699 -0.225  -0.268  0.544 21551      0\n\nTo summarize multiple variables, use across().\nHere’s a version of count that also calculates proportions.\n\n# https://twitter.com/Diabb6/status/1571635146658402309\ncount_prop &lt;- function(df, var, sort = FALSE) {\n  df |&gt;\n    count({{ var }}, sort = sort) |&gt;\n    mutate(prop = n / sum(n))\n}\n\ndiamonds |&gt; count_prop(clarity)\n\n# A tibble: 8 × 3\n  clarity     n   prop\n  &lt;ord&gt;   &lt;int&gt;  &lt;dbl&gt;\n1 I1        741 0.0137\n2 SI2      9194 0.170 \n3 SI1     13065 0.242 \n4 VS2     12258 0.227 \n5 VS1      8171 0.151 \n6 VVS2     5066 0.0939\n7 VVS1     3655 0.0678\n8 IF       1790 0.0332\n\n#&gt; # A tibble: 8 × 3\n#&gt;   clarity     n   prop\n#&gt;   &lt;ord&gt;   &lt;int&gt;  &lt;dbl&gt;\n#&gt; 1 I1        741 0.0137\n#&gt; 2 SI2      9194 0.170 \n#&gt; 3 SI1     13065 0.242 \n#&gt; 4 VS2     12258 0.227 \n#&gt; 5 VS1      8171 0.151 \n#&gt; 6 VVS2     5066 0.0939\n#&gt; # ℹ 2 more rows\n\nFinding sorted unique values of a subset of the data:\n\nunique_where &lt;- function(df, condition, var) {\n  df |&gt; \n    filter({{ condition }}) |&gt; \n    distinct({{ var }}) |&gt; \n    arrange({{ var }})\n}\n\n# Find all the destinations in December\nflights |&gt; unique_where(month == 12, dest)\n\n# A tibble: 96 × 1\n   dest \n   &lt;chr&gt;\n 1 ABQ  \n 2 ALB  \n 3 ATL  \n 4 AUS  \n 5 AVL  \n 6 BDL  \n 7 BGR  \n 8 BHM  \n 9 BNA  \n10 BOS  \n# ℹ 86 more rows\n\n#&gt; # A tibble: 96 × 1\n#&gt;   dest \n#&gt;   &lt;chr&gt;\n#&gt; 1 ABQ  \n#&gt; 2 ALB  \n#&gt; 3 ATL  \n#&gt; 4 AUS  \n#&gt; 5 AVL  \n#&gt; 6 BDL  \n#&gt; # ℹ 90 more rows\n\nData-masking vs. tidy-selection\n\n\n\n\n\n\n\n\n\n\nSituation\nWhat you have\nWhat you want\nUse this\nExample\n\n\n\n\nColumn passed as bare name\ncol\nUse column values\n{ col }\nfilter(df, {{ col }} &gt; 0)\n\n\nColumn name as string\n\"col\"\nUse column values\n.data[[col]]\nfilter(df, .data[[col]] &gt; 0)\n\n\nColumn passed bare\ncols\nSelect columns\n{ cols }\nselect(df, {{ cols }})\n\n\nVector of column names\nc(\"a\",\"b\")\nSelect columns\nall_of()\nselect(df, all_of(cols))\n\n\nOptional columns\nc(\"a\",\"b\")\nSelect if present\nany_of()\nselect(df, any_of(cols))\n\n\nApply function to columns\ntidy-select\nMultiple columns\nacross()\nmutate(df, across({{ cols }}, mean))\n\n\nProgramming w/ quosures\nexpression\nFull NSE control\nenquo() / !!\nfilter(df, !!q &gt; 0)\n\n\nMix columns + env vars\nboth\nAvoid name clash\n.data / .env\nfilter(df, x &gt; .env$cutoff)\n\n\n\nWhat if you want to select variables inside of a function that uses data-masking? The following will not work:\n\n# count_missing &lt;- function(df, group_vars, x_var) {\n#   df |&gt; \n#     group_by({{ group_vars }}) |&gt; # this is a problem. group_by uses data masking, not tidy selection.\n#     summarize(\n#       n_miss = sum(is.na({{ x_var }})), \n#       .groups = \"drop\"\n#     )\n# }\n# \n# flights |&gt; \n#   count_missing(c(year, month, day), dep_time)\n#&gt; Error in `group_by()`:\n#&gt; ℹ In argument: `c(year, month, day)`.\n#&gt; Caused by error:\n#&gt; ! `c(year, month, day)` must be size 336776 or 1, not 1010328.\n\nUse the pick() function for tidy selection inside of a data masking function.\n\ncount_missing &lt;- function(df, group_vars, x_var) {\n  df |&gt; \n    group_by(pick({{ group_vars }})) |&gt; \n    summarize(\n      n_miss = sum(is.na({{ x_var }})),\n      .groups = \"drop\"\n  )\n}\n\nflights |&gt; \n  count_missing(c(year, month, day), dep_time)\n\n# A tibble: 365 × 4\n    year month   day n_miss\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n 1  2013     1     1      4\n 2  2013     1     2      8\n 3  2013     1     3     10\n 4  2013     1     4      6\n 5  2013     1     5      3\n 6  2013     1     6      1\n 7  2013     1     7      3\n 8  2013     1     8      4\n 9  2013     1     9      5\n10  2013     1    10      3\n# ℹ 355 more rows\n\n#&gt; # A tibble: 365 × 4\n#&gt;    year month   day n_miss\n#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n#&gt; 1  2013     1     1      4\n#&gt; 2  2013     1     2      8\n#&gt; 3  2013     1     3     10\n#&gt; 4  2013     1     4      6\n#&gt; 5  2013     1     5      3\n#&gt; 6  2013     1     6      1\n#&gt; # ℹ 359 more rows\n\n\n# https://twitter.com/pollicipes/status/1571606508944719876\ncount_wide &lt;- function(data, rows, cols) {\n  data |&gt; \n    count(pick(c({{ rows }}, {{ cols }}))) |&gt; \n    pivot_wider(\n      names_from = {{ cols }}, # names_from uses tidy selection\n      values_from = n,\n      names_sort = TRUE,\n      values_fill = 0\n    )\n}\n\ndiamonds |&gt; count_wide(c(clarity, color), cut)\n\n# A tibble: 56 × 7\n   clarity color  Fair  Good `Very Good` Premium Ideal\n   &lt;ord&gt;   &lt;ord&gt; &lt;int&gt; &lt;int&gt;       &lt;int&gt;   &lt;int&gt; &lt;int&gt;\n 1 I1      D         4     8           5      12    13\n 2 I1      E         9    23          22      30    18\n 3 I1      F        35    19          13      34    42\n 4 I1      G        53    19          16      46    16\n 5 I1      H        52    14          12      46    38\n 6 I1      I        34     9           8      24    17\n 7 I1      J        23     4           8      13     2\n 8 SI2     D        56   223         314     421   356\n 9 SI2     E        78   202         445     519   469\n10 SI2     F        89   201         343     523   453\n# ℹ 46 more rows\n\n#&gt; # A tibble: 56 × 7\n#&gt;   clarity color  Fair  Good `Very Good` Premium Ideal\n#&gt;   &lt;ord&gt;   &lt;ord&gt; &lt;int&gt; &lt;int&gt;       &lt;int&gt;   &lt;int&gt; &lt;int&gt;\n#&gt; 1 I1      D         4     8           5      12    13\n#&gt; 2 I1      E         9    23          22      30    18\n#&gt; 3 I1      F        35    19          13      34    42\n#&gt; 4 I1      G        53    19          16      46    16\n#&gt; 5 I1      H        52    14          12      46    38\n#&gt; 6 I1      I        34     9           8      24    17\n#&gt; # ℹ 50 more rows"
  },
  {
    "objectID": "chapter_25_notes.html#plot-functions",
    "href": "chapter_25_notes.html#plot-functions",
    "title": "R for Data Science, 2nd Edition - Chapter 25 Notes",
    "section": "",
    "text": "Instead of returning a dataframe, you can return a plot from a function. ggplot2::aes() is a data-masking function.\n\ndiamonds |&gt; \n  ggplot(aes(x = carat)) +\n  geom_histogram(binwidth = 0.1)\n\n\n\n\n\n\n\ndiamonds |&gt; \n  ggplot(aes(x = carat)) +\n  geom_histogram(binwidth = 0.05)\n\n\n\n\n\n\n\n\n\nhistogram &lt;- function(df, var, binwidth = NULL) {\n  df |&gt; \n    ggplot(aes(x = {{ var }})) + \n    geom_histogram(binwidth = binwidth)\n}\n\ndiamonds |&gt; histogram(carat, 0.1)\n\n\n\n\n\n\n\n\nBecause a ggplot2 object is returned, you can add on additional components with +.\n\ndiamonds |&gt; \n  histogram(carat, 0.1) +\n  labs(x = \"Size (in carats)\", y = \"Number of diamonds\")\n\n\n\n\n\n\n\n\nAdding more variables\n\n# https://twitter.com/tyler_js_smith/status/1574377116988104704\nlinearity_check &lt;- function(df, x, y) {\n  df |&gt;\n    ggplot(aes(x = {{ x }}, y = {{ y }})) +\n    geom_point() +\n    geom_smooth(method = \"loess\", formula = y ~ x, color = \"red\", se = FALSE) +\n    geom_smooth(method = \"lm\", formula = y ~ x, color = \"blue\", se = FALSE) \n}\n\nstarwars |&gt; \n  filter(mass &lt; 1000) |&gt; \n  linearity_check(mass, height)\n\n\n\n\n\n\n\n\n\n# https://twitter.com/ppaxisa/status/1574398423175921665\nhex_plot &lt;- function(df, x, y, z, bins = 20, fun = \"mean\") {\n  df |&gt; \n    ggplot(aes(x = {{ x }}, y = {{ y }}, z = {{ z }})) + \n    stat_summary_hex(\n      aes(color = after_scale(fill)), # make border same color as fill\n      bins = bins, \n      fun = fun,\n    )\n}\n\ndiamonds |&gt; hex_plot(carat, price, depth)\n\n\n\n\n\n\n\n\nCombining ggplot with other tidyverse\nSorted bar chart function\n\nsorted_bars &lt;- function(df, var) {\n  df |&gt; \n    # note the walrus operator here\n    mutate({{ var }} := fct_rev(fct_infreq({{ var }})))  |&gt; \n    ggplot(aes(y = {{ var }})) +\n    geom_bar()\n}\n\ndiamonds |&gt; sorted_bars(clarity)\n\n\n\n\n\n\n\n\nConditional plot functions\n\nconditional_bars &lt;- function(df, condition, var) {\n  df |&gt; \n    filter({{ condition }}) |&gt; \n    ggplot(aes(x = {{ var }})) + \n    geom_bar()\n}\n\ndiamonds |&gt; conditional_bars(cut == \"Good\", clarity)\n\n\n\n\n\n\n\n\nLabeling the plots you create\n\nhistogram &lt;- function(df, var, binwidth = NULL) {\n  df |&gt; \n    ggplot(aes(x = {{ var }})) + \n    geom_histogram(binwidth = binwidth)\n}\n\nWhat if we could label the plot with the variable and binwidth used?\nUse rlang::englue() for this\n\nhistogram &lt;- function(df, var, binwidth) {\n  label &lt;- rlang::englue(\"A histogram of {{var}} with binwidth {binwidth}\")\n  \n  df |&gt; \n    ggplot(aes(x = {{ var }})) + \n    geom_histogram(binwidth = binwidth) + \n    labs(title = label)\n}\n\ndiamonds |&gt; histogram(carat, 0.1)"
  },
  {
    "objectID": "chapter_12_notes.html",
    "href": "chapter_12_notes.html",
    "title": "R for Data Science, 2nd Edition - Chapter 12 Notes",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.6\n✔ forcats   1.0.1     ✔ stringr   1.6.0\n✔ ggplot2   4.0.1     ✔ tibble    3.3.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.2\n✔ purrr     1.2.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "chapter_10_notes.html",
    "href": "chapter_10_notes.html",
    "title": "R for Data Science, 2nd Edition - Chapter 10 Notes",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.6\n✔ forcats   1.0.1     ✔ stringr   1.6.0\n✔ ggplot2   4.0.1     ✔ tibble    3.3.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.2\n✔ purrr     1.2.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "chapter_10_notes.html#questions",
    "href": "chapter_10_notes.html#questions",
    "title": "R for Data Science, 2nd Edition - Chapter 10 Notes",
    "section": "10.2 Questions",
    "text": "10.2 Questions\nTwo types of questions are generally useful:\n\nWhat type of variation occurs within my variables?\nWhat type of covariation occurs between my variables?"
  },
  {
    "objectID": "chapter_10_notes.html#variation",
    "href": "chapter_10_notes.html#variation",
    "title": "R for Data Science, 2nd Edition - Chapter 10 Notes",
    "section": "10.3 Variation",
    "text": "10.3 Variation\nBoth within and across units.\n\nggplot(diamonds, aes(x = carat)) +\n  geom_histogram(binwidth = 0.5)\n\n\n\n\n\n\n\n\nMore questions to ask:\n\nWhich values are the most common? Why?\nWhich values are rare? Why? Does that match your expectations?\nCan you see any unusual patterns? What might explain them?\n\n\nsmaller &lt;- diamonds |&gt; \n  filter(carat &lt; 3)\n\nggplot(smaller, aes(x = carat)) +\n  geom_histogram(binwidth = 0.01)\n\n\n\n\n\n\n\n\nExamples of follow-up questions for this plot:\n\nWhy are there more diamonds at whole carats and common fractions of carats?\nWhy are there more diamonds slightly to the right of each peak than there are slightly to the left of each peak?\n\nData visualization can reveal clusters. To understand subgroups:\n\nHow are the observations within each subgroup similar to each other?\nHow are the observations in separate clusters different from each other?\nHow can you explain or describe the clusters?\nWhy might the appearance of clusters be misleading?"
  },
  {
    "objectID": "chapter_10_notes.html#unusual-values",
    "href": "chapter_10_notes.html#unusual-values",
    "title": "R for Data Science, 2nd Edition - Chapter 10 Notes",
    "section": "10.4 Unusual values",
    "text": "10.4 Unusual values\nSometimes these are hard to see, but you can tell by the limits of the x-axis sometimes:\n\nggplot(diamonds, aes(x = y)) + \n  geom_histogram(binwidth = 0.5)\n\n\n\n\n\n\n\n\nYou can zoom in on areas of your plot using coord_cartesian.\n\nggplot(diamonds, aes(x = y)) + \n  geom_histogram(binwidth = 0.5) +\n  coord_cartesian(ylim = c(0, 50))\n\n\n\n\n\n\n\n\nThree unusual values at about 0, 30, and 60\n\nunusual &lt;- diamonds |&gt; \n  filter(y &lt; 3 | y &gt; 20) |&gt; \n  select(price, x, y, z) |&gt;\n  arrange(y)\nunusual\n\n# A tibble: 9 × 4\n  price     x     y     z\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  5139  0      0    0   \n2  6381  0      0    0   \n3 12800  0      0    0   \n4 15686  0      0    0   \n5 18034  0      0    0   \n6  2130  0      0    0   \n7  2130  0      0    0   \n8  2075  5.15  31.8  5.12\n9 12210  8.09  58.9  8.06\n\n#&gt; # A tibble: 9 × 4\n#&gt;   price     x     y     z\n#&gt;   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1  5139  0      0    0   \n#&gt; 2  6381  0      0    0   \n#&gt; 3 12800  0      0    0   \n#&gt; 4 15686  0      0    0   \n#&gt; 5 18034  0      0    0   \n#&gt; 6  2130  0      0    0   \n#&gt; 7  2130  0      0    0   \n#&gt; 8  2075  5.15  31.8  5.12\n#&gt; 9 12210  8.09  58.9  8.06\n\nOne approach is to replace unusual values with NA.\n\ndiamonds2 &lt;- diamonds |&gt; \n  mutate(y = if_else(y &lt; 3 | y &gt; 20, NA, y))\n\nggplot excludes NA values:\n\nggplot(diamonds2, aes(x = x, y = y)) + \n  geom_point()\n\nWarning: Removed 9 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n#&gt; Warning: Removed 9 rows containing missing values or values outside the scale range\n#&gt; (`geom_point()`).\n\nYou can suppress the missing warning like this:\n\nggplot(diamonds2, aes(x = x, y = y)) + \n  geom_point(na.rm = TRUE)\n\n\n\n\n\n\n\n\nInvestigating missing values is also important. Understand the patterns of missingness. Here, missing values in the dep_time variable indicate that the flight was cancelled.\n\nnycflights13::flights |&gt; \n  mutate(\n    cancelled = is.na(dep_time),\n    sched_hour = sched_dep_time %/% 100,\n    sched_min = sched_dep_time %% 100,\n    sched_dep_time = sched_hour + (sched_min / 60)\n  ) |&gt; \n  ggplot(aes(x = sched_dep_time)) + \n  geom_freqpoly(aes(color = cancelled), binwidth = 1/4)"
  },
  {
    "objectID": "chapter_10_notes.html#covariation",
    "href": "chapter_10_notes.html#covariation",
    "title": "R for Data Science, 2nd Edition - Chapter 10 Notes",
    "section": "10.5 Covariation",
    "text": "10.5 Covariation\nWhen two variables vary together. Plot them.\nA categorical and numerical.\n\nggplot(diamonds, aes(x = price)) + \n  geom_freqpoly(aes(color = cut), binwidth = 500, linewidth = 0.75)\n\n\n\n\n\n\n\n\nHard to compare because counts are so different. Use the density, which is the count standardized so that the area under each frequency polygon is one.\n\nggplot(diamonds, aes(x = price, y = after_stat(density))) + \n  geom_freqpoly(aes(color = cut), binwidth = 500, linewidth = 0.75)\n\n\n\n\n\n\n\n\nDo fair cuts have the highest average price as indicated by the prior plot?\n\nggplot(diamonds, aes(x = cut, y = price)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nWhy are better quality cuts cheaper?\nHow does highway mileage vary across classes?\n\nggplot(mpg, aes(x = class, y = hwy)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nTemporarily reordering factors would make the trend easier to see:\n\nggplot(mpg, aes(x = fct_reorder(class, hwy, median), y = hwy)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nLong variable names look better if you flip the x and y aesthetic mappings:\n\nggplot(mpg, aes(x = hwy, y = fct_reorder(class, hwy, median))) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nTwo categorical variables.\nCount the cell values in the cross tabulation table and then plot.\n\nggplot(diamonds, aes(x = cut, y = color)) +\n  geom_count()\n\n\n\n\n\n\n\n\nGenerate the counts\n\ndiamonds |&gt; \n  count(color, cut)\n\n# A tibble: 35 × 3\n   color cut           n\n   &lt;ord&gt; &lt;ord&gt;     &lt;int&gt;\n 1 D     Fair        163\n 2 D     Good        662\n 3 D     Very Good  1513\n 4 D     Premium    1603\n 5 D     Ideal      2834\n 6 E     Fair        224\n 7 E     Good        933\n 8 E     Very Good  2400\n 9 E     Premium    2337\n10 E     Ideal      3903\n# ℹ 25 more rows\n\n#&gt; # A tibble: 35 × 3\n#&gt;   color cut           n\n#&gt;   &lt;ord&gt; &lt;ord&gt;     &lt;int&gt;\n#&gt; 1 D     Fair        163\n#&gt; 2 D     Good        662\n#&gt; 3 D     Very Good  1513\n#&gt; 4 D     Premium    1603\n#&gt; 5 D     Ideal      2834\n#&gt; 6 E     Fair        224\n#&gt; # ℹ 29 more rows\n\nAnd then visualize with geom_tile().\n\ndiamonds |&gt; \n  count(color, cut) |&gt;  \n  ggplot(aes(x = color, y = cut)) +\n  geom_tile(aes(fill = n))\n\n\n\n\n\n\n\n\nTwo numerical variables.\n\nggplot(smaller, aes(x = carat, y = price)) +\n  geom_point()\n\n\n\n\n\n\n\n\nOverplotting can be a problem. You can use alpha:\n\nggplot(smaller, aes(x = carat, y = price)) + \n  geom_point(alpha = 1 / 100)\n\n\n\n\n\n\n\n\nAnother solution to overplotting is binning. geom_histogram() and geom_freqpoly() bin in one dimension. Use geom_bin2d() and geom_hex() to bin in two dimensions. The difference is between rectangular bins and hexagonal bins. Install the hexbin package to use geom_hex().\n\nggplot(smaller, aes(x = carat, y = price)) +\n  geom_bin2d()\n\n`stat_bin2d()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\n#&gt; `stat_bin2d()` using `bins = 30`. Pick better value `binwidth`.\n\n# install.packages(\"hexbin\")\nggplot(smaller, aes(x = carat, y = price)) +\n  geom_hex()\n\n\n\n\n\n\n\n\nYou can also bin one numerical variable, so that you are plotting a categorical and numerical:\n\nggplot(smaller, aes(x = carat, y = price)) + \n  geom_boxplot(aes(group = cut_width(carat, 0.1)))\n\nWarning: Orientation is not uniquely specified when both the x and y aesthetics are\ncontinuous. Picking default orientation 'x'.\n\n\n\n\n\n\n\n\n#&gt; Warning: Orientation is not uniquely specified when both the x and y aesthetics are\n#&gt; continuous. Picking default orientation 'x'.\n\nYou can make the width of boxplots proportional to number of observations with varwidth = TRUE.\n\nggplot(smaller, aes(x = carat, y = price)) + \n  geom_boxplot(aes(group = cut_width(carat, 0.1)), varwidth = TRUE)\n\nWarning: Orientation is not uniquely specified when both the x and y aesthetics are\ncontinuous. Picking default orientation 'x'."
  },
  {
    "objectID": "chapter_10_notes.html#patterns-and-models",
    "href": "chapter_10_notes.html#patterns-and-models",
    "title": "R for Data Science, 2nd Edition - Chapter 10 Notes",
    "section": "10.6 Patterns and models",
    "text": "10.6 Patterns and models\nMore questions to ask:\n\nCould this pattern be due to coincidence (i.e. random chance)?\nHow can you describe the relationship implied by the pattern?\nHow strong is the relationship implied by the pattern?\nWhat other variables might affect the relationship?\nDoes the relationship change if you look at individual subgroups of the data?\n\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.4.1 ──\n\n\n✔ broom        1.0.12     ✔ rsample      1.3.2 \n✔ dials        1.4.2      ✔ tailor       0.1.0 \n✔ infer        1.1.0      ✔ tune         2.0.1 \n✔ modeldata    1.5.1      ✔ workflows    1.3.0 \n✔ parsnip      1.4.1      ✔ workflowsets 1.1.1 \n✔ recipes      1.3.1      ✔ yardstick    1.3.2 \n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\ndiamonds &lt;- diamonds |&gt;\n  mutate(\n    log_price = log(price),\n    log_carat = log(carat)\n  )\n\ndiamonds_fit &lt;- linear_reg() |&gt;\n  fit(log_price ~ log_carat, data = diamonds)\n\ndiamonds_aug &lt;- augment(diamonds_fit, new_data = diamonds) |&gt;\n  mutate(.resid = exp(.resid))\n\nggplot(diamonds_aug, aes(x = carat, y = .resid)) + \n  geom_point()\n\n\n\n\n\n\n\n\nNow you can see the relationship between cut and price: better quality diamonds are more expensive relative to size.\n\nggplot(diamonds_aug, aes(x = cut, y = .resid)) + \n  geom_boxplot()"
  },
  {
    "objectID": "chapter_05_notes.html",
    "href": "chapter_05_notes.html",
    "title": "R for Data Science, 2nd Edition - Chapter 5 Notes",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.6\n✔ forcats   1.0.1     ✔ stringr   1.6.0\n✔ ggplot2   4.0.1     ✔ tibble    3.3.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.2\n✔ purrr     1.2.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\nUse pivot_longer().\n\nbillboard |&gt; \n  pivot_longer(\n    cols = starts_with(\"wk\"), \n    names_to = \"week\", \n    values_to = \"rank\"\n  )\n\n# A tibble: 24,092 × 5\n   artist track                   date.entered week   rank\n   &lt;chr&gt;  &lt;chr&gt;                   &lt;date&gt;       &lt;chr&gt; &lt;dbl&gt;\n 1 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk1      87\n 2 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk2      82\n 3 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk3      72\n 4 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk4      77\n 5 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk5      87\n 6 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk6      94\n 7 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk7      99\n 8 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk8      NA\n 9 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk9      NA\n10 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk10     NA\n# ℹ 24,082 more rows\n\n#&gt; # A tibble: 24,092 × 5\n#&gt;    artist track                   date.entered week   rank\n#&gt;    &lt;chr&gt;  &lt;chr&gt;                   &lt;date&gt;       &lt;chr&gt; &lt;dbl&gt;\n#&gt;  1 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk1      87\n#&gt;  2 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk2      82\n#&gt;  3 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk3      72\n#&gt;  4 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk4      77\n#&gt;  5 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk5      87\n#&gt;  6 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk6      94\n#&gt;  7 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk7      99\n#&gt;  8 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk8      NA\n#&gt;  9 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk9      NA\n#&gt; 10 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk10     NA\n#&gt; # ℹ 24,082 more rows\n\n\ncols specifies which columns need to be pivoted, i.e. which columns aren’t variables. This argument uses the same syntax as select() so here we could use !c(artist, track, date.entered) or starts_with(\"wk\").\nnames_to names the variable stored in the column names, we named that variable week.\nvalues_to names the variable stored in the cell values, we named that variable rank.\n\nSo pivot_longer() starts with identifying which columns are variables, and which are not. For those columns that are not variables, we create a single variable capturing the names of these non-variable columns, and another variable capturing the values in the cells of those non-variable columns, which we must also name.\nSome NA values may be generated by the structure of the data set. This can be avoided with values_drop_na = TRUE.\n\nbillboard |&gt; \n  pivot_longer(\n    cols = starts_with(\"wk\"), \n    names_to = \"week\", \n    values_to = \"rank\",\n    values_drop_na = TRUE\n  )\n\n# A tibble: 5,307 × 5\n   artist  track                   date.entered week   rank\n   &lt;chr&gt;   &lt;chr&gt;                   &lt;date&gt;       &lt;chr&gt; &lt;dbl&gt;\n 1 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk1      87\n 2 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk2      82\n 3 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk3      72\n 4 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk4      77\n 5 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk5      87\n 6 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk6      94\n 7 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk7      99\n 8 2Ge+her The Hardest Part Of ... 2000-09-02   wk1      91\n 9 2Ge+her The Hardest Part Of ... 2000-09-02   wk2      87\n10 2Ge+her The Hardest Part Of ... 2000-09-02   wk3      92\n# ℹ 5,297 more rows\n\n#&gt; # A tibble: 5,307 × 5\n#&gt;   artist track                   date.entered week   rank\n#&gt;   &lt;chr&gt;  &lt;chr&gt;                   &lt;date&gt;       &lt;chr&gt; &lt;dbl&gt;\n#&gt; 1 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk1      87\n#&gt; 2 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk2      82\n#&gt; 3 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk3      72\n#&gt; 4 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk4      77\n#&gt; 5 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk5      87\n#&gt; 6 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk6      94\n#&gt; # ℹ 5,301 more rows\n\nThe values for week are characters. This is a good opportunity to use parse_number().\n\nbillboard_longer &lt;- billboard |&gt; \n  pivot_longer(\n    cols = starts_with(\"wk\"), \n    names_to = \"week\", \n    values_to = \"rank\",\n    values_drop_na = TRUE\n  ) |&gt; \n  mutate(\n    week = parse_number(week)\n  )\nbillboard_longer\n\n# A tibble: 5,307 × 5\n   artist  track                   date.entered  week  rank\n   &lt;chr&gt;   &lt;chr&gt;                   &lt;date&gt;       &lt;dbl&gt; &lt;dbl&gt;\n 1 2 Pac   Baby Don't Cry (Keep... 2000-02-26       1    87\n 2 2 Pac   Baby Don't Cry (Keep... 2000-02-26       2    82\n 3 2 Pac   Baby Don't Cry (Keep... 2000-02-26       3    72\n 4 2 Pac   Baby Don't Cry (Keep... 2000-02-26       4    77\n 5 2 Pac   Baby Don't Cry (Keep... 2000-02-26       5    87\n 6 2 Pac   Baby Don't Cry (Keep... 2000-02-26       6    94\n 7 2 Pac   Baby Don't Cry (Keep... 2000-02-26       7    99\n 8 2Ge+her The Hardest Part Of ... 2000-09-02       1    91\n 9 2Ge+her The Hardest Part Of ... 2000-09-02       2    87\n10 2Ge+her The Hardest Part Of ... 2000-09-02       3    92\n# ℹ 5,297 more rows\n\n#&gt; # A tibble: 5,307 × 5\n#&gt;   artist track                   date.entered  week  rank\n#&gt;   &lt;chr&gt;  &lt;chr&gt;                   &lt;date&gt;       &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 2 Pac  Baby Don't Cry (Keep... 2000-02-26       1    87\n#&gt; 2 2 Pac  Baby Don't Cry (Keep... 2000-02-26       2    82\n#&gt; 3 2 Pac  Baby Don't Cry (Keep... 2000-02-26       3    72\n#&gt; 4 2 Pac  Baby Don't Cry (Keep... 2000-02-26       4    77\n#&gt; 5 2 Pac  Baby Don't Cry (Keep... 2000-02-26       5    87\n#&gt; 6 2 Pac  Baby Don't Cry (Keep... 2000-02-26       6    94\n#&gt; # ℹ 5,301 more rows\n\nExamining pivot_longer()\n\ndf &lt;- tribble(\n  ~id,  ~bp1, ~bp2,\n   \"A\",  100,  120,\n   \"B\",  140,  115,\n   \"C\",  120,  125\n)\n\ndf |&gt; \n  pivot_longer(\n    cols = bp1:bp2,\n    names_to = \"measurement\",\n    values_to = \"value\"\n  )\n\n# A tibble: 6 × 3\n  id    measurement value\n  &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;\n1 A     bp1           100\n2 A     bp2           120\n3 B     bp1           140\n4 B     bp2           115\n5 C     bp1           120\n6 C     bp2           125\n\n#&gt; # A tibble: 6 × 3\n#&gt;   id    measurement value\n#&gt;   &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;\n#&gt; 1 A     bp1           100\n#&gt; 2 A     bp2           120\n#&gt; 3 B     bp1           140\n#&gt; 4 B     bp2           115\n#&gt; 5 C     bp1           120\n#&gt; 6 C     bp2           125\n\nWhat if column names contain multiple pieces of information?\n\nwho2\n\n# A tibble: 7,240 × 58\n   country      year sp_m_014 sp_m_1524 sp_m_2534 sp_m_3544 sp_m_4554 sp_m_5564\n   &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 Afghanistan  1980       NA        NA        NA        NA        NA        NA\n 2 Afghanistan  1981       NA        NA        NA        NA        NA        NA\n 3 Afghanistan  1982       NA        NA        NA        NA        NA        NA\n 4 Afghanistan  1983       NA        NA        NA        NA        NA        NA\n 5 Afghanistan  1984       NA        NA        NA        NA        NA        NA\n 6 Afghanistan  1985       NA        NA        NA        NA        NA        NA\n 7 Afghanistan  1986       NA        NA        NA        NA        NA        NA\n 8 Afghanistan  1987       NA        NA        NA        NA        NA        NA\n 9 Afghanistan  1988       NA        NA        NA        NA        NA        NA\n10 Afghanistan  1989       NA        NA        NA        NA        NA        NA\n# ℹ 7,230 more rows\n# ℹ 50 more variables: sp_m_65 &lt;dbl&gt;, sp_f_014 &lt;dbl&gt;, sp_f_1524 &lt;dbl&gt;,\n#   sp_f_2534 &lt;dbl&gt;, sp_f_3544 &lt;dbl&gt;, sp_f_4554 &lt;dbl&gt;, sp_f_5564 &lt;dbl&gt;,\n#   sp_f_65 &lt;dbl&gt;, sn_m_014 &lt;dbl&gt;, sn_m_1524 &lt;dbl&gt;, sn_m_2534 &lt;dbl&gt;,\n#   sn_m_3544 &lt;dbl&gt;, sn_m_4554 &lt;dbl&gt;, sn_m_5564 &lt;dbl&gt;, sn_m_65 &lt;dbl&gt;,\n#   sn_f_014 &lt;dbl&gt;, sn_f_1524 &lt;dbl&gt;, sn_f_2534 &lt;dbl&gt;, sn_f_3544 &lt;dbl&gt;,\n#   sn_f_4554 &lt;dbl&gt;, sn_f_5564 &lt;dbl&gt;, sn_f_65 &lt;dbl&gt;, ep_m_014 &lt;dbl&gt;, …\n\n#&gt; # A tibble: 7,240 × 58\n#&gt;   country      year sp_m_014 sp_m_1524 sp_m_2534 sp_m_3544 sp_m_4554\n#&gt;   &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 Afghanistan  1980       NA        NA        NA        NA        NA\n#&gt; 2 Afghanistan  1981       NA        NA        NA        NA        NA\n#&gt; 3 Afghanistan  1982       NA        NA        NA        NA        NA\n#&gt; 4 Afghanistan  1983       NA        NA        NA        NA        NA\n#&gt; 5 Afghanistan  1984       NA        NA        NA        NA        NA\n#&gt; 6 Afghanistan  1985       NA        NA        NA        NA        NA\n#&gt; # ℹ 7,234 more rows\n#&gt; # ℹ 51 more variables: sp_m_5564 &lt;dbl&gt;, sp_m_65 &lt;dbl&gt;, sp_f_014 &lt;dbl&gt;, …\n\nSo in this case we have six pieces of information recorded in who2: the country and the year (already columns); the method of diagnosis, the gender category, and the age range category (contained in the other column names); and the count of patients in that category (cell values).\n\nwho2 |&gt; \n  pivot_longer(\n    cols = !(country:year),\n    names_to = c(\"diagnosis\", \"gender\", \"age\"), \n    names_sep = \"_\",\n    values_to = \"count\"\n  )\n\n# A tibble: 405,440 × 6\n   country      year diagnosis gender age   count\n   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt;\n 1 Afghanistan  1980 sp        m      014      NA\n 2 Afghanistan  1980 sp        m      1524     NA\n 3 Afghanistan  1980 sp        m      2534     NA\n 4 Afghanistan  1980 sp        m      3544     NA\n 5 Afghanistan  1980 sp        m      4554     NA\n 6 Afghanistan  1980 sp        m      5564     NA\n 7 Afghanistan  1980 sp        m      65       NA\n 8 Afghanistan  1980 sp        f      014      NA\n 9 Afghanistan  1980 sp        f      1524     NA\n10 Afghanistan  1980 sp        f      2534     NA\n# ℹ 405,430 more rows\n\n#&gt; # A tibble: 405,440 × 6\n#&gt;   country      year diagnosis gender age   count\n#&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt;\n#&gt; 1 Afghanistan  1980 sp        m      014      NA\n#&gt; 2 Afghanistan  1980 sp        m      1524     NA\n#&gt; 3 Afghanistan  1980 sp        m      2534     NA\n#&gt; 4 Afghanistan  1980 sp        m      3544     NA\n#&gt; 5 Afghanistan  1980 sp        m      4554     NA\n#&gt; 6 Afghanistan  1980 sp        m      5564     NA\n#&gt; # ℹ 405,434 more rows\n\nWhat if the column names include a mix of of variable values and variable names?\n\nhousehold\n\n# A tibble: 5 × 5\n  family dob_child1 dob_child2 name_child1 name_child2\n   &lt;int&gt; &lt;date&gt;     &lt;date&gt;     &lt;chr&gt;       &lt;chr&gt;      \n1      1 1998-11-26 2000-01-29 Susan       Jose       \n2      2 1996-06-22 NA         Mark        &lt;NA&gt;       \n3      3 2002-07-11 2004-04-05 Sam         Seth       \n4      4 2004-10-10 2009-08-27 Craig       Khai       \n5      5 2000-12-05 2005-02-28 Parker      Gracie     \n\n\nUse the .value sentinel value to indicate the first component of the pivoted column name as a variable name in the output. Note the lack of a values_to argument here. When you use .value in names_to, the column names in the input contribute to both values and variable names in the output.\n\nhousehold |&gt; \n  pivot_longer(\n    cols = !family, \n    names_to = c(\".value\", \"child\"), \n    names_sep = \"_\", \n    values_drop_na = TRUE\n  )\n\n# A tibble: 9 × 4\n  family child  dob        name  \n   &lt;int&gt; &lt;chr&gt;  &lt;date&gt;     &lt;chr&gt; \n1      1 child1 1998-11-26 Susan \n2      1 child2 2000-01-29 Jose  \n3      2 child1 1996-06-22 Mark  \n4      3 child1 2002-07-11 Sam   \n5      3 child2 2004-04-05 Seth  \n6      4 child1 2004-10-10 Craig \n7      4 child2 2009-08-27 Khai  \n8      5 child1 2000-12-05 Parker\n9      5 child2 2005-02-28 Gracie\n\n#&gt; # A tibble: 9 × 4\n#&gt;   family child  dob        name \n#&gt;    &lt;int&gt; &lt;chr&gt;  &lt;date&gt;     &lt;chr&gt;\n#&gt; 1      1 child1 1998-11-26 Susan\n#&gt; 2      1 child2 2000-01-29 Jose \n#&gt; 3      2 child1 1996-06-22 Mark \n#&gt; 4      3 child1 2002-07-11 Sam  \n#&gt; 5      3 child2 2004-04-05 Seth \n#&gt; 6      4 child1 2004-10-10 Craig\n#&gt; # ℹ 3 more rows\n\n\n\n\nUse pivot_wider() to increase columns and reduce rows.\n\ncms_patient_experience |&gt; \n  distinct(measure_cd, measure_title)\n\n# A tibble: 6 × 2\n  measure_cd   measure_title                                                    \n  &lt;chr&gt;        &lt;chr&gt;                                                            \n1 CAHPS_GRP_1  CAHPS for MIPS SSM: Getting Timely Care, Appointments, and Infor…\n2 CAHPS_GRP_2  CAHPS for MIPS SSM: How Well Providers Communicate               \n3 CAHPS_GRP_3  CAHPS for MIPS SSM: Patient's Rating of Provider                 \n4 CAHPS_GRP_5  CAHPS for MIPS SSM: Health Promotion and Education               \n5 CAHPS_GRP_8  CAHPS for MIPS SSM: Courteous and Helpful Office Staff           \n6 CAHPS_GRP_12 CAHPS for MIPS SSM: Stewardship of Patient Resources             \n\n#&gt; # A tibble: 6 × 2\n#&gt;   measure_cd   measure_title                                                 \n#&gt;   &lt;chr&gt;        &lt;chr&gt;                                                         \n#&gt; 1 CAHPS_GRP_1  CAHPS for MIPS SSM: Getting Timely Care, Appointments, and In…\n#&gt; 2 CAHPS_GRP_2  CAHPS for MIPS SSM: How Well Providers Communicate            \n#&gt; 3 CAHPS_GRP_3  CAHPS for MIPS SSM: Patient's Rating of Provider              \n#&gt; 4 CAHPS_GRP_5  CAHPS for MIPS SSM: Health Promotion and Education            \n#&gt; 5 CAHPS_GRP_8  CAHPS for MIPS SSM: Courteous and Helpful Office Staff        \n#&gt; 6 CAHPS_GRP_12 CAHPS for MIPS SSM: Stewardship of Patient Resources\n\npivot_wider() has the opposite interface to pivot_longer(): instead of choosing new column names, we need to provide the existing columns that define the values (values_from) and the column name (names_from):\n\ncms_patient_experience |&gt; \n  pivot_wider(\n    names_from = measure_cd,\n    values_from = prf_rate\n  )\n\n# A tibble: 500 × 9\n   org_pac_id org_nm           measure_title CAHPS_GRP_1 CAHPS_GRP_2 CAHPS_GRP_3\n   &lt;chr&gt;      &lt;chr&gt;            &lt;chr&gt;               &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1 0446157747 USC CARE MEDICA… CAHPS for MI…          63          NA          NA\n 2 0446157747 USC CARE MEDICA… CAHPS for MI…          NA          87          NA\n 3 0446157747 USC CARE MEDICA… CAHPS for MI…          NA          NA          86\n 4 0446157747 USC CARE MEDICA… CAHPS for MI…          NA          NA          NA\n 5 0446157747 USC CARE MEDICA… CAHPS for MI…          NA          NA          NA\n 6 0446157747 USC CARE MEDICA… CAHPS for MI…          NA          NA          NA\n 7 0446162697 ASSOCIATION OF … CAHPS for MI…          59          NA          NA\n 8 0446162697 ASSOCIATION OF … CAHPS for MI…          NA          85          NA\n 9 0446162697 ASSOCIATION OF … CAHPS for MI…          NA          NA          83\n10 0446162697 ASSOCIATION OF … CAHPS for MI…          NA          NA          NA\n# ℹ 490 more rows\n# ℹ 3 more variables: CAHPS_GRP_5 &lt;dbl&gt;, CAHPS_GRP_8 &lt;dbl&gt;, CAHPS_GRP_12 &lt;dbl&gt;\n\n#&gt; # A tibble: 500 × 9\n#&gt;   org_pac_id org_nm                   measure_title   CAHPS_GRP_1 CAHPS_GRP_2\n#&gt;   &lt;chr&gt;      &lt;chr&gt;                    &lt;chr&gt;                 &lt;dbl&gt;       &lt;dbl&gt;\n#&gt; 1 0446157747 USC CARE MEDICAL GROUP … CAHPS for MIPS…          63          NA\n#&gt; 2 0446157747 USC CARE MEDICAL GROUP … CAHPS for MIPS…          NA          87\n#&gt; 3 0446157747 USC CARE MEDICAL GROUP … CAHPS for MIPS…          NA          NA\n#&gt; 4 0446157747 USC CARE MEDICAL GROUP … CAHPS for MIPS…          NA          NA\n#&gt; 5 0446157747 USC CARE MEDICAL GROUP … CAHPS for MIPS…          NA          NA\n#&gt; 6 0446157747 USC CARE MEDICAL GROUP … CAHPS for MIPS…          NA          NA\n#&gt; # ℹ 494 more rows\n#&gt; # ℹ 4 more variables: CAHPS_GRP_3 &lt;dbl&gt;, CAHPS_GRP_5 &lt;dbl&gt;, …\n\nIf you still have multiple rows per unit of interest, organization in this case, then the pivot_wider() didn’t work.\nThe problem is we need to tell pivot_wider() which column or columns have values that uniquely identify each row, in this case those are the variables starting with \"org\":\n\ncms_patient_experience |&gt; \n  pivot_wider(\n    id_cols = starts_with(\"org\"),\n    names_from = measure_cd,\n    values_from = prf_rate\n  )\n\n# A tibble: 95 × 8\n   org_pac_id org_nm CAHPS_GRP_1 CAHPS_GRP_2 CAHPS_GRP_3 CAHPS_GRP_5 CAHPS_GRP_8\n   &lt;chr&gt;      &lt;chr&gt;        &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1 0446157747 USC C…          63          87          86          57          85\n 2 0446162697 ASSOC…          59          85          83          63          88\n 3 0547164295 BEAVE…          49          NA          75          44          73\n 4 0749333730 CAPE …          67          84          85          65          82\n 5 0840104360 ALLIA…          66          87          87          64          87\n 6 0840109864 REX H…          73          87          84          67          91\n 7 0840513552 SCL H…          58          83          76          58          78\n 8 0941545784 GRITM…          46          86          81          54          NA\n 9 1052612785 COMMU…          65          84          80          58          87\n10 1254237779 OUR L…          61          NA          NA          65          NA\n# ℹ 85 more rows\n# ℹ 1 more variable: CAHPS_GRP_12 &lt;dbl&gt;\n\n#&gt; # A tibble: 95 × 8\n#&gt;   org_pac_id org_nm           CAHPS_GRP_1 CAHPS_GRP_2 CAHPS_GRP_3 CAHPS_GRP_5\n#&gt;   &lt;chr&gt;      &lt;chr&gt;                  &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n#&gt; 1 0446157747 USC CARE MEDICA…          63          87          86          57\n#&gt; 2 0446162697 ASSOCIATION OF …          59          85          83          63\n#&gt; 3 0547164295 BEAVER MEDICAL …          49          NA          75          44\n#&gt; 4 0749333730 CAPE PHYSICIANS…          67          84          85          65\n#&gt; 5 0840104360 ALLIANCE PHYSIC…          66          87          87          64\n#&gt; 6 0840109864 REX HOSPITAL INC          73          87          84          67\n#&gt; # ℹ 89 more rows\n#&gt; # ℹ 2 more variables: CAHPS_GRP_8 &lt;dbl&gt;, CAHPS_GRP_12 &lt;dbl&gt;\n\nHow does pivot_wider() work?\n\ndf &lt;- tribble(\n  ~id, ~measurement, ~value,\n  \"A\",        \"bp1\",    100,\n  \"B\",        \"bp1\",    140,\n  \"B\",        \"bp2\",    115, \n  \"A\",        \"bp2\",    120,\n  \"A\",        \"bp3\",    105\n)\n\nTake the values from the value column and the column names from the measurement column.\n\ndf |&gt; \n  pivot_wider(\n    names_from = measurement,\n    values_from = value\n  )\n\n# A tibble: 2 × 4\n  id      bp1   bp2   bp3\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 A       100   120   105\n2 B       140   115    NA\n\n#&gt; # A tibble: 2 × 4\n#&gt;   id      bp1   bp2   bp3\n#&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 A       100   120   105\n#&gt; 2 B       140   115    NA\n\nAny column that does not go into the names_from or values_from argument determines the number of rows. These are called the id_cols.\nWhat if we have duplicate values in the column giving the new column names and in the column giving the new value names? We get list-columns.\n\ndf &lt;- tribble(\n  ~id, ~measurement, ~value,\n  \"A\",        \"bp1\",    100,\n  \"A\",        \"bp1\",    102,\n  \"A\",        \"bp2\",    120,\n  \"B\",        \"bp1\",    140, \n  \"B\",        \"bp2\",    115\n)\n\n\ndf |&gt;\n  pivot_wider(\n    names_from = measurement,\n    values_from = value\n  )\n\nWarning: Values from `value` are not uniquely identified; output will contain list-cols.\n• Use `values_fn = list` to suppress this warning.\n• Use `values_fn = {summary_fun}` to summarise duplicates.\n• Use the following dplyr code to identify duplicates.\n  {data} |&gt;\n  dplyr::summarise(n = dplyr::n(), .by = c(id, measurement)) |&gt;\n  dplyr::filter(n &gt; 1L)\n\n\n# A tibble: 2 × 3\n  id    bp1       bp2      \n  &lt;chr&gt; &lt;list&gt;    &lt;list&gt;   \n1 A     &lt;dbl [2]&gt; &lt;dbl [1]&gt;\n2 B     &lt;dbl [1]&gt; &lt;dbl [1]&gt;\n\n#&gt; Warning: Values from `value` are not uniquely identified; output will contain\n#&gt; list-cols.\n#&gt; • Use `values_fn = list` to suppress this warning.\n#&gt; • Use `values_fn = {summary_fun}` to summarise duplicates.\n#&gt; • Use the following dplyr code to identify duplicates.\n#&gt;   {data} |&gt;\n#&gt;   dplyr::summarise(n = dplyr::n(), .by = c(id, measurement)) |&gt;\n#&gt;   dplyr::filter(n &gt; 1L)\n#&gt; # A tibble: 2 × 3\n#&gt;   id    bp1       bp2      \n#&gt;   &lt;chr&gt; &lt;list&gt;    &lt;list&gt;   \n#&gt; 1 A     &lt;dbl [2]&gt; &lt;dbl [1]&gt;\n#&gt; 2 B     &lt;dbl [1]&gt; &lt;dbl [1]&gt;\n\nFollow the hint in the warning to locate the duplicate:\n\ndf |&gt; \n  group_by(id, measurement) |&gt; \n  summarize(n = n(), .groups = \"drop\") |&gt; \n  filter(n &gt; 1)\n\n# A tibble: 1 × 3\n  id    measurement     n\n  &lt;chr&gt; &lt;chr&gt;       &lt;int&gt;\n1 A     bp1             2\n\n#&gt; # A tibble: 1 × 3\n#&gt;   id    measurement     n\n#&gt;   &lt;chr&gt; &lt;chr&gt;       &lt;int&gt;\n#&gt; 1 A     bp1             2\n\n\ndf\n\n# A tibble: 5 × 3\n  id    measurement value\n  &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;\n1 A     bp1           100\n2 A     bp1           102\n3 A     bp2           120\n4 B     bp1           140\n5 B     bp2           115"
  },
  {
    "objectID": "chapter_05_notes.html#lengthening-data",
    "href": "chapter_05_notes.html#lengthening-data",
    "title": "R for Data Science, 2nd Edition - Chapter 5 Notes",
    "section": "",
    "text": "Use pivot_longer().\n\nbillboard |&gt; \n  pivot_longer(\n    cols = starts_with(\"wk\"), \n    names_to = \"week\", \n    values_to = \"rank\"\n  )\n\n# A tibble: 24,092 × 5\n   artist track                   date.entered week   rank\n   &lt;chr&gt;  &lt;chr&gt;                   &lt;date&gt;       &lt;chr&gt; &lt;dbl&gt;\n 1 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk1      87\n 2 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk2      82\n 3 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk3      72\n 4 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk4      77\n 5 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk5      87\n 6 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk6      94\n 7 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk7      99\n 8 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk8      NA\n 9 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk9      NA\n10 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk10     NA\n# ℹ 24,082 more rows\n\n#&gt; # A tibble: 24,092 × 5\n#&gt;    artist track                   date.entered week   rank\n#&gt;    &lt;chr&gt;  &lt;chr&gt;                   &lt;date&gt;       &lt;chr&gt; &lt;dbl&gt;\n#&gt;  1 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk1      87\n#&gt;  2 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk2      82\n#&gt;  3 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk3      72\n#&gt;  4 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk4      77\n#&gt;  5 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk5      87\n#&gt;  6 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk6      94\n#&gt;  7 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk7      99\n#&gt;  8 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk8      NA\n#&gt;  9 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk9      NA\n#&gt; 10 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk10     NA\n#&gt; # ℹ 24,082 more rows\n\n\ncols specifies which columns need to be pivoted, i.e. which columns aren’t variables. This argument uses the same syntax as select() so here we could use !c(artist, track, date.entered) or starts_with(\"wk\").\nnames_to names the variable stored in the column names, we named that variable week.\nvalues_to names the variable stored in the cell values, we named that variable rank.\n\nSo pivot_longer() starts with identifying which columns are variables, and which are not. For those columns that are not variables, we create a single variable capturing the names of these non-variable columns, and another variable capturing the values in the cells of those non-variable columns, which we must also name.\nSome NA values may be generated by the structure of the data set. This can be avoided with values_drop_na = TRUE.\n\nbillboard |&gt; \n  pivot_longer(\n    cols = starts_with(\"wk\"), \n    names_to = \"week\", \n    values_to = \"rank\",\n    values_drop_na = TRUE\n  )\n\n# A tibble: 5,307 × 5\n   artist  track                   date.entered week   rank\n   &lt;chr&gt;   &lt;chr&gt;                   &lt;date&gt;       &lt;chr&gt; &lt;dbl&gt;\n 1 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk1      87\n 2 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk2      82\n 3 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk3      72\n 4 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk4      77\n 5 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk5      87\n 6 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk6      94\n 7 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk7      99\n 8 2Ge+her The Hardest Part Of ... 2000-09-02   wk1      91\n 9 2Ge+her The Hardest Part Of ... 2000-09-02   wk2      87\n10 2Ge+her The Hardest Part Of ... 2000-09-02   wk3      92\n# ℹ 5,297 more rows\n\n#&gt; # A tibble: 5,307 × 5\n#&gt;   artist track                   date.entered week   rank\n#&gt;   &lt;chr&gt;  &lt;chr&gt;                   &lt;date&gt;       &lt;chr&gt; &lt;dbl&gt;\n#&gt; 1 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk1      87\n#&gt; 2 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk2      82\n#&gt; 3 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk3      72\n#&gt; 4 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk4      77\n#&gt; 5 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk5      87\n#&gt; 6 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk6      94\n#&gt; # ℹ 5,301 more rows\n\nThe values for week are characters. This is a good opportunity to use parse_number().\n\nbillboard_longer &lt;- billboard |&gt; \n  pivot_longer(\n    cols = starts_with(\"wk\"), \n    names_to = \"week\", \n    values_to = \"rank\",\n    values_drop_na = TRUE\n  ) |&gt; \n  mutate(\n    week = parse_number(week)\n  )\nbillboard_longer\n\n# A tibble: 5,307 × 5\n   artist  track                   date.entered  week  rank\n   &lt;chr&gt;   &lt;chr&gt;                   &lt;date&gt;       &lt;dbl&gt; &lt;dbl&gt;\n 1 2 Pac   Baby Don't Cry (Keep... 2000-02-26       1    87\n 2 2 Pac   Baby Don't Cry (Keep... 2000-02-26       2    82\n 3 2 Pac   Baby Don't Cry (Keep... 2000-02-26       3    72\n 4 2 Pac   Baby Don't Cry (Keep... 2000-02-26       4    77\n 5 2 Pac   Baby Don't Cry (Keep... 2000-02-26       5    87\n 6 2 Pac   Baby Don't Cry (Keep... 2000-02-26       6    94\n 7 2 Pac   Baby Don't Cry (Keep... 2000-02-26       7    99\n 8 2Ge+her The Hardest Part Of ... 2000-09-02       1    91\n 9 2Ge+her The Hardest Part Of ... 2000-09-02       2    87\n10 2Ge+her The Hardest Part Of ... 2000-09-02       3    92\n# ℹ 5,297 more rows\n\n#&gt; # A tibble: 5,307 × 5\n#&gt;   artist track                   date.entered  week  rank\n#&gt;   &lt;chr&gt;  &lt;chr&gt;                   &lt;date&gt;       &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 2 Pac  Baby Don't Cry (Keep... 2000-02-26       1    87\n#&gt; 2 2 Pac  Baby Don't Cry (Keep... 2000-02-26       2    82\n#&gt; 3 2 Pac  Baby Don't Cry (Keep... 2000-02-26       3    72\n#&gt; 4 2 Pac  Baby Don't Cry (Keep... 2000-02-26       4    77\n#&gt; 5 2 Pac  Baby Don't Cry (Keep... 2000-02-26       5    87\n#&gt; 6 2 Pac  Baby Don't Cry (Keep... 2000-02-26       6    94\n#&gt; # ℹ 5,301 more rows\n\nExamining pivot_longer()\n\ndf &lt;- tribble(\n  ~id,  ~bp1, ~bp2,\n   \"A\",  100,  120,\n   \"B\",  140,  115,\n   \"C\",  120,  125\n)\n\ndf |&gt; \n  pivot_longer(\n    cols = bp1:bp2,\n    names_to = \"measurement\",\n    values_to = \"value\"\n  )\n\n# A tibble: 6 × 3\n  id    measurement value\n  &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;\n1 A     bp1           100\n2 A     bp2           120\n3 B     bp1           140\n4 B     bp2           115\n5 C     bp1           120\n6 C     bp2           125\n\n#&gt; # A tibble: 6 × 3\n#&gt;   id    measurement value\n#&gt;   &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;\n#&gt; 1 A     bp1           100\n#&gt; 2 A     bp2           120\n#&gt; 3 B     bp1           140\n#&gt; 4 B     bp2           115\n#&gt; 5 C     bp1           120\n#&gt; 6 C     bp2           125\n\nWhat if column names contain multiple pieces of information?\n\nwho2\n\n# A tibble: 7,240 × 58\n   country      year sp_m_014 sp_m_1524 sp_m_2534 sp_m_3544 sp_m_4554 sp_m_5564\n   &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 Afghanistan  1980       NA        NA        NA        NA        NA        NA\n 2 Afghanistan  1981       NA        NA        NA        NA        NA        NA\n 3 Afghanistan  1982       NA        NA        NA        NA        NA        NA\n 4 Afghanistan  1983       NA        NA        NA        NA        NA        NA\n 5 Afghanistan  1984       NA        NA        NA        NA        NA        NA\n 6 Afghanistan  1985       NA        NA        NA        NA        NA        NA\n 7 Afghanistan  1986       NA        NA        NA        NA        NA        NA\n 8 Afghanistan  1987       NA        NA        NA        NA        NA        NA\n 9 Afghanistan  1988       NA        NA        NA        NA        NA        NA\n10 Afghanistan  1989       NA        NA        NA        NA        NA        NA\n# ℹ 7,230 more rows\n# ℹ 50 more variables: sp_m_65 &lt;dbl&gt;, sp_f_014 &lt;dbl&gt;, sp_f_1524 &lt;dbl&gt;,\n#   sp_f_2534 &lt;dbl&gt;, sp_f_3544 &lt;dbl&gt;, sp_f_4554 &lt;dbl&gt;, sp_f_5564 &lt;dbl&gt;,\n#   sp_f_65 &lt;dbl&gt;, sn_m_014 &lt;dbl&gt;, sn_m_1524 &lt;dbl&gt;, sn_m_2534 &lt;dbl&gt;,\n#   sn_m_3544 &lt;dbl&gt;, sn_m_4554 &lt;dbl&gt;, sn_m_5564 &lt;dbl&gt;, sn_m_65 &lt;dbl&gt;,\n#   sn_f_014 &lt;dbl&gt;, sn_f_1524 &lt;dbl&gt;, sn_f_2534 &lt;dbl&gt;, sn_f_3544 &lt;dbl&gt;,\n#   sn_f_4554 &lt;dbl&gt;, sn_f_5564 &lt;dbl&gt;, sn_f_65 &lt;dbl&gt;, ep_m_014 &lt;dbl&gt;, …\n\n#&gt; # A tibble: 7,240 × 58\n#&gt;   country      year sp_m_014 sp_m_1524 sp_m_2534 sp_m_3544 sp_m_4554\n#&gt;   &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 Afghanistan  1980       NA        NA        NA        NA        NA\n#&gt; 2 Afghanistan  1981       NA        NA        NA        NA        NA\n#&gt; 3 Afghanistan  1982       NA        NA        NA        NA        NA\n#&gt; 4 Afghanistan  1983       NA        NA        NA        NA        NA\n#&gt; 5 Afghanistan  1984       NA        NA        NA        NA        NA\n#&gt; 6 Afghanistan  1985       NA        NA        NA        NA        NA\n#&gt; # ℹ 7,234 more rows\n#&gt; # ℹ 51 more variables: sp_m_5564 &lt;dbl&gt;, sp_m_65 &lt;dbl&gt;, sp_f_014 &lt;dbl&gt;, …\n\nSo in this case we have six pieces of information recorded in who2: the country and the year (already columns); the method of diagnosis, the gender category, and the age range category (contained in the other column names); and the count of patients in that category (cell values).\n\nwho2 |&gt; \n  pivot_longer(\n    cols = !(country:year),\n    names_to = c(\"diagnosis\", \"gender\", \"age\"), \n    names_sep = \"_\",\n    values_to = \"count\"\n  )\n\n# A tibble: 405,440 × 6\n   country      year diagnosis gender age   count\n   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt;\n 1 Afghanistan  1980 sp        m      014      NA\n 2 Afghanistan  1980 sp        m      1524     NA\n 3 Afghanistan  1980 sp        m      2534     NA\n 4 Afghanistan  1980 sp        m      3544     NA\n 5 Afghanistan  1980 sp        m      4554     NA\n 6 Afghanistan  1980 sp        m      5564     NA\n 7 Afghanistan  1980 sp        m      65       NA\n 8 Afghanistan  1980 sp        f      014      NA\n 9 Afghanistan  1980 sp        f      1524     NA\n10 Afghanistan  1980 sp        f      2534     NA\n# ℹ 405,430 more rows\n\n#&gt; # A tibble: 405,440 × 6\n#&gt;   country      year diagnosis gender age   count\n#&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt;\n#&gt; 1 Afghanistan  1980 sp        m      014      NA\n#&gt; 2 Afghanistan  1980 sp        m      1524     NA\n#&gt; 3 Afghanistan  1980 sp        m      2534     NA\n#&gt; 4 Afghanistan  1980 sp        m      3544     NA\n#&gt; 5 Afghanistan  1980 sp        m      4554     NA\n#&gt; 6 Afghanistan  1980 sp        m      5564     NA\n#&gt; # ℹ 405,434 more rows\n\nWhat if the column names include a mix of of variable values and variable names?\n\nhousehold\n\n# A tibble: 5 × 5\n  family dob_child1 dob_child2 name_child1 name_child2\n   &lt;int&gt; &lt;date&gt;     &lt;date&gt;     &lt;chr&gt;       &lt;chr&gt;      \n1      1 1998-11-26 2000-01-29 Susan       Jose       \n2      2 1996-06-22 NA         Mark        &lt;NA&gt;       \n3      3 2002-07-11 2004-04-05 Sam         Seth       \n4      4 2004-10-10 2009-08-27 Craig       Khai       \n5      5 2000-12-05 2005-02-28 Parker      Gracie     \n\n\nUse the .value sentinel value to indicate the first component of the pivoted column name as a variable name in the output. Note the lack of a values_to argument here. When you use .value in names_to, the column names in the input contribute to both values and variable names in the output.\n\nhousehold |&gt; \n  pivot_longer(\n    cols = !family, \n    names_to = c(\".value\", \"child\"), \n    names_sep = \"_\", \n    values_drop_na = TRUE\n  )\n\n# A tibble: 9 × 4\n  family child  dob        name  \n   &lt;int&gt; &lt;chr&gt;  &lt;date&gt;     &lt;chr&gt; \n1      1 child1 1998-11-26 Susan \n2      1 child2 2000-01-29 Jose  \n3      2 child1 1996-06-22 Mark  \n4      3 child1 2002-07-11 Sam   \n5      3 child2 2004-04-05 Seth  \n6      4 child1 2004-10-10 Craig \n7      4 child2 2009-08-27 Khai  \n8      5 child1 2000-12-05 Parker\n9      5 child2 2005-02-28 Gracie\n\n#&gt; # A tibble: 9 × 4\n#&gt;   family child  dob        name \n#&gt;    &lt;int&gt; &lt;chr&gt;  &lt;date&gt;     &lt;chr&gt;\n#&gt; 1      1 child1 1998-11-26 Susan\n#&gt; 2      1 child2 2000-01-29 Jose \n#&gt; 3      2 child1 1996-06-22 Mark \n#&gt; 4      3 child1 2002-07-11 Sam  \n#&gt; 5      3 child2 2004-04-05 Seth \n#&gt; 6      4 child1 2004-10-10 Craig\n#&gt; # ℹ 3 more rows"
  },
  {
    "objectID": "chapter_05_notes.html#widening-data",
    "href": "chapter_05_notes.html#widening-data",
    "title": "R for Data Science, 2nd Edition - Chapter 5 Notes",
    "section": "",
    "text": "Use pivot_wider() to increase columns and reduce rows.\n\ncms_patient_experience |&gt; \n  distinct(measure_cd, measure_title)\n\n# A tibble: 6 × 2\n  measure_cd   measure_title                                                    \n  &lt;chr&gt;        &lt;chr&gt;                                                            \n1 CAHPS_GRP_1  CAHPS for MIPS SSM: Getting Timely Care, Appointments, and Infor…\n2 CAHPS_GRP_2  CAHPS for MIPS SSM: How Well Providers Communicate               \n3 CAHPS_GRP_3  CAHPS for MIPS SSM: Patient's Rating of Provider                 \n4 CAHPS_GRP_5  CAHPS for MIPS SSM: Health Promotion and Education               \n5 CAHPS_GRP_8  CAHPS for MIPS SSM: Courteous and Helpful Office Staff           \n6 CAHPS_GRP_12 CAHPS for MIPS SSM: Stewardship of Patient Resources             \n\n#&gt; # A tibble: 6 × 2\n#&gt;   measure_cd   measure_title                                                 \n#&gt;   &lt;chr&gt;        &lt;chr&gt;                                                         \n#&gt; 1 CAHPS_GRP_1  CAHPS for MIPS SSM: Getting Timely Care, Appointments, and In…\n#&gt; 2 CAHPS_GRP_2  CAHPS for MIPS SSM: How Well Providers Communicate            \n#&gt; 3 CAHPS_GRP_3  CAHPS for MIPS SSM: Patient's Rating of Provider              \n#&gt; 4 CAHPS_GRP_5  CAHPS for MIPS SSM: Health Promotion and Education            \n#&gt; 5 CAHPS_GRP_8  CAHPS for MIPS SSM: Courteous and Helpful Office Staff        \n#&gt; 6 CAHPS_GRP_12 CAHPS for MIPS SSM: Stewardship of Patient Resources\n\npivot_wider() has the opposite interface to pivot_longer(): instead of choosing new column names, we need to provide the existing columns that define the values (values_from) and the column name (names_from):\n\ncms_patient_experience |&gt; \n  pivot_wider(\n    names_from = measure_cd,\n    values_from = prf_rate\n  )\n\n# A tibble: 500 × 9\n   org_pac_id org_nm           measure_title CAHPS_GRP_1 CAHPS_GRP_2 CAHPS_GRP_3\n   &lt;chr&gt;      &lt;chr&gt;            &lt;chr&gt;               &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1 0446157747 USC CARE MEDICA… CAHPS for MI…          63          NA          NA\n 2 0446157747 USC CARE MEDICA… CAHPS for MI…          NA          87          NA\n 3 0446157747 USC CARE MEDICA… CAHPS for MI…          NA          NA          86\n 4 0446157747 USC CARE MEDICA… CAHPS for MI…          NA          NA          NA\n 5 0446157747 USC CARE MEDICA… CAHPS for MI…          NA          NA          NA\n 6 0446157747 USC CARE MEDICA… CAHPS for MI…          NA          NA          NA\n 7 0446162697 ASSOCIATION OF … CAHPS for MI…          59          NA          NA\n 8 0446162697 ASSOCIATION OF … CAHPS for MI…          NA          85          NA\n 9 0446162697 ASSOCIATION OF … CAHPS for MI…          NA          NA          83\n10 0446162697 ASSOCIATION OF … CAHPS for MI…          NA          NA          NA\n# ℹ 490 more rows\n# ℹ 3 more variables: CAHPS_GRP_5 &lt;dbl&gt;, CAHPS_GRP_8 &lt;dbl&gt;, CAHPS_GRP_12 &lt;dbl&gt;\n\n#&gt; # A tibble: 500 × 9\n#&gt;   org_pac_id org_nm                   measure_title   CAHPS_GRP_1 CAHPS_GRP_2\n#&gt;   &lt;chr&gt;      &lt;chr&gt;                    &lt;chr&gt;                 &lt;dbl&gt;       &lt;dbl&gt;\n#&gt; 1 0446157747 USC CARE MEDICAL GROUP … CAHPS for MIPS…          63          NA\n#&gt; 2 0446157747 USC CARE MEDICAL GROUP … CAHPS for MIPS…          NA          87\n#&gt; 3 0446157747 USC CARE MEDICAL GROUP … CAHPS for MIPS…          NA          NA\n#&gt; 4 0446157747 USC CARE MEDICAL GROUP … CAHPS for MIPS…          NA          NA\n#&gt; 5 0446157747 USC CARE MEDICAL GROUP … CAHPS for MIPS…          NA          NA\n#&gt; 6 0446157747 USC CARE MEDICAL GROUP … CAHPS for MIPS…          NA          NA\n#&gt; # ℹ 494 more rows\n#&gt; # ℹ 4 more variables: CAHPS_GRP_3 &lt;dbl&gt;, CAHPS_GRP_5 &lt;dbl&gt;, …\n\nIf you still have multiple rows per unit of interest, organization in this case, then the pivot_wider() didn’t work.\nThe problem is we need to tell pivot_wider() which column or columns have values that uniquely identify each row, in this case those are the variables starting with \"org\":\n\ncms_patient_experience |&gt; \n  pivot_wider(\n    id_cols = starts_with(\"org\"),\n    names_from = measure_cd,\n    values_from = prf_rate\n  )\n\n# A tibble: 95 × 8\n   org_pac_id org_nm CAHPS_GRP_1 CAHPS_GRP_2 CAHPS_GRP_3 CAHPS_GRP_5 CAHPS_GRP_8\n   &lt;chr&gt;      &lt;chr&gt;        &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1 0446157747 USC C…          63          87          86          57          85\n 2 0446162697 ASSOC…          59          85          83          63          88\n 3 0547164295 BEAVE…          49          NA          75          44          73\n 4 0749333730 CAPE …          67          84          85          65          82\n 5 0840104360 ALLIA…          66          87          87          64          87\n 6 0840109864 REX H…          73          87          84          67          91\n 7 0840513552 SCL H…          58          83          76          58          78\n 8 0941545784 GRITM…          46          86          81          54          NA\n 9 1052612785 COMMU…          65          84          80          58          87\n10 1254237779 OUR L…          61          NA          NA          65          NA\n# ℹ 85 more rows\n# ℹ 1 more variable: CAHPS_GRP_12 &lt;dbl&gt;\n\n#&gt; # A tibble: 95 × 8\n#&gt;   org_pac_id org_nm           CAHPS_GRP_1 CAHPS_GRP_2 CAHPS_GRP_3 CAHPS_GRP_5\n#&gt;   &lt;chr&gt;      &lt;chr&gt;                  &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n#&gt; 1 0446157747 USC CARE MEDICA…          63          87          86          57\n#&gt; 2 0446162697 ASSOCIATION OF …          59          85          83          63\n#&gt; 3 0547164295 BEAVER MEDICAL …          49          NA          75          44\n#&gt; 4 0749333730 CAPE PHYSICIANS…          67          84          85          65\n#&gt; 5 0840104360 ALLIANCE PHYSIC…          66          87          87          64\n#&gt; 6 0840109864 REX HOSPITAL INC          73          87          84          67\n#&gt; # ℹ 89 more rows\n#&gt; # ℹ 2 more variables: CAHPS_GRP_8 &lt;dbl&gt;, CAHPS_GRP_12 &lt;dbl&gt;\n\nHow does pivot_wider() work?\n\ndf &lt;- tribble(\n  ~id, ~measurement, ~value,\n  \"A\",        \"bp1\",    100,\n  \"B\",        \"bp1\",    140,\n  \"B\",        \"bp2\",    115, \n  \"A\",        \"bp2\",    120,\n  \"A\",        \"bp3\",    105\n)\n\nTake the values from the value column and the column names from the measurement column.\n\ndf |&gt; \n  pivot_wider(\n    names_from = measurement,\n    values_from = value\n  )\n\n# A tibble: 2 × 4\n  id      bp1   bp2   bp3\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 A       100   120   105\n2 B       140   115    NA\n\n#&gt; # A tibble: 2 × 4\n#&gt;   id      bp1   bp2   bp3\n#&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 A       100   120   105\n#&gt; 2 B       140   115    NA\n\nAny column that does not go into the names_from or values_from argument determines the number of rows. These are called the id_cols.\nWhat if we have duplicate values in the column giving the new column names and in the column giving the new value names? We get list-columns.\n\ndf &lt;- tribble(\n  ~id, ~measurement, ~value,\n  \"A\",        \"bp1\",    100,\n  \"A\",        \"bp1\",    102,\n  \"A\",        \"bp2\",    120,\n  \"B\",        \"bp1\",    140, \n  \"B\",        \"bp2\",    115\n)\n\n\ndf |&gt;\n  pivot_wider(\n    names_from = measurement,\n    values_from = value\n  )\n\nWarning: Values from `value` are not uniquely identified; output will contain list-cols.\n• Use `values_fn = list` to suppress this warning.\n• Use `values_fn = {summary_fun}` to summarise duplicates.\n• Use the following dplyr code to identify duplicates.\n  {data} |&gt;\n  dplyr::summarise(n = dplyr::n(), .by = c(id, measurement)) |&gt;\n  dplyr::filter(n &gt; 1L)\n\n\n# A tibble: 2 × 3\n  id    bp1       bp2      \n  &lt;chr&gt; &lt;list&gt;    &lt;list&gt;   \n1 A     &lt;dbl [2]&gt; &lt;dbl [1]&gt;\n2 B     &lt;dbl [1]&gt; &lt;dbl [1]&gt;\n\n#&gt; Warning: Values from `value` are not uniquely identified; output will contain\n#&gt; list-cols.\n#&gt; • Use `values_fn = list` to suppress this warning.\n#&gt; • Use `values_fn = {summary_fun}` to summarise duplicates.\n#&gt; • Use the following dplyr code to identify duplicates.\n#&gt;   {data} |&gt;\n#&gt;   dplyr::summarise(n = dplyr::n(), .by = c(id, measurement)) |&gt;\n#&gt;   dplyr::filter(n &gt; 1L)\n#&gt; # A tibble: 2 × 3\n#&gt;   id    bp1       bp2      \n#&gt;   &lt;chr&gt; &lt;list&gt;    &lt;list&gt;   \n#&gt; 1 A     &lt;dbl [2]&gt; &lt;dbl [1]&gt;\n#&gt; 2 B     &lt;dbl [1]&gt; &lt;dbl [1]&gt;\n\nFollow the hint in the warning to locate the duplicate:\n\ndf |&gt; \n  group_by(id, measurement) |&gt; \n  summarize(n = n(), .groups = \"drop\") |&gt; \n  filter(n &gt; 1)\n\n# A tibble: 1 × 3\n  id    measurement     n\n  &lt;chr&gt; &lt;chr&gt;       &lt;int&gt;\n1 A     bp1             2\n\n#&gt; # A tibble: 1 × 3\n#&gt;   id    measurement     n\n#&gt;   &lt;chr&gt; &lt;chr&gt;       &lt;int&gt;\n#&gt; 1 A     bp1             2\n\n\ndf\n\n# A tibble: 5 × 3\n  id    measurement value\n  &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;\n1 A     bp1           100\n2 A     bp1           102\n3 A     bp2           120\n4 B     bp1           140\n5 B     bp2           115"
  },
  {
    "objectID": "chapter_09_notes.html",
    "href": "chapter_09_notes.html",
    "title": "R for Data Science, 2nd Edition - Chapter 9 Notes",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.6\n✔ forcats   1.0.1     ✔ stringr   1.6.0\n✔ ggplot2   4.0.1     ✔ tibble    3.3.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.2\n✔ purrr     1.2.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\n\n# Left\nggplot(mpg, aes(x = displ, y = hwy, color = class)) +\n  geom_point()\n\n\n\n\n\n\n\n# Right\nggplot(mpg, aes(x = displ, y = hwy, shape = class)) +\n  geom_point()\n\nWarning: The shape palette can deal with a maximum of 6 discrete values because more\nthan 6 becomes difficult to discriminate\nℹ you have requested 7 values. Consider specifying shapes manually if you need\n  that many of them.\n\n\nWarning: Removed 62 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n#&gt; Warning: The shape palette can deal with a maximum of 6 discrete values because more\n#&gt; than 6 becomes difficult to discriminate\n#&gt; ℹ you have requested 7 values. Consider specifying shapes manually if you\n#&gt;   need that many of them.\n#&gt; Warning: Removed 62 rows containing missing values or values outside the scale range\n#&gt; (`geom_point()`).\n\n\n# Left\nggplot(mpg, aes(x = displ, y = hwy, size = class)) +\n  geom_point()\n\nWarning: Using size for a discrete variable is not advised.\n\n\n\n\n\n\n\n\n#&gt; Warning: Using size for a discrete variable is not advised.\n\n# Right\nggplot(mpg, aes(x = displ, y = hwy, alpha = class)) +\n  geom_point()\n\nWarning: Using alpha for a discrete variable is not advised.\n\n\n\n\n\n\n\n\n#&gt; Warning: Using alpha for a discrete variable is not advised.\n\nArguments outside of aes() do not map onto variables to determine appearance.\n\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point(color = \"blue\")\n\n\n\n\n\n\n\n\n\n\n\nSame data, different geoms:\n\n# Left\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point()\n\n\n\n\n\n\n\n# Right\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n#&gt; `geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\nNot every aesthetic works with every geom. You can’t set the shape or a line geom.\n\n# Left\nggplot(mpg, aes(x = displ, y = hwy, shape = drv)) + \n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Right\nggplot(mpg, aes(x = displ, y = hwy, linetype = drv)) + \n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nInstead of giving a shape to a line geom, ggplot split the lines by drv.\nFor geoms that produce a single geometric object, you can use the group aesthetic to draw multiple objects. Note that the group aesthetic does not add a legend or distinguishing features to the geoms.\n\nggplot(mpg, aes(x = displ, y = hwy, color = drv)) + \n  geom_point() +\n  geom_smooth(aes(linetype = drv))\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nPlacing mappings in a geom function overwrites the global mappings for that layer only. This means you can have different aesthetics in different layers.\n\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point(aes(color = class)) + \n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nYou can use this idea to specify different data for each layer.\n\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point() + \n  geom_point(\n    data = mpg |&gt; filter(class == \"2seater\"), \n    color = \"red\"\n  ) +\n  geom_point(\n    data = mpg |&gt; filter(class == \"2seater\"), \n    shape = \"circle open\", size = 3, color = \"red\"\n  )\n\n\n\n\n\n\n\n\n\n# Left\nggplot(mpg, aes(x = hwy)) +\n  geom_histogram(binwidth = 2)\n\n\n\n\n\n\n\n# Middle\nggplot(mpg, aes(x = hwy)) +\n  geom_density()\n\n\n\n\n\n\n\n# Right\nggplot(mpg, aes(x = hwy)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n\nfacet_wrap() splits a plot into subplots that each display one subset of the data based on a categorical variable.\n\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point() + \n  facet_wrap(~cyl)\n\n\n\n\n\n\n\n\nTo facet with a combination of two variables, switch to facet_grid().\n\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point() + \n  facet_grid(drv ~ cyl)\n\n\n\n\n\n\n\n\nLet the x and y axes vary across plots with scales = \"free\".\n\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point() + \n  facet_grid(drv ~ cyl, scales = \"free\")\n\n\n\n\n\n\n\n\nUse the . to facet horizontally?\n\nggplot(mpg) + \n  geom_point(aes(x = displ, y = hwy)) +\n  facet_grid(drv ~ .)\n\n\n\n\n\n\n\n\n\nggplot(mpg, aes(x = displ)) + \n  geom_histogram() + \n  facet_grid(drv ~ .)\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\nggplot(mpg, aes(x = displ)) + \n  geom_histogram() + \n  facet_wrap(~drv)\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\nggplot(mpg, aes(x = displ)) + \n  geom_histogram() +\n  facet_grid(. ~ drv)\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n\nWhen new values have to be calculated for the plot.\n\nBar charts, histograms, and frequency polygons bin your data and then plot bin counts, the number of points that fall in each bin.\nSmoothers fit a model to your data and then plot predictions from the model.\nBoxplots compute the five-number summary of the distribution and then display that summary as a specially formatted box.\n\n\nggplot(diamonds, aes(x = cut)) + \n  geom_bar()\n\n\n\n\n\n\n\n\nThe algorithm used to calculate new values for a graph is called a stat, short for statistical transformation.\nYou can see which stat a geom uses by looking at default arguments, and the documentation for a given stat can be found on the geom help page. See how stat_count() is documented on the geom_bar() help page.\nEvery geom has a default stat, and every stat has a default geom.\nYou might need to use the stat explicitly to:\n\noverride the default stat\n\n\ndiamonds |&gt;\n  count(cut) |&gt;\n  ggplot(aes(x = cut, y = n)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\n\n\n\n\n\nOverride default mapping from transformed variables to aesthetics.\n\nFor example, a bar chart of proportions instead of counts is possible like this:\n\nggplot(diamonds, aes(x = cut, y = after_stat(prop), group = 1)) + \n  geom_bar()\n\n\n\n\n\n\n\n\n\nYou might want to draw greater attention to the statistical transformation in your code. Here is an example with stat_summary(), which summarizes the y value for each unique x value plotted.\n\n\nggplot(diamonds) + \n  stat_summary(\n    aes(x = cut, y = depth),\n    fun.min = min,\n    fun.max = max,\n    fun = median\n  )\n\n\n\n\n\n\n\n\n\n\n\nYou can add color to the edges or the fill like so:\n\n# Left\nggplot(mpg, aes(x = drv, color = drv)) + \n  geom_bar()\n\n\n\n\n\n\n\n# Right\nggplot(mpg, aes(x = drv, fill = drv)) + \n  geom_bar()\n\n\n\n\n\n\n\n\nMapping the fill aesthetic to another variable results in automatic stacking of bar charts.\n\nggplot(mpg, aes(x = drv, fill = class)) + \n  geom_bar()\n\n\n\n\n\n\n\n\nThis automatic stacking is accomplished with position adjustment using the position argument. If you don’t want a stacked bar chart, you can use one of three other options: \"identity\", \"dodge\", or \"fill\".\n\"identity\" will place each object exactly where it falls in the context of the graph.\n\n# Left\nggplot(mpg, aes(x = drv, fill = class)) + \n  geom_bar(alpha = 1/5, position = \"identity\")\n\n\n\n\n\n\n\n# Right\nggplot(mpg, aes(x = drv, color = class)) + \n  geom_bar(fill = NA, position = \"identity\")\n\n\n\n\n\n\n\n\nposition = \"fill\" works like stacking, but makes each bar the same height. Easier to compare proportions.\nposition = \"dodge\" places overlapping objects directly beside one another.\n\n# Left\nggplot(mpg, aes(x = drv, fill = class)) + \n  geom_bar(position = \"fill\")\n\n\n\n\n\n\n\n# Right\nggplot(mpg, aes(x = drv, fill = class)) + \n  geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\n\nWhat about overplotting? Use position = \"jitter\". This is most useful with a scatter plot.\n\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point(position = \"jitter\")\n\n\n\n\n\n\n\n\n\n\n\nMaps. coord_quickmap() sets the aspect ratio for maps.\n\nnz &lt;- map_data(\"nz\")\n\nggplot(nz, aes(x = long, y = lat, group = group)) +\n  geom_polygon(fill = \"white\", color = \"black\")\n\n\n\n\n\n\n\nggplot(nz, aes(x = long, y = lat, group = group)) +\n  geom_polygon(fill = \"white\", color = \"black\") +\n  coord_quickmap()\n\n\n\n\n\n\n\n\ncoord_polar() uses polar coordinates. Polar coordinates reveal an interesting connection between a bar chart and a Coxcomb chart.\n\nbar &lt;- ggplot(data = diamonds) + \n  geom_bar(\n    mapping = aes(x = clarity, fill = clarity), \n    show.legend = FALSE,\n    width = 1\n  ) + \n  theme(aspect.ratio = 1)\n\nbar + coord_flip()\n\n\n\n\n\n\n\nbar + coord_polar()\n\n\n\n\n\n\n\n\n\nggplot(data = mpg, mapping = aes(x = cty, y = hwy)) +\n  geom_point() + \n  geom_abline() +\n  coord_fixed()\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = ) + ( mapping = aes(), stat = , position =  ) +  +"
  },
  {
    "objectID": "chapter_09_notes.html#aesthetic-mappings",
    "href": "chapter_09_notes.html#aesthetic-mappings",
    "title": "R for Data Science, 2nd Edition - Chapter 9 Notes",
    "section": "",
    "text": "# Left\nggplot(mpg, aes(x = displ, y = hwy, color = class)) +\n  geom_point()\n\n\n\n\n\n\n\n# Right\nggplot(mpg, aes(x = displ, y = hwy, shape = class)) +\n  geom_point()\n\nWarning: The shape palette can deal with a maximum of 6 discrete values because more\nthan 6 becomes difficult to discriminate\nℹ you have requested 7 values. Consider specifying shapes manually if you need\n  that many of them.\n\n\nWarning: Removed 62 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n#&gt; Warning: The shape palette can deal with a maximum of 6 discrete values because more\n#&gt; than 6 becomes difficult to discriminate\n#&gt; ℹ you have requested 7 values. Consider specifying shapes manually if you\n#&gt;   need that many of them.\n#&gt; Warning: Removed 62 rows containing missing values or values outside the scale range\n#&gt; (`geom_point()`).\n\n\n# Left\nggplot(mpg, aes(x = displ, y = hwy, size = class)) +\n  geom_point()\n\nWarning: Using size for a discrete variable is not advised.\n\n\n\n\n\n\n\n\n#&gt; Warning: Using size for a discrete variable is not advised.\n\n# Right\nggplot(mpg, aes(x = displ, y = hwy, alpha = class)) +\n  geom_point()\n\nWarning: Using alpha for a discrete variable is not advised.\n\n\n\n\n\n\n\n\n#&gt; Warning: Using alpha for a discrete variable is not advised.\n\nArguments outside of aes() do not map onto variables to determine appearance.\n\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point(color = \"blue\")"
  },
  {
    "objectID": "chapter_09_notes.html#geometric-objects",
    "href": "chapter_09_notes.html#geometric-objects",
    "title": "R for Data Science, 2nd Edition - Chapter 9 Notes",
    "section": "",
    "text": "Same data, different geoms:\n\n# Left\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point()\n\n\n\n\n\n\n\n# Right\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n#&gt; `geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\nNot every aesthetic works with every geom. You can’t set the shape or a line geom.\n\n# Left\nggplot(mpg, aes(x = displ, y = hwy, shape = drv)) + \n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Right\nggplot(mpg, aes(x = displ, y = hwy, linetype = drv)) + \n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nInstead of giving a shape to a line geom, ggplot split the lines by drv.\nFor geoms that produce a single geometric object, you can use the group aesthetic to draw multiple objects. Note that the group aesthetic does not add a legend or distinguishing features to the geoms.\n\nggplot(mpg, aes(x = displ, y = hwy, color = drv)) + \n  geom_point() +\n  geom_smooth(aes(linetype = drv))\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nPlacing mappings in a geom function overwrites the global mappings for that layer only. This means you can have different aesthetics in different layers.\n\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point(aes(color = class)) + \n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nYou can use this idea to specify different data for each layer.\n\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point() + \n  geom_point(\n    data = mpg |&gt; filter(class == \"2seater\"), \n    color = \"red\"\n  ) +\n  geom_point(\n    data = mpg |&gt; filter(class == \"2seater\"), \n    shape = \"circle open\", size = 3, color = \"red\"\n  )\n\n\n\n\n\n\n\n\n\n# Left\nggplot(mpg, aes(x = hwy)) +\n  geom_histogram(binwidth = 2)\n\n\n\n\n\n\n\n# Middle\nggplot(mpg, aes(x = hwy)) +\n  geom_density()\n\n\n\n\n\n\n\n# Right\nggplot(mpg, aes(x = hwy)) +\n  geom_boxplot()"
  },
  {
    "objectID": "chapter_09_notes.html#facets",
    "href": "chapter_09_notes.html#facets",
    "title": "R for Data Science, 2nd Edition - Chapter 9 Notes",
    "section": "",
    "text": "facet_wrap() splits a plot into subplots that each display one subset of the data based on a categorical variable.\n\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point() + \n  facet_wrap(~cyl)\n\n\n\n\n\n\n\n\nTo facet with a combination of two variables, switch to facet_grid().\n\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point() + \n  facet_grid(drv ~ cyl)\n\n\n\n\n\n\n\n\nLet the x and y axes vary across plots with scales = \"free\".\n\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point() + \n  facet_grid(drv ~ cyl, scales = \"free\")\n\n\n\n\n\n\n\n\nUse the . to facet horizontally?\n\nggplot(mpg) + \n  geom_point(aes(x = displ, y = hwy)) +\n  facet_grid(drv ~ .)\n\n\n\n\n\n\n\n\n\nggplot(mpg, aes(x = displ)) + \n  geom_histogram() + \n  facet_grid(drv ~ .)\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\nggplot(mpg, aes(x = displ)) + \n  geom_histogram() + \n  facet_wrap(~drv)\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\nggplot(mpg, aes(x = displ)) + \n  geom_histogram() +\n  facet_grid(. ~ drv)\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`."
  },
  {
    "objectID": "chapter_09_notes.html#statistical-transformations",
    "href": "chapter_09_notes.html#statistical-transformations",
    "title": "R for Data Science, 2nd Edition - Chapter 9 Notes",
    "section": "",
    "text": "When new values have to be calculated for the plot.\n\nBar charts, histograms, and frequency polygons bin your data and then plot bin counts, the number of points that fall in each bin.\nSmoothers fit a model to your data and then plot predictions from the model.\nBoxplots compute the five-number summary of the distribution and then display that summary as a specially formatted box.\n\n\nggplot(diamonds, aes(x = cut)) + \n  geom_bar()\n\n\n\n\n\n\n\n\nThe algorithm used to calculate new values for a graph is called a stat, short for statistical transformation.\nYou can see which stat a geom uses by looking at default arguments, and the documentation for a given stat can be found on the geom help page. See how stat_count() is documented on the geom_bar() help page.\nEvery geom has a default stat, and every stat has a default geom.\nYou might need to use the stat explicitly to:\n\noverride the default stat\n\n\ndiamonds |&gt;\n  count(cut) |&gt;\n  ggplot(aes(x = cut, y = n)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\n\n\n\n\n\nOverride default mapping from transformed variables to aesthetics.\n\nFor example, a bar chart of proportions instead of counts is possible like this:\n\nggplot(diamonds, aes(x = cut, y = after_stat(prop), group = 1)) + \n  geom_bar()\n\n\n\n\n\n\n\n\n\nYou might want to draw greater attention to the statistical transformation in your code. Here is an example with stat_summary(), which summarizes the y value for each unique x value plotted.\n\n\nggplot(diamonds) + \n  stat_summary(\n    aes(x = cut, y = depth),\n    fun.min = min,\n    fun.max = max,\n    fun = median\n  )"
  },
  {
    "objectID": "chapter_09_notes.html#position-adjustments",
    "href": "chapter_09_notes.html#position-adjustments",
    "title": "R for Data Science, 2nd Edition - Chapter 9 Notes",
    "section": "",
    "text": "You can add color to the edges or the fill like so:\n\n# Left\nggplot(mpg, aes(x = drv, color = drv)) + \n  geom_bar()\n\n\n\n\n\n\n\n# Right\nggplot(mpg, aes(x = drv, fill = drv)) + \n  geom_bar()\n\n\n\n\n\n\n\n\nMapping the fill aesthetic to another variable results in automatic stacking of bar charts.\n\nggplot(mpg, aes(x = drv, fill = class)) + \n  geom_bar()\n\n\n\n\n\n\n\n\nThis automatic stacking is accomplished with position adjustment using the position argument. If you don’t want a stacked bar chart, you can use one of three other options: \"identity\", \"dodge\", or \"fill\".\n\"identity\" will place each object exactly where it falls in the context of the graph.\n\n# Left\nggplot(mpg, aes(x = drv, fill = class)) + \n  geom_bar(alpha = 1/5, position = \"identity\")\n\n\n\n\n\n\n\n# Right\nggplot(mpg, aes(x = drv, color = class)) + \n  geom_bar(fill = NA, position = \"identity\")\n\n\n\n\n\n\n\n\nposition = \"fill\" works like stacking, but makes each bar the same height. Easier to compare proportions.\nposition = \"dodge\" places overlapping objects directly beside one another.\n\n# Left\nggplot(mpg, aes(x = drv, fill = class)) + \n  geom_bar(position = \"fill\")\n\n\n\n\n\n\n\n# Right\nggplot(mpg, aes(x = drv, fill = class)) + \n  geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\n\nWhat about overplotting? Use position = \"jitter\". This is most useful with a scatter plot.\n\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point(position = \"jitter\")"
  },
  {
    "objectID": "chapter_09_notes.html#coordinate-systems",
    "href": "chapter_09_notes.html#coordinate-systems",
    "title": "R for Data Science, 2nd Edition - Chapter 9 Notes",
    "section": "",
    "text": "Maps. coord_quickmap() sets the aspect ratio for maps.\n\nnz &lt;- map_data(\"nz\")\n\nggplot(nz, aes(x = long, y = lat, group = group)) +\n  geom_polygon(fill = \"white\", color = \"black\")\n\n\n\n\n\n\n\nggplot(nz, aes(x = long, y = lat, group = group)) +\n  geom_polygon(fill = \"white\", color = \"black\") +\n  coord_quickmap()\n\n\n\n\n\n\n\n\ncoord_polar() uses polar coordinates. Polar coordinates reveal an interesting connection between a bar chart and a Coxcomb chart.\n\nbar &lt;- ggplot(data = diamonds) + \n  geom_bar(\n    mapping = aes(x = clarity, fill = clarity), \n    show.legend = FALSE,\n    width = 1\n  ) + \n  theme(aspect.ratio = 1)\n\nbar + coord_flip()\n\n\n\n\n\n\n\nbar + coord_polar()\n\n\n\n\n\n\n\n\n\nggplot(data = mpg, mapping = aes(x = cty, y = hwy)) +\n  geom_point() + \n  geom_abline() +\n  coord_fixed()"
  },
  {
    "objectID": "chapter_09_notes.html#the-layered-grammar-of-graphics",
    "href": "chapter_09_notes.html#the-layered-grammar-of-graphics",
    "title": "R for Data Science, 2nd Edition - Chapter 9 Notes",
    "section": "",
    "text": "ggplot(data = ) + ( mapping = aes(), stat = , position =  ) +  +"
  },
  {
    "objectID": "chapter_11_notes.html",
    "href": "chapter_11_notes.html",
    "title": "R for Data Science, 2nd Edition - Chapter 11 Notes",
    "section": "",
    "text": "Here we go from exploratory to expository graphs.\nThey recommend The Truthful Art by Albert Cairo as a guide for data visualization.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.6\n✔ forcats   1.0.1     ✔ stringr   1.6.0\n✔ ggplot2   4.0.1     ✔ tibble    3.3.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.2\n✔ purrr     1.2.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(scales)\n\n\nAttaching package: 'scales'\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\nlibrary(ggrepel)\nlibrary(patchwork)\n\n\n\nLabels help to make a chart expository.\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) +\n  geom_smooth(se = FALSE) +\n  labs(\n    x = \"Engine displacement (L)\",\n    y = \"Highway fuel economy (mpg)\",\n    color = \"Car type\",\n    title = \"Fuel efficiency generally decreases with engine size\",\n    subtitle = \"Two seaters (sports cars) are an exception because of their light weight\",\n    caption = \"Data from fueleconomy.gov\"\n  )\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\nSometimes it’s helpful to label individual observations or groups of observations. You can use geom_text(), which functions like geom_point(), but it has an additional aesthetic: label, which is for textual labels to add to your plot.\n\nlabel_info &lt;- mpg |&gt;\n  group_by(drv) |&gt;\n  arrange(desc(displ)) |&gt;\n  slice_head(n = 1) |&gt;\n  mutate(\n    drive_type = case_when(\n      drv == \"f\" ~ \"front-wheel drive\",\n      drv == \"r\" ~ \"rear-wheel drive\",\n      drv == \"4\" ~ \"4-wheel drive\"\n    )\n  ) |&gt;\n  select(displ, hwy, drv, drive_type)\n\nlabel_info\n\n# A tibble: 3 × 4\n# Groups:   drv [3]\n  displ   hwy drv   drive_type       \n  &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;            \n1   6.5    17 4     4-wheel drive    \n2   5.3    25 f     front-wheel drive\n3   7      24 r     rear-wheel drive \n\n#&gt; # A tibble: 3 × 4\n#&gt; # Groups:   drv [3]\n#&gt;   displ   hwy drv   drive_type       \n#&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;            \n#&gt; 1   6.5    17 4     4-wheel drive    \n#&gt; 2   5.3    25 f     front-wheel drive\n#&gt; 3   7      24 r     rear-wheel drive\n\n\nggplot(mpg, aes(x = displ, y = hwy, color = drv)) +\n  geom_point(alpha = 0.3) +\n  geom_smooth(se = FALSE) +\n  geom_text(\n    data = label_info, \n    aes(x = displ, y = hwy, label = drive_type),\n    fontface = \"bold\", size = 5, hjust = \"right\", vjust = \"bottom\"\n  ) +\n  theme(legend.position = \"none\")\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n#&gt; `geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\nTo prevent labels from overlapping with one another and/or with the data, use geom_label_repel() from the ggrepel package.\n\nggplot(mpg, aes(x = displ, y = hwy, color = drv)) +\n  geom_point(alpha = 0.3) +\n  geom_smooth(se = FALSE) +\n  geom_label_repel(\n    data = label_info, \n    aes(x = displ, y = hwy, label = drive_type),\n    fontface = \"bold\", size = 5, nudge_y = 2\n  ) +\n  theme(legend.position = \"none\")\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n#&gt; `geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\nAnother handy use case for geom_text_repel() is to highlight outliers.\n\npotential_outliers &lt;- mpg |&gt;\n  filter(hwy &gt; 40 | (hwy &gt; 20 & displ &gt; 5))\n  \nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point() +\n  geom_text_repel(data = potential_outliers, aes(label = model)) +\n  geom_point(data = potential_outliers, color = \"red\") +\n  geom_point(\n    data = potential_outliers,\n    color = \"red\", size = 3, shape = \"circle open\"\n  )\n\n\n\n\n\n\n\n\nMore annotation ideas:\nUse geom_hline() and geom_vline() to add reference lines. We often make them thick (linewidth = 2) and white (color = white), and draw them underneath the primary data layer. That makes them easy to see, without drawing attention away from the data.\nUse geom_rect() to draw a rectangle around points of interest. The boundaries of the rectangle are defined by aesthetics xmin, xmax, ymin, ymax. Alternatively, look into the ggforce package, specifically geom_mark_hull(), which allows you to annotate subsets of points with hulls.\nUse geom_segment() with the arrow argument to draw attention to a point with an arrow. Use aesthetics x and y to define the starting location, and xend and yend to define the end location.\nUse annotate() to highlight just one or two points or ideas.\n\ntrend_text &lt;- \"Larger engine sizes tend to have lower fuel economy.\" |&gt;\n  str_wrap(width = 30)\ntrend_text\n\n[1] \"Larger engine sizes tend to\\nhave lower fuel economy.\"\n\n#&gt; [1] \"Larger engine sizes tend to\\nhave lower fuel economy.\"\n\nNote the annotaiton is accomplised with a label geom and a segment geom, which are both called within the annotate function, in two separate calls to the function.\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point() +\n  annotate(\n    geom = \"label\", x = 3.5, y = 38,\n    label = trend_text,\n    hjust = \"left\", color = \"red\"\n  ) +\n  annotate(\n    geom = \"segment\",\n    x = 3, y = 35, xend = 5, yend = 25, color = \"red\",\n    arrow = arrow(type = \"closed\")\n  )\n\n\n\n\n\n\n\n\n\n\n\nControl how the aesthetic mappings manifest visually.\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class))\n\n\n\n\n\n\n\n\nBehind the scenes, this is what happens:\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) +\n  scale_x_continuous() +\n  scale_y_continuous() +\n  scale_color_discrete() # chooses colors for each of the class of cars\n\n\n\n\n\n\n\n\nYou may want to access these scale functions to change things like breaks on the axes, or the key labels on the legend.\nYou might want to replace the scale altogether.\nAxis ticks and legend keys.\nCollectively, axes and legends are called guides. Two arguments affect the appearance of the ticks on the axes and keys on the legend: breaks and labels.\n\nggplot(mpg, aes(x = displ, y = hwy, color = drv)) +\n  geom_point() +\n  scale_y_continuous(breaks = seq(15, 40, by = 5)) \n\n\n\n\n\n\n\n\nlabels can be set to NULL, or a vector of length breaks, or in the case of a categorical variables, a named list with mappings to existing level names.\n\nggplot(mpg, aes(x = displ, y = hwy, color = drv)) +\n  geom_point() +\n  scale_x_continuous(labels = NULL) +\n  scale_y_continuous(labels = NULL) +\n  scale_color_discrete(labels = c(\"4\" = \"4-wheel\", \"f\" = \"front\", \"r\" = \"rear\"))\n\n\n\n\n\n\n\n\nThe labels argument can be combined with the scales package. Note that the breaks argument is on the original scale.\n\n# Left\nggplot(diamonds, aes(x = price, y = cut)) +\n  geom_boxplot(alpha = 0.05) +\n  scale_x_continuous(labels = label_dollar())\n\n\n\n\n\n\n\n# Right\nggplot(diamonds, aes(x = price, y = cut)) +\n  geom_boxplot(alpha = 0.05) +\n  scale_x_continuous(\n    labels = label_dollar(scale = 1/1000, suffix = \"K\"), \n    breaks = seq(1000, 19000, by = 6000)\n  )\n\n\n\n\n\n\n\n\nlabel_percent() is handy when visualizing proportions.\n\nggplot(diamonds, aes(x = cut, fill = clarity)) +\n  geom_bar(position = \"fill\") +\n  scale_y_continuous(name = \"Percentage\", labels = label_percent())\n\n\n\n\n\n\n\n\nCustom breaks arguments can be useful when there isn’t much data and you want to highlight specific data points.\n\npresidential |&gt;\n  mutate(id = 33 + row_number()) |&gt;\n  ggplot(aes(x = start, y = id)) +\n  geom_point() +\n  geom_segment(aes(xend = end, yend = id)) +\n  scale_x_date(name = NULL, breaks = presidential$start, date_labels = \"'%y\")\n\n\n\n\n\n\n\n\nLegend layout.\nUse the theme() setting legend_position(). Also use guides.\n\nbase &lt;- ggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class))\n\nbase + theme(legend.position = \"right\") # the default\n\n\n\n\n\n\n\nbase + theme(legend.position = \"left\")\n\n\n\n\n\n\n\nbase + \n  theme(legend.position = \"top\") +\n  guides(color = guide_legend(nrow = 3))\n\n\n\n\n\n\n\nbase + \n  theme(legend.position = \"bottom\") +\n  guides(color = guide_legend(nrow = 3))\n\n\n\n\n\n\n\n\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) +\n  geom_smooth(se = FALSE) +\n  theme(legend.position = \"bottom\") +\n  guides(color = guide_legend(nrow = 2, override.aes = list(size = 4))) # aes override\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n#&gt; `geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\nReplacing a scale.\n\n# Left\nggplot(diamonds, aes(x = carat, y = price)) +\n  geom_bin2d()\n\n`stat_bin2d()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\n#&gt; `stat_bin2d()` using `bins = 30`. Pick better value `binwidth`.\n\n# Right\nggplot(diamonds, aes(x = log10(carat), y = log10(price))) +\n  geom_bin2d()\n\n`stat_bin2d()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\n#&gt; `stat_bin2d()` using `bins = 30`. Pick better value `binwidth`.\n\nscale_color_brewer() allows access to palettes that can be seen by people with color blindness.\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = drv))\n\n\n\n\n\n\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = drv)) +\n  scale_color_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\n\nRedundant shape mapping also improves accessibility\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = drv, shape = drv)) +\n  scale_color_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\n\nSpecify mapping between colors and variables using scale_color_manual().\n\npresidential |&gt;\n  mutate(id = 33 + row_number()) |&gt;\n  ggplot(aes(x = start, y = id, color = party)) +\n  geom_point() +\n  geom_segment(aes(xend = end, yend = id)) +\n  scale_color_manual(values = c(Republican = \"#E81B23\", Democratic = \"#00AEF3\"))\n\n\n\n\n\n\n\n\nViridis color scales are available, and also accessible for continuous, discrete, and binned palettes.\n\ndf &lt;- tibble(\n  x = rnorm(10000),\n  y = rnorm(10000)\n)\n\nggplot(df, aes(x, y)) +\n  geom_hex() +\n  coord_fixed() +\n  labs(title = \"Default, continuous\", x = NULL, y = NULL)\n\n\n\n\n\n\n\nggplot(df, aes(x, y)) +\n  geom_hex() +\n  coord_fixed() +\n  scale_fill_viridis_c() +\n  labs(title = \"Viridis, continuous\", x = NULL, y = NULL)\n\n\n\n\n\n\n\nggplot(df, aes(x, y)) +\n  geom_hex() +\n  coord_fixed() +\n  scale_fill_viridis_b() +\n  labs(title = \"Viridis, binned\", x = NULL, y = NULL)\n\n\n\n\n\n\n\n\nYou can zoom in on a portion of the plot using coord_cartesian().\n\n# Left\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = drv)) +\n  geom_smooth() +\n  scale_x_continuous(limits = c(5, 6)) +\n  scale_y_continuous(limits = c(10, 25))\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning: Removed 202 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 202 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n# Right\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = drv)) +\n  geom_smooth() +\n  coord_cartesian(xlim = c(5, 6), ylim = c(10, 25))\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nsuv &lt;- mpg |&gt; filter(class == \"suv\")\ncompact &lt;- mpg |&gt; filter(class == \"compact\")\n\n# Left\nggplot(suv, aes(x = displ, y = hwy, color = drv)) +\n  geom_point()\n\n\n\n\n\n\n\n# Right\nggplot(compact, aes(x = displ, y = hwy, color = drv)) +\n  geom_point()\n\n\n\n\n\n\n\n\nEnforcing common scale limits across plots to enhance comparison.\n\nx_scale &lt;- scale_x_continuous(limits = range(mpg$displ))\ny_scale &lt;- scale_y_continuous(limits = range(mpg$hwy))\ncol_scale &lt;- scale_color_discrete(limits = unique(mpg$drv))\n\n# Left\nggplot(suv, aes(x = displ, y = hwy, color = drv)) +\n  geom_point() +\n  x_scale +\n  y_scale +\n  col_scale\n\n\n\n\n\n\n\n# Right\nggplot(compact, aes(x = displ, y = hwy, color = drv)) +\n  geom_point() +\n  x_scale +\n  y_scale +\n  col_scale\n\n\n\n\n\n\n\n\n\n\n\nCustomize non-data elements with a theme.\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) +\n  geom_smooth(se = FALSE) +\n  theme_bw()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nCreate your own themes, use those included in ggplot2 or use extension packages like ggthemes.\n\n\n\nThe + operator in the patchwork package places two plots next to each other.\n\np1 &lt;- ggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point() + \n  labs(title = \"Plot 1\")\np2 &lt;- ggplot(mpg, aes(x = drv, y = hwy)) + \n  geom_boxplot() + \n  labs(title = \"Plot 2\")\np1 + p2\n\n\n\n\n\n\n\n\nMore complex layouts with | and /.\n\np3 &lt;- ggplot(mpg, aes(x = cty, y = hwy)) + \n  geom_point() + \n  labs(title = \"Plot 3\")\n(p1 | p3) / p2\n\n\n\n\n\n\n\n\n\np1 &lt;- ggplot(mpg, aes(x = drv, y = cty, color = drv)) + \n  geom_boxplot(show.legend = FALSE) + \n  labs(title = \"Plot 1\")\n\np2 &lt;- ggplot(mpg, aes(x = drv, y = hwy, color = drv)) + \n  geom_boxplot(show.legend = FALSE) + \n  labs(title = \"Plot 2\")\n\np3 &lt;- ggplot(mpg, aes(x = cty, color = drv, fill = drv)) + \n  geom_density(alpha = 0.5) + \n  labs(title = \"Plot 3\")\n\np4 &lt;- ggplot(mpg, aes(x = hwy, color = drv, fill = drv)) + \n  geom_density(alpha = 0.5) + \n  labs(title = \"Plot 4\")\n\np5 &lt;- ggplot(mpg, aes(x = cty, y = hwy, color = drv)) + \n  geom_point(show.legend = FALSE) + \n  facet_wrap(~drv) +\n  labs(title = \"Plot 5\")\n\n(guide_area() / (p1 + p2) / (p3 + p4) / p5) +\n  plot_annotation(\n    title = \"City and highway mileage for cars with different drive trains\",\n    caption = \"Source: https://fueleconomy.gov.\"\n  ) +\n  plot_layout(\n    guides = \"collect\",\n    heights = c(1, 3, 2, 4)\n    ) &\n  theme(legend.position = \"top\")"
  },
  {
    "objectID": "chapter_11_notes.html#labels",
    "href": "chapter_11_notes.html#labels",
    "title": "R for Data Science, 2nd Edition - Chapter 11 Notes",
    "section": "",
    "text": "Labels help to make a chart expository.\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) +\n  geom_smooth(se = FALSE) +\n  labs(\n    x = \"Engine displacement (L)\",\n    y = \"Highway fuel economy (mpg)\",\n    color = \"Car type\",\n    title = \"Fuel efficiency generally decreases with engine size\",\n    subtitle = \"Two seaters (sports cars) are an exception because of their light weight\",\n    caption = \"Data from fueleconomy.gov\"\n  )\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'"
  },
  {
    "objectID": "chapter_11_notes.html#annotations",
    "href": "chapter_11_notes.html#annotations",
    "title": "R for Data Science, 2nd Edition - Chapter 11 Notes",
    "section": "",
    "text": "Sometimes it’s helpful to label individual observations or groups of observations. You can use geom_text(), which functions like geom_point(), but it has an additional aesthetic: label, which is for textual labels to add to your plot.\n\nlabel_info &lt;- mpg |&gt;\n  group_by(drv) |&gt;\n  arrange(desc(displ)) |&gt;\n  slice_head(n = 1) |&gt;\n  mutate(\n    drive_type = case_when(\n      drv == \"f\" ~ \"front-wheel drive\",\n      drv == \"r\" ~ \"rear-wheel drive\",\n      drv == \"4\" ~ \"4-wheel drive\"\n    )\n  ) |&gt;\n  select(displ, hwy, drv, drive_type)\n\nlabel_info\n\n# A tibble: 3 × 4\n# Groups:   drv [3]\n  displ   hwy drv   drive_type       \n  &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;            \n1   6.5    17 4     4-wheel drive    \n2   5.3    25 f     front-wheel drive\n3   7      24 r     rear-wheel drive \n\n#&gt; # A tibble: 3 × 4\n#&gt; # Groups:   drv [3]\n#&gt;   displ   hwy drv   drive_type       \n#&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;            \n#&gt; 1   6.5    17 4     4-wheel drive    \n#&gt; 2   5.3    25 f     front-wheel drive\n#&gt; 3   7      24 r     rear-wheel drive\n\n\nggplot(mpg, aes(x = displ, y = hwy, color = drv)) +\n  geom_point(alpha = 0.3) +\n  geom_smooth(se = FALSE) +\n  geom_text(\n    data = label_info, \n    aes(x = displ, y = hwy, label = drive_type),\n    fontface = \"bold\", size = 5, hjust = \"right\", vjust = \"bottom\"\n  ) +\n  theme(legend.position = \"none\")\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n#&gt; `geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\nTo prevent labels from overlapping with one another and/or with the data, use geom_label_repel() from the ggrepel package.\n\nggplot(mpg, aes(x = displ, y = hwy, color = drv)) +\n  geom_point(alpha = 0.3) +\n  geom_smooth(se = FALSE) +\n  geom_label_repel(\n    data = label_info, \n    aes(x = displ, y = hwy, label = drive_type),\n    fontface = \"bold\", size = 5, nudge_y = 2\n  ) +\n  theme(legend.position = \"none\")\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n#&gt; `geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\nAnother handy use case for geom_text_repel() is to highlight outliers.\n\npotential_outliers &lt;- mpg |&gt;\n  filter(hwy &gt; 40 | (hwy &gt; 20 & displ &gt; 5))\n  \nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point() +\n  geom_text_repel(data = potential_outliers, aes(label = model)) +\n  geom_point(data = potential_outliers, color = \"red\") +\n  geom_point(\n    data = potential_outliers,\n    color = \"red\", size = 3, shape = \"circle open\"\n  )\n\n\n\n\n\n\n\n\nMore annotation ideas:\nUse geom_hline() and geom_vline() to add reference lines. We often make them thick (linewidth = 2) and white (color = white), and draw them underneath the primary data layer. That makes them easy to see, without drawing attention away from the data.\nUse geom_rect() to draw a rectangle around points of interest. The boundaries of the rectangle are defined by aesthetics xmin, xmax, ymin, ymax. Alternatively, look into the ggforce package, specifically geom_mark_hull(), which allows you to annotate subsets of points with hulls.\nUse geom_segment() with the arrow argument to draw attention to a point with an arrow. Use aesthetics x and y to define the starting location, and xend and yend to define the end location.\nUse annotate() to highlight just one or two points or ideas.\n\ntrend_text &lt;- \"Larger engine sizes tend to have lower fuel economy.\" |&gt;\n  str_wrap(width = 30)\ntrend_text\n\n[1] \"Larger engine sizes tend to\\nhave lower fuel economy.\"\n\n#&gt; [1] \"Larger engine sizes tend to\\nhave lower fuel economy.\"\n\nNote the annotaiton is accomplised with a label geom and a segment geom, which are both called within the annotate function, in two separate calls to the function.\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point() +\n  annotate(\n    geom = \"label\", x = 3.5, y = 38,\n    label = trend_text,\n    hjust = \"left\", color = \"red\"\n  ) +\n  annotate(\n    geom = \"segment\",\n    x = 3, y = 35, xend = 5, yend = 25, color = \"red\",\n    arrow = arrow(type = \"closed\")\n  )"
  },
  {
    "objectID": "chapter_11_notes.html#scales",
    "href": "chapter_11_notes.html#scales",
    "title": "R for Data Science, 2nd Edition - Chapter 11 Notes",
    "section": "",
    "text": "Control how the aesthetic mappings manifest visually.\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class))\n\n\n\n\n\n\n\n\nBehind the scenes, this is what happens:\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) +\n  scale_x_continuous() +\n  scale_y_continuous() +\n  scale_color_discrete() # chooses colors for each of the class of cars\n\n\n\n\n\n\n\n\nYou may want to access these scale functions to change things like breaks on the axes, or the key labels on the legend.\nYou might want to replace the scale altogether.\nAxis ticks and legend keys.\nCollectively, axes and legends are called guides. Two arguments affect the appearance of the ticks on the axes and keys on the legend: breaks and labels.\n\nggplot(mpg, aes(x = displ, y = hwy, color = drv)) +\n  geom_point() +\n  scale_y_continuous(breaks = seq(15, 40, by = 5)) \n\n\n\n\n\n\n\n\nlabels can be set to NULL, or a vector of length breaks, or in the case of a categorical variables, a named list with mappings to existing level names.\n\nggplot(mpg, aes(x = displ, y = hwy, color = drv)) +\n  geom_point() +\n  scale_x_continuous(labels = NULL) +\n  scale_y_continuous(labels = NULL) +\n  scale_color_discrete(labels = c(\"4\" = \"4-wheel\", \"f\" = \"front\", \"r\" = \"rear\"))\n\n\n\n\n\n\n\n\nThe labels argument can be combined with the scales package. Note that the breaks argument is on the original scale.\n\n# Left\nggplot(diamonds, aes(x = price, y = cut)) +\n  geom_boxplot(alpha = 0.05) +\n  scale_x_continuous(labels = label_dollar())\n\n\n\n\n\n\n\n# Right\nggplot(diamonds, aes(x = price, y = cut)) +\n  geom_boxplot(alpha = 0.05) +\n  scale_x_continuous(\n    labels = label_dollar(scale = 1/1000, suffix = \"K\"), \n    breaks = seq(1000, 19000, by = 6000)\n  )\n\n\n\n\n\n\n\n\nlabel_percent() is handy when visualizing proportions.\n\nggplot(diamonds, aes(x = cut, fill = clarity)) +\n  geom_bar(position = \"fill\") +\n  scale_y_continuous(name = \"Percentage\", labels = label_percent())\n\n\n\n\n\n\n\n\nCustom breaks arguments can be useful when there isn’t much data and you want to highlight specific data points.\n\npresidential |&gt;\n  mutate(id = 33 + row_number()) |&gt;\n  ggplot(aes(x = start, y = id)) +\n  geom_point() +\n  geom_segment(aes(xend = end, yend = id)) +\n  scale_x_date(name = NULL, breaks = presidential$start, date_labels = \"'%y\")\n\n\n\n\n\n\n\n\nLegend layout.\nUse the theme() setting legend_position(). Also use guides.\n\nbase &lt;- ggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class))\n\nbase + theme(legend.position = \"right\") # the default\n\n\n\n\n\n\n\nbase + theme(legend.position = \"left\")\n\n\n\n\n\n\n\nbase + \n  theme(legend.position = \"top\") +\n  guides(color = guide_legend(nrow = 3))\n\n\n\n\n\n\n\nbase + \n  theme(legend.position = \"bottom\") +\n  guides(color = guide_legend(nrow = 3))\n\n\n\n\n\n\n\n\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) +\n  geom_smooth(se = FALSE) +\n  theme(legend.position = \"bottom\") +\n  guides(color = guide_legend(nrow = 2, override.aes = list(size = 4))) # aes override\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n#&gt; `geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\nReplacing a scale.\n\n# Left\nggplot(diamonds, aes(x = carat, y = price)) +\n  geom_bin2d()\n\n`stat_bin2d()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\n#&gt; `stat_bin2d()` using `bins = 30`. Pick better value `binwidth`.\n\n# Right\nggplot(diamonds, aes(x = log10(carat), y = log10(price))) +\n  geom_bin2d()\n\n`stat_bin2d()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\n#&gt; `stat_bin2d()` using `bins = 30`. Pick better value `binwidth`.\n\nscale_color_brewer() allows access to palettes that can be seen by people with color blindness.\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = drv))\n\n\n\n\n\n\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = drv)) +\n  scale_color_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\n\nRedundant shape mapping also improves accessibility\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = drv, shape = drv)) +\n  scale_color_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\n\nSpecify mapping between colors and variables using scale_color_manual().\n\npresidential |&gt;\n  mutate(id = 33 + row_number()) |&gt;\n  ggplot(aes(x = start, y = id, color = party)) +\n  geom_point() +\n  geom_segment(aes(xend = end, yend = id)) +\n  scale_color_manual(values = c(Republican = \"#E81B23\", Democratic = \"#00AEF3\"))\n\n\n\n\n\n\n\n\nViridis color scales are available, and also accessible for continuous, discrete, and binned palettes.\n\ndf &lt;- tibble(\n  x = rnorm(10000),\n  y = rnorm(10000)\n)\n\nggplot(df, aes(x, y)) +\n  geom_hex() +\n  coord_fixed() +\n  labs(title = \"Default, continuous\", x = NULL, y = NULL)\n\n\n\n\n\n\n\nggplot(df, aes(x, y)) +\n  geom_hex() +\n  coord_fixed() +\n  scale_fill_viridis_c() +\n  labs(title = \"Viridis, continuous\", x = NULL, y = NULL)\n\n\n\n\n\n\n\nggplot(df, aes(x, y)) +\n  geom_hex() +\n  coord_fixed() +\n  scale_fill_viridis_b() +\n  labs(title = \"Viridis, binned\", x = NULL, y = NULL)\n\n\n\n\n\n\n\n\nYou can zoom in on a portion of the plot using coord_cartesian().\n\n# Left\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = drv)) +\n  geom_smooth() +\n  scale_x_continuous(limits = c(5, 6)) +\n  scale_y_continuous(limits = c(10, 25))\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning: Removed 202 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 202 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n# Right\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = drv)) +\n  geom_smooth() +\n  coord_cartesian(xlim = c(5, 6), ylim = c(10, 25))\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nsuv &lt;- mpg |&gt; filter(class == \"suv\")\ncompact &lt;- mpg |&gt; filter(class == \"compact\")\n\n# Left\nggplot(suv, aes(x = displ, y = hwy, color = drv)) +\n  geom_point()\n\n\n\n\n\n\n\n# Right\nggplot(compact, aes(x = displ, y = hwy, color = drv)) +\n  geom_point()\n\n\n\n\n\n\n\n\nEnforcing common scale limits across plots to enhance comparison.\n\nx_scale &lt;- scale_x_continuous(limits = range(mpg$displ))\ny_scale &lt;- scale_y_continuous(limits = range(mpg$hwy))\ncol_scale &lt;- scale_color_discrete(limits = unique(mpg$drv))\n\n# Left\nggplot(suv, aes(x = displ, y = hwy, color = drv)) +\n  geom_point() +\n  x_scale +\n  y_scale +\n  col_scale\n\n\n\n\n\n\n\n# Right\nggplot(compact, aes(x = displ, y = hwy, color = drv)) +\n  geom_point() +\n  x_scale +\n  y_scale +\n  col_scale"
  },
  {
    "objectID": "chapter_11_notes.html#themes",
    "href": "chapter_11_notes.html#themes",
    "title": "R for Data Science, 2nd Edition - Chapter 11 Notes",
    "section": "",
    "text": "Customize non-data elements with a theme.\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) +\n  geom_smooth(se = FALSE) +\n  theme_bw()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nCreate your own themes, use those included in ggplot2 or use extension packages like ggthemes."
  },
  {
    "objectID": "chapter_11_notes.html#layout",
    "href": "chapter_11_notes.html#layout",
    "title": "R for Data Science, 2nd Edition - Chapter 11 Notes",
    "section": "",
    "text": "The + operator in the patchwork package places two plots next to each other.\n\np1 &lt;- ggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point() + \n  labs(title = \"Plot 1\")\np2 &lt;- ggplot(mpg, aes(x = drv, y = hwy)) + \n  geom_boxplot() + \n  labs(title = \"Plot 2\")\np1 + p2\n\n\n\n\n\n\n\n\nMore complex layouts with | and /.\n\np3 &lt;- ggplot(mpg, aes(x = cty, y = hwy)) + \n  geom_point() + \n  labs(title = \"Plot 3\")\n(p1 | p3) / p2\n\n\n\n\n\n\n\n\n\np1 &lt;- ggplot(mpg, aes(x = drv, y = cty, color = drv)) + \n  geom_boxplot(show.legend = FALSE) + \n  labs(title = \"Plot 1\")\n\np2 &lt;- ggplot(mpg, aes(x = drv, y = hwy, color = drv)) + \n  geom_boxplot(show.legend = FALSE) + \n  labs(title = \"Plot 2\")\n\np3 &lt;- ggplot(mpg, aes(x = cty, color = drv, fill = drv)) + \n  geom_density(alpha = 0.5) + \n  labs(title = \"Plot 3\")\n\np4 &lt;- ggplot(mpg, aes(x = hwy, color = drv, fill = drv)) + \n  geom_density(alpha = 0.5) + \n  labs(title = \"Plot 4\")\n\np5 &lt;- ggplot(mpg, aes(x = cty, y = hwy, color = drv)) + \n  geom_point(show.legend = FALSE) + \n  facet_wrap(~drv) +\n  labs(title = \"Plot 5\")\n\n(guide_area() / (p1 + p2) / (p3 + p4) / p5) +\n  plot_annotation(\n    title = \"City and highway mileage for cars with different drive trains\",\n    caption = \"Source: https://fueleconomy.gov.\"\n  ) +\n  plot_layout(\n    guides = \"collect\",\n    heights = c(1, 3, 2, 4)\n    ) &\n  theme(legend.position = \"top\")"
  },
  {
    "objectID": "chapter_17_notes.html",
    "href": "chapter_17_notes.html",
    "title": "R for Data Science, 2nd Edition - Chapter 17 Notes",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.6\n✔ forcats   1.0.1     ✔ stringr   1.6.0\n✔ ggplot2   4.0.1     ✔ tibble    3.3.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.2\n✔ purrr     1.2.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(nycflights13)\n\n\n\nThere are three types of date/time data: date, time, date-time.\n\ncsv &lt;- \"\n  date,datetime\n  2022-01-02,2022-01-02 05:12\n\"\nread_csv(csv)\n\nWarning: The `file` argument of `vroom()` must use `I()` for literal data as of vroom\n1.5.0.\n  \n  # Bad:\n  vroom(\"X,Y\\n1.5,2.3\\n\")\n  \n  # Good:\n  vroom(I(\"X,Y\\n1.5,2.3\\n\"))\nℹ The deprecated feature was likely used in the readr package.\n  Please report the issue at &lt;https://github.com/tidyverse/readr/issues&gt;.\n\n\nRows: 1 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndttm (1): datetime\ndate (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 1 × 2\n  date       datetime           \n  &lt;date&gt;     &lt;dttm&gt;             \n1 2022-01-02 2022-01-02 05:12:00\n\n#&gt; Warning: The `file` argument of `vroom()` must use `I()` for literal data as of vroom\n#&gt; 1.5.0.\n#&gt;   \n#&gt;   # Bad:\n#&gt;   vroom(\"X,Y\\n1.5,2.3\\n\")\n#&gt;   \n#&gt;   # Good:\n#&gt;   vroom(I(\"X,Y\\n1.5,2.3\\n\"))\n#&gt; ℹ The deprecated feature was likely used in the readr package.\n#&gt;   Please report the issue at &lt;https://github.com/tidyverse/readr/issues&gt;.\n#&gt; # A tibble: 1 × 2\n#&gt;   date       datetime           \n#&gt;   &lt;date&gt;     &lt;dttm&gt;             \n#&gt; 1 2022-01-02 2022-01-02 05:12:00\n\nA few options for an ambigous date:\n\ncsv &lt;- \"\n  date\n  01/02/15\n\"\n\nread_csv(csv, col_types = cols(date = col_date(\"%m/%d/%y\")))\n\n# A tibble: 1 × 1\n  date      \n  &lt;date&gt;    \n1 2015-01-02\n\n#&gt; # A tibble: 1 × 1\n#&gt;   date      \n#&gt;   &lt;date&gt;    \n#&gt; 1 2015-01-02\n\nread_csv(csv, col_types = cols(date = col_date(\"%d/%m/%y\")))\n\n# A tibble: 1 × 1\n  date      \n  &lt;date&gt;    \n1 2015-02-01\n\n#&gt; # A tibble: 1 × 1\n#&gt;   date      \n#&gt;   &lt;date&gt;    \n#&gt; 1 2015-02-01\n\nread_csv(csv, col_types = cols(date = col_date(\"%y/%m/%d\")))\n\n# A tibble: 1 × 1\n  date      \n  &lt;date&gt;    \n1 2001-02-15\n\n#&gt; # A tibble: 1 × 1\n#&gt;   date      \n#&gt;   &lt;date&gt;    \n#&gt; 1 2001-02-15\n\nYou can also parse dates from strings using the ymd, mdy, and dmy functions.\n\nymd(\"2017-01-31\")\n\n[1] \"2017-01-31\"\n\n#&gt; [1] \"2017-01-31\"\nmdy(\"January 31st, 2017\")\n\n[1] \"2017-01-31\"\n\n#&gt; [1] \"2017-01-31\"\ndmy(\"31-Jan-2017\")\n\n[1] \"2017-01-31\"\n\n#&gt; [1] \"2017-01-31\"\n\nTo create date-times, just add an underscore and then one or more of “h”, “m”, or “s”.\n\nymd_hms(\"2017-01-31 20:11:59\")\n\n[1] \"2017-01-31 20:11:59 UTC\"\n\n#&gt; [1] \"2017-01-31 20:11:59 UTC\"\nmdy_hm(\"01/31/2017 08:01\")\n\n[1] \"2017-01-31 08:01:00 UTC\"\n\n#&gt; [1] \"2017-01-31 08:01:00 UTC\"\n\nSometimes date components are spread across columns:\n\nflights |&gt; \n  select(year, month, day, hour, minute)\n\n# A tibble: 336,776 × 5\n    year month   day  hour minute\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1  2013     1     1     5     15\n 2  2013     1     1     5     29\n 3  2013     1     1     5     40\n 4  2013     1     1     5     45\n 5  2013     1     1     6      0\n 6  2013     1     1     5     58\n 7  2013     1     1     6      0\n 8  2013     1     1     6      0\n 9  2013     1     1     6      0\n10  2013     1     1     6      0\n# ℹ 336,766 more rows\n\n#&gt; # A tibble: 336,776 × 5\n#&gt;    year month   day  hour minute\n#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1  2013     1     1     5     15\n#&gt; 2  2013     1     1     5     29\n#&gt; 3  2013     1     1     5     40\n#&gt; 4  2013     1     1     5     45\n#&gt; 5  2013     1     1     6      0\n#&gt; 6  2013     1     1     5     58\n#&gt; # ℹ 336,770 more rows\n\nThe make_dates() function is useful here:\n\nflights |&gt; \n  select(year, month, day, hour, minute) |&gt; \n  mutate(departure = make_datetime(year, month, day, hour, minute))\n\n# A tibble: 336,776 × 6\n    year month   day  hour minute departure          \n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dttm&gt;             \n 1  2013     1     1     5     15 2013-01-01 05:15:00\n 2  2013     1     1     5     29 2013-01-01 05:29:00\n 3  2013     1     1     5     40 2013-01-01 05:40:00\n 4  2013     1     1     5     45 2013-01-01 05:45:00\n 5  2013     1     1     6      0 2013-01-01 06:00:00\n 6  2013     1     1     5     58 2013-01-01 05:58:00\n 7  2013     1     1     6      0 2013-01-01 06:00:00\n 8  2013     1     1     6      0 2013-01-01 06:00:00\n 9  2013     1     1     6      0 2013-01-01 06:00:00\n10  2013     1     1     6      0 2013-01-01 06:00:00\n# ℹ 336,766 more rows\n\n#&gt; # A tibble: 336,776 × 6\n#&gt;    year month   day  hour minute departure          \n#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dttm&gt;             \n#&gt; 1  2013     1     1     5     15 2013-01-01 05:15:00\n#&gt; 2  2013     1     1     5     29 2013-01-01 05:29:00\n#&gt; 3  2013     1     1     5     40 2013-01-01 05:40:00\n#&gt; 4  2013     1     1     5     45 2013-01-01 05:45:00\n#&gt; 5  2013     1     1     6      0 2013-01-01 06:00:00\n#&gt; 6  2013     1     1     5     58 2013-01-01 05:58:00\n#&gt; # ℹ 336,770 more rows\n\nUse modulus arithmetic to pull out hour and minute components.\n\nmake_datetime_100 &lt;- function(year, month, day, time) {\n  make_datetime(year, month, day, time %/% 100, time %% 100)\n}\n\nflights_dt &lt;- flights |&gt; \n  filter(!is.na(dep_time), !is.na(arr_time)) |&gt; \n  mutate(\n    dep_time = make_datetime_100(year, month, day, dep_time),\n    arr_time = make_datetime_100(year, month, day, arr_time),\n    sched_dep_time = make_datetime_100(year, month, day, sched_dep_time),\n    sched_arr_time = make_datetime_100(year, month, day, sched_arr_time)\n  ) |&gt; \n  select(origin, dest, ends_with(\"delay\"), ends_with(\"time\"))\n\nflights_dt\n\n# A tibble: 328,063 × 9\n   origin dest  dep_delay arr_delay dep_time            sched_dep_time     \n   &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dttm&gt;              &lt;dttm&gt;             \n 1 EWR    IAH           2        11 2013-01-01 05:17:00 2013-01-01 05:15:00\n 2 LGA    IAH           4        20 2013-01-01 05:33:00 2013-01-01 05:29:00\n 3 JFK    MIA           2        33 2013-01-01 05:42:00 2013-01-01 05:40:00\n 4 JFK    BQN          -1       -18 2013-01-01 05:44:00 2013-01-01 05:45:00\n 5 LGA    ATL          -6       -25 2013-01-01 05:54:00 2013-01-01 06:00:00\n 6 EWR    ORD          -4        12 2013-01-01 05:54:00 2013-01-01 05:58:00\n 7 EWR    FLL          -5        19 2013-01-01 05:55:00 2013-01-01 06:00:00\n 8 LGA    IAD          -3       -14 2013-01-01 05:57:00 2013-01-01 06:00:00\n 9 JFK    MCO          -3        -8 2013-01-01 05:57:00 2013-01-01 06:00:00\n10 LGA    ORD          -2         8 2013-01-01 05:58:00 2013-01-01 06:00:00\n# ℹ 328,053 more rows\n# ℹ 3 more variables: arr_time &lt;dttm&gt;, sched_arr_time &lt;dttm&gt;, air_time &lt;dbl&gt;\n\n#&gt; # A tibble: 328,063 × 9\n#&gt;   origin dest  dep_delay arr_delay dep_time            sched_dep_time     \n#&gt;   &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dttm&gt;              &lt;dttm&gt;             \n#&gt; 1 EWR    IAH           2        11 2013-01-01 05:17:00 2013-01-01 05:15:00\n#&gt; 2 LGA    IAH           4        20 2013-01-01 05:33:00 2013-01-01 05:29:00\n#&gt; 3 JFK    MIA           2        33 2013-01-01 05:42:00 2013-01-01 05:40:00\n#&gt; 4 JFK    BQN          -1       -18 2013-01-01 05:44:00 2013-01-01 05:45:00\n#&gt; 5 LGA    ATL          -6       -25 2013-01-01 05:54:00 2013-01-01 06:00:00\n#&gt; 6 EWR    ORD          -4        12 2013-01-01 05:54:00 2013-01-01 05:58:00\n#&gt; # ℹ 328,057 more rows\n#&gt; # ℹ 3 more variables: arr_time &lt;dttm&gt;, sched_arr_time &lt;dttm&gt;, …\n\nVisualize departure time across the year\n\nflights_dt |&gt; \n  ggplot(aes(x = dep_time)) + \n  geom_freqpoly(binwidth = 86400) # 86400 seconds = 1 day\n\n\n\n\n\n\n\n\nor a single day\n\nflights_dt |&gt; \n  filter(dep_time &lt; ymd(20130102)) |&gt; \n  ggplot(aes(x = dep_time)) + \n  geom_freqpoly(binwidth = 600) # 600 s = 10 minutes\n\n\n\n\n\n\n\n\n\nas_datetime(today())\n\n[1] \"2026-02-07 UTC\"\n\n#&gt; [1] \"2026-02-05 UTC\"\nas_date(now())\n\n[1] \"2026-02-07\"\n\n#&gt; [1] \"2026-02-05\"\n\nParse the following with a readr column specification and a lubridate function. This is a good exercise.\n\nd1 &lt;- \"January 1, 2010\"\nd2 &lt;- \"2015-Mar-07\"\nd3 &lt;- \"06-Jun-2017\"\nd4 &lt;- c(\"August 19 (2015)\", \"July 1 (2015)\")\nd5 &lt;- \"12/30/14\" # Dec 30, 2014\nt1 &lt;- \"1705\"\nt2 &lt;- \"11:15:10.12 PM\"\n\n\n\n\nYou can pull out individual parts of the date with the accessor functions year(), month(), mday() (day of the month), yday() (day of the year), wday() (day of the week), hour(), minute(), and second(). These are effectively the opposites of make_datetime().\n\ndatetime &lt;- ymd_hms(\"2026-07-08 12:34:56\")\n\nyear(datetime)\n\n[1] 2026\n\n#&gt; [1] 2026\nmonth(datetime)\n\n[1] 7\n\n#&gt; [1] 7\nmday(datetime)\n\n[1] 8\n\n#&gt; [1] 8\n\nyday(datetime)\n\n[1] 189\n\n#&gt; [1] 189\nwday(datetime)\n\n[1] 4\n\n#&gt; [1] 4\n\nFor month() and wday() you can set label = TRUE to return the abbreviated name of the month or day of the week. Set abbr = FALSE to return the full name.\n\nflights_dt |&gt; \n  mutate(wday = wday(dep_time, label = TRUE)) |&gt; \n  ggplot(aes(x = wday)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\nflights_dt |&gt; \n  mutate(minute = minute(dep_time)) |&gt; \n  group_by(minute) |&gt; \n  summarize(\n    avg_delay = mean(dep_delay, na.rm = TRUE),\n    n = n()\n  ) |&gt; \n  ggplot(aes(x = minute, y = avg_delay)) +\n  geom_line()\n\n\n\n\n\n\n\n\ncontrast with scheduled departure time\n\nsched_dep &lt;- flights_dt |&gt; \n  mutate(minute = minute(sched_dep_time)) |&gt; \n  group_by(minute) |&gt; \n  summarize(\n    avg_delay = mean(arr_delay, na.rm = TRUE),\n    n = n()\n  )\n\nggplot(sched_dep, aes(x = minute, y = avg_delay)) +\n  geom_line()\n\n\n\n\n\n\n\n\nRounding dates with floor_date(), round_date(), and ceiling_date().\nAn example of plotting the number of flights per week.\n\nflights_dt |&gt; \n  count(week = floor_date(dep_time, \"week\")) |&gt; \n  ggplot(aes(x = week, y = n)) +\n  geom_line() + \n  geom_point()\n\n\n\n\n\n\n\n\nShow distribution of flights across the day:\n\nflights_dt |&gt; \n  mutate(dep_hour = dep_time - floor_date(dep_time, \"day\")) |&gt; \n  ggplot(aes(x = dep_hour)) +\n  geom_freqpoly(binwidth = 60 * 30)\n\nDon't know how to automatically pick scale for object of type &lt;difftime&gt;.\nDefaulting to continuous.\n\n\n\n\n\n\n\n\n#&gt; Don't know how to automatically pick scale for object of type &lt;difftime&gt;.\n#&gt; Defaulting to continuous.\n\nModifying components. You can modify components of a date/time.\n\n(datetime &lt;- ymd_hms(\"2026-07-08 12:34:56\"))\n\n[1] \"2026-07-08 12:34:56 UTC\"\n\n#&gt; [1] \"2026-07-08 12:34:56 UTC\"\n\nyear(datetime) &lt;- 2030\ndatetime\n\n[1] \"2030-07-08 12:34:56 UTC\"\n\n#&gt; [1] \"2030-07-08 12:34:56 UTC\"\nmonth(datetime) &lt;- 01\ndatetime\n\n[1] \"2030-01-08 12:34:56 UTC\"\n\n#&gt; [1] \"2030-01-08 12:34:56 UTC\"\nhour(datetime) &lt;- hour(datetime) + 1\ndatetime\n\n[1] \"2030-01-08 13:34:56 UTC\"\n\n#&gt; [1] \"2030-01-08 13:34:56 UTC\"\n\nYou can modify multiple components at once:\n\nupdate(datetime, year = 2030, month = 2, mday = 2, hour = 2)\n\n[1] \"2030-02-02 02:34:56 UTC\"\n\n#&gt; [1] \"2030-02-02 02:34:56 UTC\"\n#&gt; \n\nupdate(ymd(\"2023-02-01\"), mday = 30)\n\n[1] \"2023-03-02\"\n\n#&gt; [1] \"2023-03-02\"\nupdate(ymd(\"2023-02-01\"), hour = 400)\n\n[1] \"2023-02-17 16:00:00 UTC\"\n\n#&gt; [1] \"2023-02-17 16:00:00 UTC\"\n\n\n\n\nDurations, which represent an exact number of seconds. Periods, which represent human units like weeks and months. Intervals, which represent a starting and ending point.\nHow do you pick between duration, periods, and intervals? As always, pick the simplest data structure that solves your problem. If you only care about physical time, use a duration; if you need to add human times, use a period; if you need to figure out how long a span is in human units, use an interval.\nDurations. Subtracting two times gives you a difftime object:\n\n# How old is Edward?\nh_age &lt;- today() - ymd(\"1989-04-16\")\nh_age\n\nTime difference of 13446 days\n\n#&gt; Time difference of 16916 days\n\ndifftimes can be hard to work with, so it can be useful to convert to a form that always uses seconds: the duration.\n\nas.duration(h_age)\n\n[1] \"1161734400s (~36.81 years)\"\n\n#&gt; [1] \"1461542400s (~46.31 years)\"\n\ndurations come with convenient constructors:\n\ndseconds(15)\n\n[1] \"15s\"\n\n#&gt; [1] \"15s\"\ndminutes(10)\n\n[1] \"600s (~10 minutes)\"\n\n#&gt; [1] \"600s (~10 minutes)\"\ndhours(c(12, 24))\n\n[1] \"43200s (~12 hours)\" \"86400s (~1 days)\"  \n\n#&gt; [1] \"43200s (~12 hours)\" \"86400s (~1 days)\"\nddays(0:5)\n\n[1] \"0s\"                \"86400s (~1 days)\"  \"172800s (~2 days)\"\n[4] \"259200s (~3 days)\" \"345600s (~4 days)\" \"432000s (~5 days)\"\n\n#&gt; [1] \"0s\"                \"86400s (~1 days)\"  \"172800s (~2 days)\"\n#&gt; [4] \"259200s (~3 days)\" \"345600s (~4 days)\" \"432000s (~5 days)\"\ndweeks(3)\n\n[1] \"1814400s (~3 weeks)\"\n\n#&gt; [1] \"1814400s (~3 weeks)\"\ndyears(1)\n\n[1] \"31557600s (~1 years)\"\n\n#&gt; [1] \"31557600s (~1 years)\"\n\nYou can add and multiply durations:\n\n2 * dyears(1)\n\n[1] \"63115200s (~2 years)\"\n\n#&gt; [1] \"63115200s (~2 years)\"\ndyears(1) + dweeks(12) + dhours(15)\n\n[1] \"38869200s (~1.23 years)\"\n\n#&gt; [1] \"38869200s (~1.23 years)\"\n\nSometimes with durations you get unexpected results because of things like daylight savings time.\n\none_am &lt;- ymd_hms(\"2026-03-08 01:00:00\", tz = \"America/New_York\")\n\none_am\n\n[1] \"2026-03-08 01:00:00 EST\"\n\n#&gt; [1] \"2026-03-08 01:00:00 EST\"\none_am + ddays(1)\n\n[1] \"2026-03-09 02:00:00 EDT\"\n\n#&gt; [1] \"2026-03-09 02:00:00 EDT\"\n\nPeriods solve this for you. These work like human times. They don’t have a fixed length in seconds.\n\none_am\n\n[1] \"2026-03-08 01:00:00 EST\"\n\n#&gt; [1] \"2026-03-08 01:00:00 EST\"\none_am + days(1)\n\n[1] \"2026-03-09 01:00:00 EDT\"\n\n#&gt; [1] \"2026-03-09 01:00:00 EDT\"\n\nperiods can be constructed with a number of helper functions:\n\nhours(c(12, 24))\n\n[1] \"12H 0M 0S\" \"24H 0M 0S\"\n\n#&gt; [1] \"12H 0M 0S\" \"24H 0M 0S\"\ndays(7)\n\n[1] \"7d 0H 0M 0S\"\n\n#&gt; [1] \"7d 0H 0M 0S\"\nmonths(1:6)\n\n[1] \"1m 0d 0H 0M 0S\" \"2m 0d 0H 0M 0S\" \"3m 0d 0H 0M 0S\" \"4m 0d 0H 0M 0S\"\n[5] \"5m 0d 0H 0M 0S\" \"6m 0d 0H 0M 0S\"\n\n#&gt; [1] \"1m 0d 0H 0M 0S\" \"2m 0d 0H 0M 0S\" \"3m 0d 0H 0M 0S\" \"4m 0d 0H 0M 0S\"\n#&gt; [5] \"5m 0d 0H 0M 0S\" \"6m 0d 0H 0M 0S\"\n\nYou can add and multiply periods\n\n10 * (months(6) + days(1))\n\n[1] \"60m 10d 0H 0M 0S\"\n\n#&gt; [1] \"60m 10d 0H 0M 0S\"\ndays(50) + hours(25) + minutes(2)\n\n[1] \"50d 25H 2M 0S\"\n\n#&gt; [1] \"50d 25H 2M 0S\"\n\nAnd add them to dates\n\n# A leap year\nymd(\"2024-01-01\") + dyears(1)\n\n[1] \"2024-12-31 06:00:00 UTC\"\n\n#&gt; [1] \"2024-12-31 06:00:00 UTC\"\nymd(\"2024-01-01\") + years(1)\n\n[1] \"2025-01-01\"\n\n#&gt; [1] \"2025-01-01\"\n\n# Daylight saving time\none_am + ddays(1)\n\n[1] \"2026-03-09 02:00:00 EDT\"\n\n#&gt; [1] \"2026-03-09 02:00:00 EDT\"\none_am + days(1)\n\n[1] \"2026-03-09 01:00:00 EDT\"\n\n#&gt; [1] \"2026-03-09 01:00:00 EDT\"\n\nUsing periods to fix erroneous data:\n\nflights_dt |&gt; \n  filter(arr_time &lt; dep_time) \n\n# A tibble: 10,633 × 9\n   origin dest  dep_delay arr_delay dep_time            sched_dep_time     \n   &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dttm&gt;              &lt;dttm&gt;             \n 1 EWR    BQN           9        -4 2013-01-01 19:29:00 2013-01-01 19:20:00\n 2 JFK    DFW          59        NA 2013-01-01 19:39:00 2013-01-01 18:40:00\n 3 EWR    TPA          -2         9 2013-01-01 20:58:00 2013-01-01 21:00:00\n 4 EWR    SJU          -6       -12 2013-01-01 21:02:00 2013-01-01 21:08:00\n 5 EWR    SFO          11       -14 2013-01-01 21:08:00 2013-01-01 20:57:00\n 6 LGA    FLL         -10        -2 2013-01-01 21:20:00 2013-01-01 21:30:00\n 7 EWR    MCO          41        43 2013-01-01 21:21:00 2013-01-01 20:40:00\n 8 JFK    LAX          -7       -24 2013-01-01 21:28:00 2013-01-01 21:35:00\n 9 EWR    FLL          49        28 2013-01-01 21:34:00 2013-01-01 20:45:00\n10 EWR    FLL          -9       -14 2013-01-01 21:36:00 2013-01-01 21:45:00\n# ℹ 10,623 more rows\n# ℹ 3 more variables: arr_time &lt;dttm&gt;, sched_arr_time &lt;dttm&gt;, air_time &lt;dbl&gt;\n\n#&gt; # A tibble: 10,633 × 9\n#&gt;   origin dest  dep_delay arr_delay dep_time            sched_dep_time     \n#&gt;   &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dttm&gt;              &lt;dttm&gt;             \n#&gt; 1 EWR    BQN           9        -4 2013-01-01 19:29:00 2013-01-01 19:20:00\n#&gt; 2 JFK    DFW          59        NA 2013-01-01 19:39:00 2013-01-01 18:40:00\n#&gt; 3 EWR    TPA          -2         9 2013-01-01 20:58:00 2013-01-01 21:00:00\n#&gt; 4 EWR    SJU          -6       -12 2013-01-01 21:02:00 2013-01-01 21:08:00\n#&gt; 5 EWR    SFO          11       -14 2013-01-01 21:08:00 2013-01-01 20:57:00\n#&gt; 6 LGA    FLL         -10        -2 2013-01-01 21:20:00 2013-01-01 21:30:00\n#&gt; # ℹ 10,627 more rows\n#&gt; # ℹ 3 more variables: arr_time &lt;dttm&gt;, sched_arr_time &lt;dttm&gt;, …\n\n\nflights_dt &lt;- flights_dt |&gt; \n  mutate(\n    overnight = arr_time &lt; dep_time,\n    arr_time = arr_time + days(overnight),\n    sched_arr_time = sched_arr_time + days(overnight)\n  )\n\nIntervals. lubridate estimates some calculations like this:\n\nyears(1) / days(1)\n\n[1] 365.25\n\n#&gt; [1] 365.25\n\nAn interval is a pair of starting and ending date times, or you can think of it as a duration with a starting point.\nCreate an interval by writing start %--% end.\n\ny2023 &lt;- ymd(\"2023-01-01\") %--% ymd(\"2024-01-01\")\ny2024 &lt;- ymd(\"2024-01-01\") %--% ymd(\"2025-01-01\")\n\ny2023\n\n[1] 2023-01-01 UTC--2024-01-01 UTC\n\n#&gt; [1] 2023-01-01 UTC--2024-01-01 UTC\ny2024\n\n[1] 2024-01-01 UTC--2025-01-01 UTC\n\n#&gt; [1] 2024-01-01 UTC--2025-01-01 UTC\n\n\ny2023 / days(1)\n\n[1] 365\n\n#&gt; [1] 365\ny2024 / days(1)\n\n[1] 366\n\n#&gt; [1] 366\n\n\n\n\n\nx1 &lt;- ymd_hms(\"2024-06-01 12:00:00\", tz = \"America/New_York\")\nx1\n\n[1] \"2024-06-01 12:00:00 EDT\"\n\n#&gt; [1] \"2024-06-01 12:00:00 EDT\"\n\nx2 &lt;- ymd_hms(\"2024-06-01 18:00:00\", tz = \"Europe/Copenhagen\")\nx2\n\n[1] \"2024-06-01 18:00:00 CEST\"\n\n#&gt; [1] \"2024-06-01 18:00:00 CEST\"\n\nx3 &lt;- ymd_hms(\"2024-06-02 04:00:00\", tz = \"Pacific/Auckland\")\nx3\n\n[1] \"2024-06-02 04:00:00 NZST\"\n\n#&gt; [1] \"2024-06-02 04:00:00 NZST\"\n\n\nx1 - x2\n\nTime difference of 0 secs\n\n#&gt; Time difference of 0 secs\nx1 - x3\n\nTime difference of 0 secs\n\n#&gt; Time difference of 0 secs\n\n\nx4 &lt;- c(x1, x2, x3)\nx4\n\n[1] \"2024-06-01 12:00:00 EDT\" \"2024-06-01 12:00:00 EDT\"\n[3] \"2024-06-01 12:00:00 EDT\"\n\n#&gt; [1] \"2024-06-01 12:00:00 EDT\" \"2024-06-01 12:00:00 EDT\"\n#&gt; [3] \"2024-06-01 12:00:00 EDT\"\n\n\nx4b &lt;- force_tz(x4, tzone = \"Australia/Lord_Howe\")\nx4b\n\n[1] \"2024-06-01 12:00:00 +1030\" \"2024-06-01 12:00:00 +1030\"\n[3] \"2024-06-01 12:00:00 +1030\"\n\n#&gt; [1] \"2024-06-01 12:00:00 +1030\" \"2024-06-01 12:00:00 +1030\"\n#&gt; [3] \"2024-06-01 12:00:00 +1030\"\nx4b - x4\n\nTime differences in hours\n[1] -14.5 -14.5 -14.5\n\n#&gt; Time differences in hours\n#&gt; [1] -14.5 -14.5 -14.5"
  },
  {
    "objectID": "chapter_17_notes.html#creating-datetimes",
    "href": "chapter_17_notes.html#creating-datetimes",
    "title": "R for Data Science, 2nd Edition - Chapter 17 Notes",
    "section": "",
    "text": "There are three types of date/time data: date, time, date-time.\n\ncsv &lt;- \"\n  date,datetime\n  2022-01-02,2022-01-02 05:12\n\"\nread_csv(csv)\n\nWarning: The `file` argument of `vroom()` must use `I()` for literal data as of vroom\n1.5.0.\n  \n  # Bad:\n  vroom(\"X,Y\\n1.5,2.3\\n\")\n  \n  # Good:\n  vroom(I(\"X,Y\\n1.5,2.3\\n\"))\nℹ The deprecated feature was likely used in the readr package.\n  Please report the issue at &lt;https://github.com/tidyverse/readr/issues&gt;.\n\n\nRows: 1 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndttm (1): datetime\ndate (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 1 × 2\n  date       datetime           \n  &lt;date&gt;     &lt;dttm&gt;             \n1 2022-01-02 2022-01-02 05:12:00\n\n#&gt; Warning: The `file` argument of `vroom()` must use `I()` for literal data as of vroom\n#&gt; 1.5.0.\n#&gt;   \n#&gt;   # Bad:\n#&gt;   vroom(\"X,Y\\n1.5,2.3\\n\")\n#&gt;   \n#&gt;   # Good:\n#&gt;   vroom(I(\"X,Y\\n1.5,2.3\\n\"))\n#&gt; ℹ The deprecated feature was likely used in the readr package.\n#&gt;   Please report the issue at &lt;https://github.com/tidyverse/readr/issues&gt;.\n#&gt; # A tibble: 1 × 2\n#&gt;   date       datetime           \n#&gt;   &lt;date&gt;     &lt;dttm&gt;             \n#&gt; 1 2022-01-02 2022-01-02 05:12:00\n\nA few options for an ambigous date:\n\ncsv &lt;- \"\n  date\n  01/02/15\n\"\n\nread_csv(csv, col_types = cols(date = col_date(\"%m/%d/%y\")))\n\n# A tibble: 1 × 1\n  date      \n  &lt;date&gt;    \n1 2015-01-02\n\n#&gt; # A tibble: 1 × 1\n#&gt;   date      \n#&gt;   &lt;date&gt;    \n#&gt; 1 2015-01-02\n\nread_csv(csv, col_types = cols(date = col_date(\"%d/%m/%y\")))\n\n# A tibble: 1 × 1\n  date      \n  &lt;date&gt;    \n1 2015-02-01\n\n#&gt; # A tibble: 1 × 1\n#&gt;   date      \n#&gt;   &lt;date&gt;    \n#&gt; 1 2015-02-01\n\nread_csv(csv, col_types = cols(date = col_date(\"%y/%m/%d\")))\n\n# A tibble: 1 × 1\n  date      \n  &lt;date&gt;    \n1 2001-02-15\n\n#&gt; # A tibble: 1 × 1\n#&gt;   date      \n#&gt;   &lt;date&gt;    \n#&gt; 1 2001-02-15\n\nYou can also parse dates from strings using the ymd, mdy, and dmy functions.\n\nymd(\"2017-01-31\")\n\n[1] \"2017-01-31\"\n\n#&gt; [1] \"2017-01-31\"\nmdy(\"January 31st, 2017\")\n\n[1] \"2017-01-31\"\n\n#&gt; [1] \"2017-01-31\"\ndmy(\"31-Jan-2017\")\n\n[1] \"2017-01-31\"\n\n#&gt; [1] \"2017-01-31\"\n\nTo create date-times, just add an underscore and then one or more of “h”, “m”, or “s”.\n\nymd_hms(\"2017-01-31 20:11:59\")\n\n[1] \"2017-01-31 20:11:59 UTC\"\n\n#&gt; [1] \"2017-01-31 20:11:59 UTC\"\nmdy_hm(\"01/31/2017 08:01\")\n\n[1] \"2017-01-31 08:01:00 UTC\"\n\n#&gt; [1] \"2017-01-31 08:01:00 UTC\"\n\nSometimes date components are spread across columns:\n\nflights |&gt; \n  select(year, month, day, hour, minute)\n\n# A tibble: 336,776 × 5\n    year month   day  hour minute\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1  2013     1     1     5     15\n 2  2013     1     1     5     29\n 3  2013     1     1     5     40\n 4  2013     1     1     5     45\n 5  2013     1     1     6      0\n 6  2013     1     1     5     58\n 7  2013     1     1     6      0\n 8  2013     1     1     6      0\n 9  2013     1     1     6      0\n10  2013     1     1     6      0\n# ℹ 336,766 more rows\n\n#&gt; # A tibble: 336,776 × 5\n#&gt;    year month   day  hour minute\n#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1  2013     1     1     5     15\n#&gt; 2  2013     1     1     5     29\n#&gt; 3  2013     1     1     5     40\n#&gt; 4  2013     1     1     5     45\n#&gt; 5  2013     1     1     6      0\n#&gt; 6  2013     1     1     5     58\n#&gt; # ℹ 336,770 more rows\n\nThe make_dates() function is useful here:\n\nflights |&gt; \n  select(year, month, day, hour, minute) |&gt; \n  mutate(departure = make_datetime(year, month, day, hour, minute))\n\n# A tibble: 336,776 × 6\n    year month   day  hour minute departure          \n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dttm&gt;             \n 1  2013     1     1     5     15 2013-01-01 05:15:00\n 2  2013     1     1     5     29 2013-01-01 05:29:00\n 3  2013     1     1     5     40 2013-01-01 05:40:00\n 4  2013     1     1     5     45 2013-01-01 05:45:00\n 5  2013     1     1     6      0 2013-01-01 06:00:00\n 6  2013     1     1     5     58 2013-01-01 05:58:00\n 7  2013     1     1     6      0 2013-01-01 06:00:00\n 8  2013     1     1     6      0 2013-01-01 06:00:00\n 9  2013     1     1     6      0 2013-01-01 06:00:00\n10  2013     1     1     6      0 2013-01-01 06:00:00\n# ℹ 336,766 more rows\n\n#&gt; # A tibble: 336,776 × 6\n#&gt;    year month   day  hour minute departure          \n#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dttm&gt;             \n#&gt; 1  2013     1     1     5     15 2013-01-01 05:15:00\n#&gt; 2  2013     1     1     5     29 2013-01-01 05:29:00\n#&gt; 3  2013     1     1     5     40 2013-01-01 05:40:00\n#&gt; 4  2013     1     1     5     45 2013-01-01 05:45:00\n#&gt; 5  2013     1     1     6      0 2013-01-01 06:00:00\n#&gt; 6  2013     1     1     5     58 2013-01-01 05:58:00\n#&gt; # ℹ 336,770 more rows\n\nUse modulus arithmetic to pull out hour and minute components.\n\nmake_datetime_100 &lt;- function(year, month, day, time) {\n  make_datetime(year, month, day, time %/% 100, time %% 100)\n}\n\nflights_dt &lt;- flights |&gt; \n  filter(!is.na(dep_time), !is.na(arr_time)) |&gt; \n  mutate(\n    dep_time = make_datetime_100(year, month, day, dep_time),\n    arr_time = make_datetime_100(year, month, day, arr_time),\n    sched_dep_time = make_datetime_100(year, month, day, sched_dep_time),\n    sched_arr_time = make_datetime_100(year, month, day, sched_arr_time)\n  ) |&gt; \n  select(origin, dest, ends_with(\"delay\"), ends_with(\"time\"))\n\nflights_dt\n\n# A tibble: 328,063 × 9\n   origin dest  dep_delay arr_delay dep_time            sched_dep_time     \n   &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dttm&gt;              &lt;dttm&gt;             \n 1 EWR    IAH           2        11 2013-01-01 05:17:00 2013-01-01 05:15:00\n 2 LGA    IAH           4        20 2013-01-01 05:33:00 2013-01-01 05:29:00\n 3 JFK    MIA           2        33 2013-01-01 05:42:00 2013-01-01 05:40:00\n 4 JFK    BQN          -1       -18 2013-01-01 05:44:00 2013-01-01 05:45:00\n 5 LGA    ATL          -6       -25 2013-01-01 05:54:00 2013-01-01 06:00:00\n 6 EWR    ORD          -4        12 2013-01-01 05:54:00 2013-01-01 05:58:00\n 7 EWR    FLL          -5        19 2013-01-01 05:55:00 2013-01-01 06:00:00\n 8 LGA    IAD          -3       -14 2013-01-01 05:57:00 2013-01-01 06:00:00\n 9 JFK    MCO          -3        -8 2013-01-01 05:57:00 2013-01-01 06:00:00\n10 LGA    ORD          -2         8 2013-01-01 05:58:00 2013-01-01 06:00:00\n# ℹ 328,053 more rows\n# ℹ 3 more variables: arr_time &lt;dttm&gt;, sched_arr_time &lt;dttm&gt;, air_time &lt;dbl&gt;\n\n#&gt; # A tibble: 328,063 × 9\n#&gt;   origin dest  dep_delay arr_delay dep_time            sched_dep_time     \n#&gt;   &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dttm&gt;              &lt;dttm&gt;             \n#&gt; 1 EWR    IAH           2        11 2013-01-01 05:17:00 2013-01-01 05:15:00\n#&gt; 2 LGA    IAH           4        20 2013-01-01 05:33:00 2013-01-01 05:29:00\n#&gt; 3 JFK    MIA           2        33 2013-01-01 05:42:00 2013-01-01 05:40:00\n#&gt; 4 JFK    BQN          -1       -18 2013-01-01 05:44:00 2013-01-01 05:45:00\n#&gt; 5 LGA    ATL          -6       -25 2013-01-01 05:54:00 2013-01-01 06:00:00\n#&gt; 6 EWR    ORD          -4        12 2013-01-01 05:54:00 2013-01-01 05:58:00\n#&gt; # ℹ 328,057 more rows\n#&gt; # ℹ 3 more variables: arr_time &lt;dttm&gt;, sched_arr_time &lt;dttm&gt;, …\n\nVisualize departure time across the year\n\nflights_dt |&gt; \n  ggplot(aes(x = dep_time)) + \n  geom_freqpoly(binwidth = 86400) # 86400 seconds = 1 day\n\n\n\n\n\n\n\n\nor a single day\n\nflights_dt |&gt; \n  filter(dep_time &lt; ymd(20130102)) |&gt; \n  ggplot(aes(x = dep_time)) + \n  geom_freqpoly(binwidth = 600) # 600 s = 10 minutes\n\n\n\n\n\n\n\n\n\nas_datetime(today())\n\n[1] \"2026-02-07 UTC\"\n\n#&gt; [1] \"2026-02-05 UTC\"\nas_date(now())\n\n[1] \"2026-02-07\"\n\n#&gt; [1] \"2026-02-05\"\n\nParse the following with a readr column specification and a lubridate function. This is a good exercise.\n\nd1 &lt;- \"January 1, 2010\"\nd2 &lt;- \"2015-Mar-07\"\nd3 &lt;- \"06-Jun-2017\"\nd4 &lt;- c(\"August 19 (2015)\", \"July 1 (2015)\")\nd5 &lt;- \"12/30/14\" # Dec 30, 2014\nt1 &lt;- \"1705\"\nt2 &lt;- \"11:15:10.12 PM\""
  },
  {
    "objectID": "chapter_17_notes.html#date-time-components",
    "href": "chapter_17_notes.html#date-time-components",
    "title": "R for Data Science, 2nd Edition - Chapter 17 Notes",
    "section": "",
    "text": "You can pull out individual parts of the date with the accessor functions year(), month(), mday() (day of the month), yday() (day of the year), wday() (day of the week), hour(), minute(), and second(). These are effectively the opposites of make_datetime().\n\ndatetime &lt;- ymd_hms(\"2026-07-08 12:34:56\")\n\nyear(datetime)\n\n[1] 2026\n\n#&gt; [1] 2026\nmonth(datetime)\n\n[1] 7\n\n#&gt; [1] 7\nmday(datetime)\n\n[1] 8\n\n#&gt; [1] 8\n\nyday(datetime)\n\n[1] 189\n\n#&gt; [1] 189\nwday(datetime)\n\n[1] 4\n\n#&gt; [1] 4\n\nFor month() and wday() you can set label = TRUE to return the abbreviated name of the month or day of the week. Set abbr = FALSE to return the full name.\n\nflights_dt |&gt; \n  mutate(wday = wday(dep_time, label = TRUE)) |&gt; \n  ggplot(aes(x = wday)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\nflights_dt |&gt; \n  mutate(minute = minute(dep_time)) |&gt; \n  group_by(minute) |&gt; \n  summarize(\n    avg_delay = mean(dep_delay, na.rm = TRUE),\n    n = n()\n  ) |&gt; \n  ggplot(aes(x = minute, y = avg_delay)) +\n  geom_line()\n\n\n\n\n\n\n\n\ncontrast with scheduled departure time\n\nsched_dep &lt;- flights_dt |&gt; \n  mutate(minute = minute(sched_dep_time)) |&gt; \n  group_by(minute) |&gt; \n  summarize(\n    avg_delay = mean(arr_delay, na.rm = TRUE),\n    n = n()\n  )\n\nggplot(sched_dep, aes(x = minute, y = avg_delay)) +\n  geom_line()\n\n\n\n\n\n\n\n\nRounding dates with floor_date(), round_date(), and ceiling_date().\nAn example of plotting the number of flights per week.\n\nflights_dt |&gt; \n  count(week = floor_date(dep_time, \"week\")) |&gt; \n  ggplot(aes(x = week, y = n)) +\n  geom_line() + \n  geom_point()\n\n\n\n\n\n\n\n\nShow distribution of flights across the day:\n\nflights_dt |&gt; \n  mutate(dep_hour = dep_time - floor_date(dep_time, \"day\")) |&gt; \n  ggplot(aes(x = dep_hour)) +\n  geom_freqpoly(binwidth = 60 * 30)\n\nDon't know how to automatically pick scale for object of type &lt;difftime&gt;.\nDefaulting to continuous.\n\n\n\n\n\n\n\n\n#&gt; Don't know how to automatically pick scale for object of type &lt;difftime&gt;.\n#&gt; Defaulting to continuous.\n\nModifying components. You can modify components of a date/time.\n\n(datetime &lt;- ymd_hms(\"2026-07-08 12:34:56\"))\n\n[1] \"2026-07-08 12:34:56 UTC\"\n\n#&gt; [1] \"2026-07-08 12:34:56 UTC\"\n\nyear(datetime) &lt;- 2030\ndatetime\n\n[1] \"2030-07-08 12:34:56 UTC\"\n\n#&gt; [1] \"2030-07-08 12:34:56 UTC\"\nmonth(datetime) &lt;- 01\ndatetime\n\n[1] \"2030-01-08 12:34:56 UTC\"\n\n#&gt; [1] \"2030-01-08 12:34:56 UTC\"\nhour(datetime) &lt;- hour(datetime) + 1\ndatetime\n\n[1] \"2030-01-08 13:34:56 UTC\"\n\n#&gt; [1] \"2030-01-08 13:34:56 UTC\"\n\nYou can modify multiple components at once:\n\nupdate(datetime, year = 2030, month = 2, mday = 2, hour = 2)\n\n[1] \"2030-02-02 02:34:56 UTC\"\n\n#&gt; [1] \"2030-02-02 02:34:56 UTC\"\n#&gt; \n\nupdate(ymd(\"2023-02-01\"), mday = 30)\n\n[1] \"2023-03-02\"\n\n#&gt; [1] \"2023-03-02\"\nupdate(ymd(\"2023-02-01\"), hour = 400)\n\n[1] \"2023-02-17 16:00:00 UTC\"\n\n#&gt; [1] \"2023-02-17 16:00:00 UTC\""
  },
  {
    "objectID": "chapter_17_notes.html#time-spans",
    "href": "chapter_17_notes.html#time-spans",
    "title": "R for Data Science, 2nd Edition - Chapter 17 Notes",
    "section": "",
    "text": "Durations, which represent an exact number of seconds. Periods, which represent human units like weeks and months. Intervals, which represent a starting and ending point.\nHow do you pick between duration, periods, and intervals? As always, pick the simplest data structure that solves your problem. If you only care about physical time, use a duration; if you need to add human times, use a period; if you need to figure out how long a span is in human units, use an interval.\nDurations. Subtracting two times gives you a difftime object:\n\n# How old is Edward?\nh_age &lt;- today() - ymd(\"1989-04-16\")\nh_age\n\nTime difference of 13446 days\n\n#&gt; Time difference of 16916 days\n\ndifftimes can be hard to work with, so it can be useful to convert to a form that always uses seconds: the duration.\n\nas.duration(h_age)\n\n[1] \"1161734400s (~36.81 years)\"\n\n#&gt; [1] \"1461542400s (~46.31 years)\"\n\ndurations come with convenient constructors:\n\ndseconds(15)\n\n[1] \"15s\"\n\n#&gt; [1] \"15s\"\ndminutes(10)\n\n[1] \"600s (~10 minutes)\"\n\n#&gt; [1] \"600s (~10 minutes)\"\ndhours(c(12, 24))\n\n[1] \"43200s (~12 hours)\" \"86400s (~1 days)\"  \n\n#&gt; [1] \"43200s (~12 hours)\" \"86400s (~1 days)\"\nddays(0:5)\n\n[1] \"0s\"                \"86400s (~1 days)\"  \"172800s (~2 days)\"\n[4] \"259200s (~3 days)\" \"345600s (~4 days)\" \"432000s (~5 days)\"\n\n#&gt; [1] \"0s\"                \"86400s (~1 days)\"  \"172800s (~2 days)\"\n#&gt; [4] \"259200s (~3 days)\" \"345600s (~4 days)\" \"432000s (~5 days)\"\ndweeks(3)\n\n[1] \"1814400s (~3 weeks)\"\n\n#&gt; [1] \"1814400s (~3 weeks)\"\ndyears(1)\n\n[1] \"31557600s (~1 years)\"\n\n#&gt; [1] \"31557600s (~1 years)\"\n\nYou can add and multiply durations:\n\n2 * dyears(1)\n\n[1] \"63115200s (~2 years)\"\n\n#&gt; [1] \"63115200s (~2 years)\"\ndyears(1) + dweeks(12) + dhours(15)\n\n[1] \"38869200s (~1.23 years)\"\n\n#&gt; [1] \"38869200s (~1.23 years)\"\n\nSometimes with durations you get unexpected results because of things like daylight savings time.\n\none_am &lt;- ymd_hms(\"2026-03-08 01:00:00\", tz = \"America/New_York\")\n\none_am\n\n[1] \"2026-03-08 01:00:00 EST\"\n\n#&gt; [1] \"2026-03-08 01:00:00 EST\"\none_am + ddays(1)\n\n[1] \"2026-03-09 02:00:00 EDT\"\n\n#&gt; [1] \"2026-03-09 02:00:00 EDT\"\n\nPeriods solve this for you. These work like human times. They don’t have a fixed length in seconds.\n\none_am\n\n[1] \"2026-03-08 01:00:00 EST\"\n\n#&gt; [1] \"2026-03-08 01:00:00 EST\"\none_am + days(1)\n\n[1] \"2026-03-09 01:00:00 EDT\"\n\n#&gt; [1] \"2026-03-09 01:00:00 EDT\"\n\nperiods can be constructed with a number of helper functions:\n\nhours(c(12, 24))\n\n[1] \"12H 0M 0S\" \"24H 0M 0S\"\n\n#&gt; [1] \"12H 0M 0S\" \"24H 0M 0S\"\ndays(7)\n\n[1] \"7d 0H 0M 0S\"\n\n#&gt; [1] \"7d 0H 0M 0S\"\nmonths(1:6)\n\n[1] \"1m 0d 0H 0M 0S\" \"2m 0d 0H 0M 0S\" \"3m 0d 0H 0M 0S\" \"4m 0d 0H 0M 0S\"\n[5] \"5m 0d 0H 0M 0S\" \"6m 0d 0H 0M 0S\"\n\n#&gt; [1] \"1m 0d 0H 0M 0S\" \"2m 0d 0H 0M 0S\" \"3m 0d 0H 0M 0S\" \"4m 0d 0H 0M 0S\"\n#&gt; [5] \"5m 0d 0H 0M 0S\" \"6m 0d 0H 0M 0S\"\n\nYou can add and multiply periods\n\n10 * (months(6) + days(1))\n\n[1] \"60m 10d 0H 0M 0S\"\n\n#&gt; [1] \"60m 10d 0H 0M 0S\"\ndays(50) + hours(25) + minutes(2)\n\n[1] \"50d 25H 2M 0S\"\n\n#&gt; [1] \"50d 25H 2M 0S\"\n\nAnd add them to dates\n\n# A leap year\nymd(\"2024-01-01\") + dyears(1)\n\n[1] \"2024-12-31 06:00:00 UTC\"\n\n#&gt; [1] \"2024-12-31 06:00:00 UTC\"\nymd(\"2024-01-01\") + years(1)\n\n[1] \"2025-01-01\"\n\n#&gt; [1] \"2025-01-01\"\n\n# Daylight saving time\none_am + ddays(1)\n\n[1] \"2026-03-09 02:00:00 EDT\"\n\n#&gt; [1] \"2026-03-09 02:00:00 EDT\"\none_am + days(1)\n\n[1] \"2026-03-09 01:00:00 EDT\"\n\n#&gt; [1] \"2026-03-09 01:00:00 EDT\"\n\nUsing periods to fix erroneous data:\n\nflights_dt |&gt; \n  filter(arr_time &lt; dep_time) \n\n# A tibble: 10,633 × 9\n   origin dest  dep_delay arr_delay dep_time            sched_dep_time     \n   &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dttm&gt;              &lt;dttm&gt;             \n 1 EWR    BQN           9        -4 2013-01-01 19:29:00 2013-01-01 19:20:00\n 2 JFK    DFW          59        NA 2013-01-01 19:39:00 2013-01-01 18:40:00\n 3 EWR    TPA          -2         9 2013-01-01 20:58:00 2013-01-01 21:00:00\n 4 EWR    SJU          -6       -12 2013-01-01 21:02:00 2013-01-01 21:08:00\n 5 EWR    SFO          11       -14 2013-01-01 21:08:00 2013-01-01 20:57:00\n 6 LGA    FLL         -10        -2 2013-01-01 21:20:00 2013-01-01 21:30:00\n 7 EWR    MCO          41        43 2013-01-01 21:21:00 2013-01-01 20:40:00\n 8 JFK    LAX          -7       -24 2013-01-01 21:28:00 2013-01-01 21:35:00\n 9 EWR    FLL          49        28 2013-01-01 21:34:00 2013-01-01 20:45:00\n10 EWR    FLL          -9       -14 2013-01-01 21:36:00 2013-01-01 21:45:00\n# ℹ 10,623 more rows\n# ℹ 3 more variables: arr_time &lt;dttm&gt;, sched_arr_time &lt;dttm&gt;, air_time &lt;dbl&gt;\n\n#&gt; # A tibble: 10,633 × 9\n#&gt;   origin dest  dep_delay arr_delay dep_time            sched_dep_time     \n#&gt;   &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dttm&gt;              &lt;dttm&gt;             \n#&gt; 1 EWR    BQN           9        -4 2013-01-01 19:29:00 2013-01-01 19:20:00\n#&gt; 2 JFK    DFW          59        NA 2013-01-01 19:39:00 2013-01-01 18:40:00\n#&gt; 3 EWR    TPA          -2         9 2013-01-01 20:58:00 2013-01-01 21:00:00\n#&gt; 4 EWR    SJU          -6       -12 2013-01-01 21:02:00 2013-01-01 21:08:00\n#&gt; 5 EWR    SFO          11       -14 2013-01-01 21:08:00 2013-01-01 20:57:00\n#&gt; 6 LGA    FLL         -10        -2 2013-01-01 21:20:00 2013-01-01 21:30:00\n#&gt; # ℹ 10,627 more rows\n#&gt; # ℹ 3 more variables: arr_time &lt;dttm&gt;, sched_arr_time &lt;dttm&gt;, …\n\n\nflights_dt &lt;- flights_dt |&gt; \n  mutate(\n    overnight = arr_time &lt; dep_time,\n    arr_time = arr_time + days(overnight),\n    sched_arr_time = sched_arr_time + days(overnight)\n  )\n\nIntervals. lubridate estimates some calculations like this:\n\nyears(1) / days(1)\n\n[1] 365.25\n\n#&gt; [1] 365.25\n\nAn interval is a pair of starting and ending date times, or you can think of it as a duration with a starting point.\nCreate an interval by writing start %--% end.\n\ny2023 &lt;- ymd(\"2023-01-01\") %--% ymd(\"2024-01-01\")\ny2024 &lt;- ymd(\"2024-01-01\") %--% ymd(\"2025-01-01\")\n\ny2023\n\n[1] 2023-01-01 UTC--2024-01-01 UTC\n\n#&gt; [1] 2023-01-01 UTC--2024-01-01 UTC\ny2024\n\n[1] 2024-01-01 UTC--2025-01-01 UTC\n\n#&gt; [1] 2024-01-01 UTC--2025-01-01 UTC\n\n\ny2023 / days(1)\n\n[1] 365\n\n#&gt; [1] 365\ny2024 / days(1)\n\n[1] 366\n\n#&gt; [1] 366"
  },
  {
    "objectID": "chapter_17_notes.html#timezones",
    "href": "chapter_17_notes.html#timezones",
    "title": "R for Data Science, 2nd Edition - Chapter 17 Notes",
    "section": "",
    "text": "x1 &lt;- ymd_hms(\"2024-06-01 12:00:00\", tz = \"America/New_York\")\nx1\n\n[1] \"2024-06-01 12:00:00 EDT\"\n\n#&gt; [1] \"2024-06-01 12:00:00 EDT\"\n\nx2 &lt;- ymd_hms(\"2024-06-01 18:00:00\", tz = \"Europe/Copenhagen\")\nx2\n\n[1] \"2024-06-01 18:00:00 CEST\"\n\n#&gt; [1] \"2024-06-01 18:00:00 CEST\"\n\nx3 &lt;- ymd_hms(\"2024-06-02 04:00:00\", tz = \"Pacific/Auckland\")\nx3\n\n[1] \"2024-06-02 04:00:00 NZST\"\n\n#&gt; [1] \"2024-06-02 04:00:00 NZST\"\n\n\nx1 - x2\n\nTime difference of 0 secs\n\n#&gt; Time difference of 0 secs\nx1 - x3\n\nTime difference of 0 secs\n\n#&gt; Time difference of 0 secs\n\n\nx4 &lt;- c(x1, x2, x3)\nx4\n\n[1] \"2024-06-01 12:00:00 EDT\" \"2024-06-01 12:00:00 EDT\"\n[3] \"2024-06-01 12:00:00 EDT\"\n\n#&gt; [1] \"2024-06-01 12:00:00 EDT\" \"2024-06-01 12:00:00 EDT\"\n#&gt; [3] \"2024-06-01 12:00:00 EDT\"\n\n\nx4b &lt;- force_tz(x4, tzone = \"Australia/Lord_Howe\")\nx4b\n\n[1] \"2024-06-01 12:00:00 +1030\" \"2024-06-01 12:00:00 +1030\"\n[3] \"2024-06-01 12:00:00 +1030\"\n\n#&gt; [1] \"2024-06-01 12:00:00 +1030\" \"2024-06-01 12:00:00 +1030\"\n#&gt; [3] \"2024-06-01 12:00:00 +1030\"\nx4b - x4\n\nTime differences in hours\n[1] -14.5 -14.5 -14.5\n\n#&gt; Time differences in hours\n#&gt; [1] -14.5 -14.5 -14.5"
  },
  {
    "objectID": "chapter_26_notes.html",
    "href": "chapter_26_notes.html",
    "title": "R for Data Science, 2nd Edition - Chapter 26 Notes",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.6\n✔ forcats   1.0.1     ✔ stringr   1.6.0\n✔ ggplot2   4.0.1     ✔ tibble    3.3.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.2\n✔ purrr     1.2.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\n\nset.seed(1014)\ndf &lt;- tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\n\nHow to get medians of each column?\n\ndf |&gt; summarize(\n  n = n(),\n  a = median(a),\n  b = median(b),\n  c = median(c),\n  d = median(d),\n)\n\n# A tibble: 1 × 5\n      n      a      b       c     d\n  &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1    10 -0.246 -0.287 -0.0567 0.144\n\n#&gt; # A tibble: 1 × 5\n#&gt;       n      a      b       c     d\n#&gt;   &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1    10 -0.246 -0.287 -0.0567 0.144\n\nUse across().\n\ndf |&gt; summarize(\n  n = n(),\n  across(a:d, median),\n)\n\n# A tibble: 1 × 5\n      n      a      b       c     d\n  &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1    10 -0.246 -0.287 -0.0567 0.144\n\n#&gt; # A tibble: 1 × 5\n#&gt;       n      a      b       c     d\n#&gt;   &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1    10 -0.246 -0.287 -0.0567 0.144\n\nKey arguments: .cols, .fns, and you can use .names if you want to control names of output columns. if_any() and if_all() may be important within filter() calls.\n.cols can be supplied with the same syntax as select(), and everything() and where(). Use where() to select based on type.\n\nset.seed(1014)\ndf &lt;- tibble(\n  grp = sample(2, 10, replace = TRUE),\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\n\ndf |&gt; \n  group_by(grp) |&gt; \n  summarize(across(everything(), median))\n\n# A tibble: 2 × 5\n    grp      a      b       c       d\n  &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     1 -0.244 -0.522 -0.0974 -0.251 \n2     2 -0.247  0.468  0.112   0.0700\n\n#&gt; # A tibble: 2 × 5\n#&gt;     grp      a      b       c       d\n#&gt;   &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1     1 -0.244 -0.522 -0.0974 -0.251 \n#&gt; 2     2 -0.247  0.468  0.112   0.0700\n\nJust like other selectors, you can combine these with Boolean algebra. For example, !where(is.numeric) selects all non-numeric columns, and starts_with(\"a\") & where(is.logical) selects all logical columns whose name starts with “a”.\nCalling a single function.\nCalling multiple functions. (or multiple arguments). Here is an example of handling NA values in a call to median().\n\nset.seed(1014)\nrnorm_na &lt;- function(n, n_na, mean = 0, sd = 1) {\n  sample(c(rnorm(n - n_na, mean = mean, sd = sd), rep(NA, n_na)))\n}\n\ndf_miss &lt;- tibble(\n  a = rnorm_na(5, 1),\n  b = rnorm_na(5, 1),\n  c = rnorm_na(5, 2),\n  d = rnorm(5)\n)\ndf_miss |&gt; \n  summarize(\n    across(a:d, median),\n    n = n()\n  )\n\n# A tibble: 1 × 5\n      a     b     c     d     n\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1    NA    NA    NA 0.413     5\n\n#&gt; # A tibble: 1 × 5\n#&gt;       a     b     c     d     n\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n#&gt; 1    NA    NA    NA 0.413     5\n\nWe need to create a new function that calls median() with the desired arguments:\n\ndf_miss |&gt; \n  summarize(\n    across(a:d, function(x) median(x, na.rm = TRUE)),\n    n = n()\n  )\n\n# A tibble: 1 × 5\n       a      b      c     d     n\n   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 -0.703 -0.265 -0.522 0.413     5\n\n#&gt; # A tibble: 1 × 5\n#&gt;        a      b      c     d     n\n#&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n#&gt; 1 -0.703 -0.265 -0.522 0.413     5\n\nThis is verbose, so we can use anonymous function syntax (\\(x) recommended over ~ .x).\n\ndf_miss |&gt; \n  summarize(\n    across(a:d, \\(x) median(x, na.rm = TRUE)),\n    n = n()\n  )\n\n# A tibble: 1 × 5\n       a      b      c     d     n\n   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 -0.703 -0.265 -0.522 0.413     5\n\n\nIf you want to supply more than one function in the call to across(), use a named list:\n\ndf_miss |&gt; \n  summarize(\n    across(a:d, list(\n      median = \\(x) median(x, na.rm = TRUE),\n      n_miss = \\(x) sum(is.na(x))\n    )),\n    n = n()\n  )\n\n# A tibble: 1 × 9\n  a_median a_n_miss b_median b_n_miss c_median c_n_miss d_median d_n_miss     n\n     &lt;dbl&gt;    &lt;int&gt;    &lt;dbl&gt;    &lt;int&gt;    &lt;dbl&gt;    &lt;int&gt;    &lt;dbl&gt;    &lt;int&gt; &lt;int&gt;\n1   -0.703        1   -0.265        1   -0.522        2    0.413        0     5\n\n#&gt; # A tibble: 1 × 9\n#&gt;   a_median a_n_miss b_median b_n_miss c_median c_n_miss d_median d_n_miss\n#&gt;      &lt;dbl&gt;    &lt;int&gt;    &lt;dbl&gt;    &lt;int&gt;    &lt;dbl&gt;    &lt;int&gt;    &lt;dbl&gt;    &lt;int&gt;\n#&gt; 1   -0.703        1   -0.265        1   -0.522        2    0.413        0\n#&gt; # ℹ 1 more variable: n &lt;int&gt;\n\nColumn names are specified with the .names argument.\n\ndf_miss |&gt; \n  summarize(\n    across(\n      a:d,\n      list(\n        median = \\(x) median(x, na.rm = TRUE),\n        n_miss = \\(x) sum(is.na(x))\n      ),\n      .names = \"{.fn}_{.col}\"\n    ),\n    n = n(),\n  )\n\n# A tibble: 1 × 9\n  median_a n_miss_a median_b n_miss_b median_c n_miss_c median_d n_miss_d     n\n     &lt;dbl&gt;    &lt;int&gt;    &lt;dbl&gt;    &lt;int&gt;    &lt;dbl&gt;    &lt;int&gt;    &lt;dbl&gt;    &lt;int&gt; &lt;int&gt;\n1   -0.703        1   -0.265        1   -0.522        2    0.413        0     5\n\n#&gt; # A tibble: 1 × 9\n#&gt;   median_a n_miss_a median_b n_miss_b median_c n_miss_c median_d n_miss_d\n#&gt;      &lt;dbl&gt;    &lt;int&gt;    &lt;dbl&gt;    &lt;int&gt;    &lt;dbl&gt;    &lt;int&gt;    &lt;dbl&gt;    &lt;int&gt;\n#&gt; 1   -0.703        1   -0.265        1   -0.522        2    0.413        0\n#&gt; # ℹ 1 more variable: n &lt;int&gt;\n\n.names is especially helpful when calling mutate() within across() to prevent overwriting columns, since the output of across() is given the same name as the inputs.\n\ndf_miss |&gt; \n  mutate(\n    across(a:d, \\(x) coalesce(x, 0))\n  )\n\n# A tibble: 5 × 4\n         a      b      c        d\n     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n1 -0.00557 -0.283 -1.86  -0.783  \n2  0.255   -0.247 -0.522 -0.00289\n3 -1.40    -0.554  0.512  0.413  \n4 -2.44    -0.244  0      0.724  \n5  0        0      0      2.35   \n\n#&gt; # A tibble: 5 × 4\n#&gt;          a      b      c        d\n#&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1 -0.00557 -0.283 -1.86  -0.783  \n#&gt; 2  0.255   -0.247 -0.522 -0.00289\n#&gt; 3 -1.40    -0.554  0.512  0.413  \n#&gt; 4 -2.44    -0.244  0      0.724  \n#&gt; 5  0        0      0      2.35\n\nUse the .names argument to instead create new columns.\n\ndf_miss |&gt; \n  mutate(\n    across(a:d, \\(x) coalesce(x, 0), .names = \"{.col}_na_zero\")\n  )\n\n# A tibble: 5 × 8\n         a      b      c        d a_na_zero b_na_zero c_na_zero d_na_zero\n     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 -0.00557 -0.283 -1.86  -0.783    -0.00557    -0.283    -1.86   -0.783  \n2  0.255   -0.247 -0.522 -0.00289   0.255      -0.247    -0.522  -0.00289\n3 -1.40    -0.554  0.512  0.413    -1.40       -0.554     0.512   0.413  \n4 -2.44    -0.244 NA      0.724    -2.44       -0.244     0       0.724  \n5 NA       NA     NA      2.35      0           0         0       2.35   \n\n#&gt; # A tibble: 5 × 8\n#&gt;          a      b      c        d a_na_zero b_na_zero c_na_zero d_na_zero\n#&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 -0.00557 -0.283 -1.86  -0.783    -0.00557    -0.283    -1.86   -0.783  \n#&gt; 2  0.255   -0.247 -0.522 -0.00289   0.255      -0.247    -0.522  -0.00289\n#&gt; 3 -1.40    -0.554  0.512  0.413    -1.40       -0.554     0.512   0.413  \n#&gt; 4 -2.44    -0.244 NA      0.724    -2.44       -0.244     0       0.724  \n#&gt; 5 NA       NA     NA      2.35      0           0         0       2.35\n\nFiltering. Two variants of across() are provided for use inside of filter(): if_any() and if_all()\n\n# same as df_miss |&gt; filter(is.na(a) | is.na(b) | is.na(c) | is.na(d))\ndf_miss |&gt; filter(if_any(a:d, is.na))\n\n# A tibble: 2 × 4\n      a      b     c     d\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 -2.44 -0.244    NA 0.724\n2 NA    NA        NA 2.35 \n\n#&gt; # A tibble: 2 × 4\n#&gt;       a      b     c     d\n#&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 -2.44 -0.244    NA 0.724\n#&gt; 2 NA    NA        NA 2.35\n\n# same as df_miss |&gt; filter(is.na(a) & is.na(b) & is.na(c) & is.na(d))\ndf_miss |&gt; filter(if_all(a:d, is.na))\n\n# A tibble: 0 × 4\n# ℹ 4 variables: a &lt;dbl&gt;, b &lt;dbl&gt;, c &lt;dbl&gt;, d &lt;dbl&gt;\n\n#&gt; # A tibble: 0 × 4\n#&gt; # ℹ 4 variables: a &lt;dbl&gt;, b &lt;dbl&gt;, c &lt;dbl&gt;, d &lt;dbl&gt;\n\nacross() can also be quite useful inside of functions.\nA helper function to expand all date columns into year, month, and day columns:\n\nexpand_dates &lt;- function(df) {\n  df |&gt; \n    mutate(\n      across(where(is.Date), list(year = year, month = month, day = mday))\n    )\n}\n\ndf_date &lt;- tibble(\n  name = c(\"Amy\", \"Bob\"),\n  date = ymd(c(\"2009-08-03\", \"2010-01-16\"))\n)\n\ndf_date |&gt; \n  expand_dates()\n\n# A tibble: 2 × 5\n  name  date       date_year date_month date_day\n  &lt;chr&gt; &lt;date&gt;         &lt;dbl&gt;      &lt;dbl&gt;    &lt;int&gt;\n1 Amy   2009-08-03      2009          8        3\n2 Bob   2010-01-16      2010          1       16\n\n#&gt; # A tibble: 2 × 5\n#&gt;   name  date       date_year date_month date_day\n#&gt;   &lt;chr&gt; &lt;date&gt;         &lt;dbl&gt;      &lt;dbl&gt;    &lt;int&gt;\n#&gt; 1 Amy   2009-08-03      2009          8        3\n#&gt; 2 Bob   2010-01-16      2010          1       16\n\nDon’t forget to use{ } when selecting columns inside of a function, since across() uses tidy-select.\n\nsummarize_means &lt;- function(df, summary_vars = where(is.numeric)) {\n  df |&gt; \n    summarize(\n      across({{ summary_vars }}, \\(x) mean(x, na.rm = TRUE)),\n      n = n(),\n      .groups = \"drop\"\n    )\n}\n\ndiamonds |&gt; \n  group_by(cut) |&gt; \n  summarize_means()\n\n# A tibble: 5 × 9\n  cut       carat depth table price     x     y     z     n\n  &lt;ord&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 Fair      1.05   64.0  59.1 4359.  6.25  6.18  3.98  1610\n2 Good      0.849  62.4  58.7 3929.  5.84  5.85  3.64  4906\n3 Very Good 0.806  61.8  58.0 3982.  5.74  5.77  3.56 12082\n4 Premium   0.892  61.3  58.7 4584.  5.97  5.94  3.65 13791\n5 Ideal     0.703  61.7  56.0 3458.  5.51  5.52  3.40 21551\n\n#&gt; # A tibble: 5 × 9\n#&gt;   cut       carat depth table price     x     y     z     n\n#&gt;   &lt;ord&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n#&gt; 1 Fair      1.05   64.0  59.1 4359.  6.25  6.18  3.98  1610\n#&gt; 2 Good      0.849  62.4  58.7 3929.  5.84  5.85  3.64  4906\n#&gt; 3 Very Good 0.806  61.8  58.0 3982.  5.74  5.77  3.56 12082\n#&gt; 4 Premium   0.892  61.3  58.7 4584.  5.97  5.94  3.65 13791\n#&gt; 5 Ideal     0.703  61.7  56.0 3458.  5.51  5.52  3.40 21551\n\ndiamonds |&gt; \n  group_by(cut) |&gt; \n  summarize_means(c(carat, x:z))\n\n# A tibble: 5 × 6\n  cut       carat     x     y     z     n\n  &lt;ord&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 Fair      1.05   6.25  6.18  3.98  1610\n2 Good      0.849  5.84  5.85  3.64  4906\n3 Very Good 0.806  5.74  5.77  3.56 12082\n4 Premium   0.892  5.97  5.94  3.65 13791\n5 Ideal     0.703  5.51  5.52  3.40 21551\n\n#&gt; # A tibble: 5 × 6\n#&gt;   cut       carat     x     y     z     n\n#&gt;   &lt;ord&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n#&gt; 1 Fair      1.05   6.25  6.18  3.98  1610\n#&gt; 2 Good      0.849  5.84  5.85  3.64  4906\n#&gt; 3 Very Good 0.806  5.74  5.77  3.56 12082\n#&gt; 4 Premium   0.892  5.97  5.94  3.65 13791\n#&gt; 5 Ideal     0.703  5.51  5.52  3.40 21551\n\nThere is a connection between across() and pivot_longer().\n\ndf |&gt; \n  summarize(across(a:d, list(median = median, mean = mean)))\n\n# A tibble: 1 × 8\n  a_median  a_mean b_median  b_mean c_median  c_mean d_median d_mean\n     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n1   -0.246 -0.0426    0.155 -0.0656   0.0480 -0.0297   -0.193 -0.200\n\n#&gt; # A tibble: 1 × 8\n#&gt;   a_median  a_mean b_median  b_mean c_median  c_mean d_median d_mean\n#&gt;      &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1   -0.246 -0.0426    0.155 -0.0656   0.0480 -0.0297   -0.193 -0.200\n\nCompare this to:\n\nlong &lt;- df |&gt; \n  pivot_longer(a:d) |&gt; \n  group_by(name) |&gt; \n  summarize(\n    median = median(value),\n    mean = mean(value)\n  )\nlong\n\n# A tibble: 4 × 3\n  name   median    mean\n  &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 a     -0.246  -0.0426\n2 b      0.155  -0.0656\n3 c      0.0480 -0.0297\n4 d     -0.193  -0.200 \n\n#&gt; # A tibble: 4 × 3\n#&gt;   name   median    mean\n#&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1 a     -0.246  -0.0426\n#&gt; 2 b      0.155  -0.0656\n#&gt; 3 c      0.0480 -0.0297\n#&gt; 4 d     -0.193  -0.200\n\nAnd to get it back to the structure across() gives you, pivot_wider()\n\nlong |&gt; \n  pivot_wider(\n    names_from = name,\n    values_from = c(median, mean),\n    names_vary = \"slowest\",\n    names_glue = \"{name}_{.value}\"\n  )\n\n# A tibble: 1 × 8\n  a_median  a_mean b_median  b_mean c_median  c_mean d_median d_mean\n     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n1   -0.246 -0.0426    0.155 -0.0656   0.0480 -0.0297   -0.193 -0.200\n\n#&gt; # A tibble: 1 × 8\n#&gt;   a_median  a_mean b_median  b_mean c_median  c_mean d_median d_mean\n#&gt;      &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1   -0.246 -0.0426    0.155 -0.0656   0.0480 -0.0297   -0.193 -0.200\n\nThis is useful because occasionally you won’t be able to solve a problem with across(), such as groups of columns that you want to compute with simultaneously. Here is an example of a weighted mean:\n\nset.seed(1014)\ndf_paired &lt;- tibble(\n  a_val = rnorm(10),\n  a_wts = runif(10),\n  b_val = rnorm(10),\n  b_wts = runif(10),\n  c_val = rnorm(10),\n  c_wts = runif(10),\n  d_val = rnorm(10),\n  d_wts = runif(10)\n)\n\nYou can do this with pivot_longer().\n\ndf_long &lt;- df_paired |&gt; \n  pivot_longer(\n    everything(), \n    names_to = c(\"group\", \".value\"), \n    names_sep = \"_\"\n  )\ndf_long\n\n# A tibble: 40 × 3\n   group     val   wts\n   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1 a     -1.40   0.290\n 2 b     -1.86   0.461\n 3 c      0.935  0.528\n 4 d      2.76   0.709\n 5 a      0.255  0.678\n 6 b     -0.522  0.315\n 7 c      0.176  0.601\n 8 d      0.0465 0.874\n 9 a     -2.44   0.735\n10 b     -0.0526 0.175\n# ℹ 30 more rows\n\n#&gt; # A tibble: 40 × 3\n#&gt;   group    val   wts\n#&gt;   &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 a     -1.40  0.290\n#&gt; 2 b     -1.86  0.461\n#&gt; 3 c      0.935 0.528\n#&gt; 4 d      2.76  0.709\n#&gt; 5 a      0.255 0.678\n#&gt; 6 b     -0.522 0.315\n#&gt; # ℹ 34 more rows\n\ndf_long |&gt; \n  group_by(group) |&gt; \n  summarize(mean = weighted.mean(val, wts))\n\n# A tibble: 4 × 2\n  group    mean\n  &lt;chr&gt;   &lt;dbl&gt;\n1 a     -0.207 \n2 b     -0.237 \n3 c      0.0208\n4 d      0.0655\n\n#&gt; # A tibble: 4 × 2\n#&gt;   group    mean\n#&gt;   &lt;chr&gt;   &lt;dbl&gt;\n#&gt; 1 a     -0.207 \n#&gt; 2 b     -0.237 \n#&gt; 3 c      0.0208\n#&gt; 4 d      0.0655\n\nfind missing values after grouping on columns you choose.\n\nshow_missing &lt;- function(df, group_vars, summary_vars = everything()) {\n  df |&gt; \n    group_by(pick({{ group_vars }})) |&gt; \n    summarize(\n      across({{ summary_vars }}, \\(x) sum(is.na(x))),\n      .groups = \"drop\"\n    ) |&gt;\n    select(where(\\(x) any(x &gt; 0)))\n}\nnycflights13::flights |&gt; show_missing(c(year, month, day))\n\n# A tibble: 365 × 9\n    year month   day dep_time dep_delay arr_time arr_delay tailnum air_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;     &lt;int&gt;    &lt;int&gt;     &lt;int&gt;   &lt;int&gt;    &lt;int&gt;\n 1  2013     1     1        4         4        5        11       0       11\n 2  2013     1     2        8         8       10        15       2       15\n 3  2013     1     3       10        10       10        14       2       14\n 4  2013     1     4        6         6        6         7       2        7\n 5  2013     1     5        3         3        3         3       1        3\n 6  2013     1     6        1         1        1         3       0        3\n 7  2013     1     7        3         3        3         3       1        3\n 8  2013     1     8        4         4        4         7       1        7\n 9  2013     1     9        5         5        7         9       2        9\n10  2013     1    10        3         3        3         3       2        3\n# ℹ 355 more rows\n\n\n\n\n\nThe basic pattern is to use list.files(), then purrr:map(), then bind the rows together across elements of the list with purrr:list_bind().\n\n# \n# paths &lt;- list.files(\"data/gapminder\", pattern = \"[.]xlsx$\", full.names = TRUE)\n# paths\n#&gt;  [1] \"data/gapminder/1952.xlsx\" \"data/gapminder/1957.xlsx\"\n#&gt;  [3] \"data/gapminder/1962.xlsx\" \"data/gapminder/1967.xlsx\"\n#&gt;  [5] \"data/gapminder/1972.xlsx\" \"data/gapminder/1977.xlsx\"\n#&gt;  [7] \"data/gapminder/1982.xlsx\" \"data/gapminder/1987.xlsx\"\n#&gt;  [9] \"data/gapminder/1992.xlsx\" \"data/gapminder/1997.xlsx\"\n#&gt; [11] \"data/gapminder/2002.xlsx\" \"data/gapminder/2007.xlsx\"\n\nNow use map().\n\n# files &lt;- map(paths, readxl::read_excel)\n# length(files)\n# #&gt; [1] 12\n# \n# files[[1]]\n#&gt; # A tibble: 142 × 5\n#&gt;   country     continent lifeExp      pop gdpPercap\n#&gt;   &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 Afghanistan Asia         28.8  8425333      779.\n#&gt; 2 Albania     Europe       55.2  1282697     1601.\n#&gt; 3 Algeria     Africa       43.1  9279525     2449.\n#&gt; 4 Angola      Africa       30.0  4232095     3521.\n#&gt; 5 Argentina   Americas     62.5 17876956     5911.\n#&gt; 6 Australia   Oceania      69.1  8691212    10040.\n#&gt; # ℹ 136 more rows\n\nNow combine elements of the list with purrr::list_rbind().\n\n# list_rbind(files)\n# #&gt; # A tibble: 1,704 × 5\n# #&gt;   country     continent lifeExp      pop gdpPercap\n# #&gt;   &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n# #&gt; 1 Afghanistan Asia         28.8  8425333      779.\n# #&gt; 2 Albania     Europe       55.2  1282697     1601.\n# #&gt; 3 Algeria     Africa       43.1  9279525     2449.\n# #&gt; 4 Angola      Africa       30.0  4232095     3521.\n# #&gt; 5 Argentina   Americas     62.5 17876956     5911.\n# #&gt; 6 Australia   Oceania      69.1  8691212    10040.\n# #&gt; # ℹ 1,698 more rows\n\n# Or we could do this:\n\n# paths |&gt; \n#   map(readxl::read_excel) |&gt; \n#   list_rbind()\n\nAdding additional arguments to the function being applied to each element of the list can be accomplished with an anonymous function.\n\n# paths |&gt; \n#   map(\\(path) readxl::read_excel(path, n_max = 1)) |&gt; \n#   list_rbind()\n# #&gt; # A tibble: 12 × 5\n# #&gt;   country     continent lifeExp      pop gdpPercap\n# #&gt;   &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n# #&gt; 1 Afghanistan Asia         28.8  8425333      779.\n# #&gt; 2 Afghanistan Asia         30.3  9240934      821.\n# #&gt; 3 Afghanistan Asia         32.0 10267083      853.\n# #&gt; 4 Afghanistan Asia         34.0 11537966      836.\n# #&gt; 5 Afghanistan Asia         36.1 13079460      740.\n# #&gt; 6 Afghanistan Asia         38.4 14880372      786.\n# #&gt; # ℹ 6 more rows\n\nWhat if variables are in the file names?\n\n# paths |&gt; set_names(basename) \n# #&gt;                  1952.xlsx                  1957.xlsx \n# #&gt; \"data/gapminder/1952.xlsx\" \"data/gapminder/1957.xlsx\" \n# #&gt;                  1962.xlsx                  1967.xlsx \n# #&gt; \"data/gapminder/1962.xlsx\" \"data/gapminder/1967.xlsx\" \n# #&gt;                  1972.xlsx                  1977.xlsx \n# #&gt; \"data/gapminder/1972.xlsx\" \"data/gapminder/1977.xlsx\" \n# #&gt;                  1982.xlsx                  1987.xlsx \n# #&gt; \"data/gapminder/1982.xlsx\" \"data/gapminder/1987.xlsx\" \n# #&gt;                  1992.xlsx                  1997.xlsx \n# #&gt; \"data/gapminder/1992.xlsx\" \"data/gapminder/1997.xlsx\" \n# #&gt;                  2002.xlsx                  2007.xlsx \n# #&gt; \"data/gapminder/2002.xlsx\" \"data/gapminder/2007.xlsx\"\n# \n# files &lt;- paths |&gt; \n#   set_names(basename) |&gt; \n#   map(readxl::read_excel)\n\n\n# paths |&gt; \n#   set_names(basename) |&gt; \n#   map(readxl::read_excel) |&gt; \n#   list_rbind(names_to = \"year\") |&gt; \n#   mutate(year = parse_number(year))\n# #&gt; # A tibble: 1,704 × 6\n# #&gt;    year country     continent lifeExp      pop gdpPercap\n# #&gt;   &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n# #&gt; 1  1952 Afghanistan Asia         28.8  8425333      779.\n# #&gt; 2  1952 Albania     Europe       55.2  1282697     1601.\n# #&gt; 3  1952 Algeria     Africa       43.1  9279525     2449.\n# #&gt; 4  1952 Angola      Africa       30.0  4232095     3521.\n# #&gt; 5  1952 Argentina   Americas     62.5 17876956     5911.\n# #&gt; 6  1952 Australia   Oceania      69.1  8691212    10040.\n# #&gt; # ℹ 1,698 more rows\n\nIn cases where there is more than one variable in the file name:\n\n# paths |&gt; \n#   set_names() |&gt; \n#   map(readxl::read_excel) |&gt; \n#   list_rbind(names_to = \"year\") |&gt; \n#   separate_wider_delim(year, delim = \"/\", names = c(NA, \"dir\", \"file\")) |&gt; \n#   separate_wider_delim(file, delim = \".\", names = c(\"file\", \"ext\"))\n# #&gt; # A tibble: 1,704 × 8\n# #&gt;   dir       file  ext   country     continent lifeExp      pop gdpPercap\n# #&gt;   &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n# #&gt; 1 gapminder 1952  xlsx  Afghanistan Asia         28.8  8425333      779.\n# #&gt; 2 gapminder 1952  xlsx  Albania     Europe       55.2  1282697     1601.\n# #&gt; 3 gapminder 1952  xlsx  Algeria     Africa       43.1  9279525     2449.\n# #&gt; 4 gapminder 1952  xlsx  Angola      Africa       30.0  4232095     3521.\n# #&gt; 5 gapminder 1952  xlsx  Argentina   Americas     62.5 17876956     5911.\n# #&gt; 6 gapminder 1952  xlsx  Australia   Oceania      69.1  8691212    10040.\n# #&gt; # ℹ 1,698 more rows\n\nThis all works if the data you get are already tidy, but in many cases this is not true. Then, it is useful to explore the structure of the data you have been given.\n\ndf_types &lt;- function(df) {\n  tibble(\n    col_name = names(df), \n    col_type = map_chr(df, vctrs::vec_ptype_full),\n    n_miss = map_int(df, \\(x) sum(is.na(x)))\n  )\n}\n\ndf_types(gapminder::gapminder)\n\n# A tibble: 6 × 3\n  col_name  col_type      n_miss\n  &lt;chr&gt;     &lt;chr&gt;          &lt;int&gt;\n1 country   factor&lt;39935&gt;      0\n2 continent factor&lt;be586&gt;      0\n3 year      integer            0\n4 lifeExp   double             0\n5 pop       integer            0\n6 gdpPercap double             0\n\n#&gt; # A tibble: 6 × 3\n#&gt;   col_name  col_type  n_miss\n#&gt;   &lt;chr&gt;     &lt;chr&gt;      &lt;int&gt;\n#&gt; 1 year      double         0\n#&gt; 2 country   character      0\n#&gt; 3 continent character      0\n#&gt; 4 lifeExp   double         0\n#&gt; 5 pop       double         0\n#&gt; 6 gdpPercap double         0\n\nTwo functions that may prove useful in modifying heterogenous files are map_if() and map_at().\nHandle failures in calls to map() with possibly().\n\n# \n# files &lt;- paths |&gt; \n#   map(possibly(\\(path) readxl::read_excel(path), NULL))\n# \n# data &lt;- files |&gt; list_rbind()\n# \n# # Now figure out which files failed.\n# \n# failed &lt;- map_vec(files, is.null)\n# paths[failed]\n# #&gt; character(0)\n\n\n\n\n\nSaving multiple data frames into one database.\nSaving multiple data frames into multiple .csv files.\nSaving multiple plots to multiple .png files.\n\nWriting to a database\nThe following would work if we just had .csv files.\n\n# con &lt;- DBI::dbConnect(duckdb::duckdb())\n# duckdb::duckdb_read_csv(con, \"gapminder\", paths)\n\nWe actually have .xlsx files, so we’ll have to do this by hand. First, make a template.\n\n# template &lt;- readxl::read_excel(paths[[1]])\n# template$year &lt;- 1952\n# template\n# #&gt; # A tibble: 142 × 6\n# #&gt;   country     continent lifeExp      pop gdpPercap  year\n# #&gt;   &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n# #&gt; 1 Afghanistan Asia         28.8  8425333      779.  1952\n# #&gt; 2 Albania     Europe       55.2  1282697     1601.  1952\n# #&gt; 3 Algeria     Africa       43.1  9279525     2449.  1952\n# #&gt; 4 Angola      Africa       30.0  4232095     3521.  1952\n# #&gt; 5 Argentina   Americas     62.5 17876956     5911.  1952\n# #&gt; 6 Australia   Oceania      69.1  8691212    10040.  1952\n# #&gt; # ℹ 136 more rows\n\nNow connect to the database, and create a database table with the template.\n\n# \n# con &lt;- DBI::dbConnect(duckdb::duckdb())\n# DBI::dbCreateTable(con, \"gapminder\", template)\n\nNow the table is created with the correct variable types, but no data are in the table.\n\n# con |&gt; tbl(\"gapminder\")\n# #&gt; # Source:   table&lt;gapminder&gt; [?? x 6]\n# #&gt; # Database: DuckDB 1.4.4 [unknown@Linux 6.11.0-1018-azure:R 4.5.2/:memory:]\n# #&gt; # ℹ 6 variables: country &lt;chr&gt;, continent &lt;chr&gt;, lifeExp &lt;dbl&gt;, pop &lt;dbl&gt;,\n# #&gt; #   gdpPercap &lt;dbl&gt;, year &lt;dbl&gt;\n\nNow write a function that will read in files and append them to the database table:\n\n# append_file &lt;- function(path) {\n#   df &lt;- readxl::read_excel(path)\n#   df$year &lt;- parse_number(basename(path))\n#   \n#   DBI::dbAppendTable(con, \"gapminder\", df)\n# }\n\nWe don’t actually need the output of this function. So it’s a good time to use walk().\n\n# paths |&gt; walk(append_file)\n\n# con |&gt; \n#   tbl(\"gapminder\") |&gt; \n#   count(year)\n# #&gt; # Source:   SQL [?? x 2]\n# #&gt; # Database: DuckDB 1.4.4 [unknown@Linux 6.11.0-1018-azure:R 4.5.2/:memory:]\n# #&gt;    year     n\n# #&gt;   &lt;dbl&gt; &lt;dbl&gt;\n# #&gt; 1  1952   142\n# #&gt; 2  1957   142\n# #&gt; 3  1962   142\n# #&gt; 4  1972   142\n# #&gt; 5  1982   142\n# #&gt; 6  1992   142\n# #&gt; # ℹ more rows\n\nWriting csv files. Here, they use group_nest to create a tibble per group.\n\nby_clarity &lt;- diamonds |&gt; \n  group_nest(clarity)\n\nby_clarity\n\n# A tibble: 8 × 2\n  clarity               data\n  &lt;ord&gt;   &lt;list&lt;tibble[,9]&gt;&gt;\n1 I1               [741 × 9]\n2 SI2            [9,194 × 9]\n3 SI1           [13,065 × 9]\n4 VS2           [12,258 × 9]\n5 VS1            [8,171 × 9]\n6 VVS2           [5,066 × 9]\n7 VVS1           [3,655 × 9]\n8 IF             [1,790 × 9]\n\n#&gt; # A tibble: 8 × 2\n#&gt;   clarity               data\n#&gt;   &lt;ord&gt;   &lt;list&lt;tibble[,9]&gt;&gt;\n#&gt; 1 I1               [741 × 9]\n#&gt; 2 SI2            [9,194 × 9]\n#&gt; 3 SI1           [13,065 × 9]\n#&gt; 4 VS2           [12,258 × 9]\n#&gt; 5 VS1            [8,171 × 9]\n#&gt; 6 VVS2           [5,066 × 9]\n#&gt; # ℹ 2 more rows\n\n\nby_clarity$data[[1]]\n\n# A tibble: 741 × 9\n   carat cut       color depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.32 Premium   E      60.9    58   345  4.38  4.42  2.68\n 2  1.17 Very Good J      60.2    61  2774  6.83  6.9   4.13\n 3  1.01 Premium   F      61.8    60  2781  6.39  6.36  3.94\n 4  1.01 Fair      E      64.5    58  2788  6.29  6.21  4.03\n 5  0.96 Ideal     F      60.7    55  2801  6.37  6.41  3.88\n 6  1.04 Premium   G      62.2    58  2801  6.46  6.41  4   \n 7  1    Fair      G      66.4    59  2808  6.16  6.09  4.07\n 8  1.2  Fair      F      64.6    56  2809  6.73  6.66  4.33\n 9  0.43 Very Good E      58.4    62   555  4.94  5     2.9 \n10  1.02 Premium   G      60.3    58  2815  6.55  6.5   3.94\n# ℹ 731 more rows\n\n#&gt; # A tibble: 741 × 9\n#&gt;   carat cut       color depth table price     x     y     z\n#&gt;   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1  0.32 Premium   E      60.9    58   345  4.38  4.42  2.68\n#&gt; 2  1.17 Very Good J      60.2    61  2774  6.83  6.9   4.13\n#&gt; 3  1.01 Premium   F      61.8    60  2781  6.39  6.36  3.94\n#&gt; 4  1.01 Fair      E      64.5    58  2788  6.29  6.21  4.03\n#&gt; 5  0.96 Ideal     F      60.7    55  2801  6.37  6.41  3.88\n#&gt; 6  1.04 Premium   G      62.2    58  2801  6.46  6.41  4   \n#&gt; # ℹ 735 more rows\n\nNow add a column with the name of the output file.\n\nby_clarity &lt;- by_clarity |&gt; \n  mutate(path = str_glue(\"diamonds-{clarity}.csv\"))\n\nby_clarity\n\n# A tibble: 8 × 3\n  clarity               data path             \n  &lt;ord&gt;   &lt;list&lt;tibble[,9]&gt;&gt; &lt;glue&gt;           \n1 I1               [741 × 9] diamonds-I1.csv  \n2 SI2            [9,194 × 9] diamonds-SI2.csv \n3 SI1           [13,065 × 9] diamonds-SI1.csv \n4 VS2           [12,258 × 9] diamonds-VS2.csv \n5 VS1            [8,171 × 9] diamonds-VS1.csv \n6 VVS2           [5,066 × 9] diamonds-VVS2.csv\n7 VVS1           [3,655 × 9] diamonds-VVS1.csv\n8 IF             [1,790 × 9] diamonds-IF.csv  \n\n#&gt; # A tibble: 8 × 3\n#&gt;   clarity               data path             \n#&gt;   &lt;ord&gt;   &lt;list&lt;tibble[,9]&gt;&gt; &lt;glue&gt;           \n#&gt; 1 I1               [741 × 9] diamonds-I1.csv  \n#&gt; 2 SI2            [9,194 × 9] diamonds-SI2.csv \n#&gt; 3 SI1           [13,065 × 9] diamonds-SI1.csv \n#&gt; 4 VS2           [12,258 × 9] diamonds-VS2.csv \n#&gt; 5 VS1            [8,171 × 9] diamonds-VS1.csv \n#&gt; 6 VVS2           [5,066 × 9] diamonds-VVS2.csv\n#&gt; # ℹ 2 more rows\n\nTo write each of these files to disk, we need to vary two arguments, the object and the path. That would be a good time to use map2(), but again, we care more about the side effect than the returned value, so use walk2() instead.\n\n# walk2(by_clarity$data, by_clarity$path, write_csv)\n\nSaving plots. Use the same basic approach.\n\ncarat_histogram &lt;- function(df) {\n  ggplot(df, aes(x = carat)) + geom_histogram(binwidth = 0.1)  \n}\n\ncarat_histogram(by_clarity$data[[1]])\n\n\n\n\n\n\n\n\n\nby_clarity &lt;- by_clarity |&gt; \n  mutate(\n    plot = map(data, carat_histogram),\n    path = str_glue(\"clarity-{clarity}.png\")\n  )\n\nNow use walk2() again:\n\n# walk2(\n#   by_clarity$path,\n#   by_clarity$plot,\n#   \\(path, plot) ggsave(path, plot, width = 6, height = 6)\n# )\n\n\nby_clarity\n\n# A tibble: 8 × 4\n  clarity               data path             plot      \n  &lt;ord&gt;   &lt;list&lt;tibble[,9]&gt;&gt; &lt;glue&gt;           &lt;list&gt;    \n1 I1               [741 × 9] clarity-I1.png   &lt;ggplt2::&gt;\n2 SI2            [9,194 × 9] clarity-SI2.png  &lt;ggplt2::&gt;\n3 SI1           [13,065 × 9] clarity-SI1.png  &lt;ggplt2::&gt;\n4 VS2           [12,258 × 9] clarity-VS2.png  &lt;ggplt2::&gt;\n5 VS1            [8,171 × 9] clarity-VS1.png  &lt;ggplt2::&gt;\n6 VVS2           [5,066 × 9] clarity-VVS2.png &lt;ggplt2::&gt;\n7 VVS1           [3,655 × 9] clarity-VVS1.png &lt;ggplt2::&gt;\n8 IF             [1,790 × 9] clarity-IF.png   &lt;ggplt2::&gt;"
  },
  {
    "objectID": "chapter_26_notes.html#modifying-multiple-columns",
    "href": "chapter_26_notes.html#modifying-multiple-columns",
    "title": "R for Data Science, 2nd Edition - Chapter 26 Notes",
    "section": "",
    "text": "set.seed(1014)\ndf &lt;- tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\n\nHow to get medians of each column?\n\ndf |&gt; summarize(\n  n = n(),\n  a = median(a),\n  b = median(b),\n  c = median(c),\n  d = median(d),\n)\n\n# A tibble: 1 × 5\n      n      a      b       c     d\n  &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1    10 -0.246 -0.287 -0.0567 0.144\n\n#&gt; # A tibble: 1 × 5\n#&gt;       n      a      b       c     d\n#&gt;   &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1    10 -0.246 -0.287 -0.0567 0.144\n\nUse across().\n\ndf |&gt; summarize(\n  n = n(),\n  across(a:d, median),\n)\n\n# A tibble: 1 × 5\n      n      a      b       c     d\n  &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1    10 -0.246 -0.287 -0.0567 0.144\n\n#&gt; # A tibble: 1 × 5\n#&gt;       n      a      b       c     d\n#&gt;   &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1    10 -0.246 -0.287 -0.0567 0.144\n\nKey arguments: .cols, .fns, and you can use .names if you want to control names of output columns. if_any() and if_all() may be important within filter() calls.\n.cols can be supplied with the same syntax as select(), and everything() and where(). Use where() to select based on type.\n\nset.seed(1014)\ndf &lt;- tibble(\n  grp = sample(2, 10, replace = TRUE),\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\n\ndf |&gt; \n  group_by(grp) |&gt; \n  summarize(across(everything(), median))\n\n# A tibble: 2 × 5\n    grp      a      b       c       d\n  &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     1 -0.244 -0.522 -0.0974 -0.251 \n2     2 -0.247  0.468  0.112   0.0700\n\n#&gt; # A tibble: 2 × 5\n#&gt;     grp      a      b       c       d\n#&gt;   &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1     1 -0.244 -0.522 -0.0974 -0.251 \n#&gt; 2     2 -0.247  0.468  0.112   0.0700\n\nJust like other selectors, you can combine these with Boolean algebra. For example, !where(is.numeric) selects all non-numeric columns, and starts_with(\"a\") & where(is.logical) selects all logical columns whose name starts with “a”.\nCalling a single function.\nCalling multiple functions. (or multiple arguments). Here is an example of handling NA values in a call to median().\n\nset.seed(1014)\nrnorm_na &lt;- function(n, n_na, mean = 0, sd = 1) {\n  sample(c(rnorm(n - n_na, mean = mean, sd = sd), rep(NA, n_na)))\n}\n\ndf_miss &lt;- tibble(\n  a = rnorm_na(5, 1),\n  b = rnorm_na(5, 1),\n  c = rnorm_na(5, 2),\n  d = rnorm(5)\n)\ndf_miss |&gt; \n  summarize(\n    across(a:d, median),\n    n = n()\n  )\n\n# A tibble: 1 × 5\n      a     b     c     d     n\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1    NA    NA    NA 0.413     5\n\n#&gt; # A tibble: 1 × 5\n#&gt;       a     b     c     d     n\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n#&gt; 1    NA    NA    NA 0.413     5\n\nWe need to create a new function that calls median() with the desired arguments:\n\ndf_miss |&gt; \n  summarize(\n    across(a:d, function(x) median(x, na.rm = TRUE)),\n    n = n()\n  )\n\n# A tibble: 1 × 5\n       a      b      c     d     n\n   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 -0.703 -0.265 -0.522 0.413     5\n\n#&gt; # A tibble: 1 × 5\n#&gt;        a      b      c     d     n\n#&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n#&gt; 1 -0.703 -0.265 -0.522 0.413     5\n\nThis is verbose, so we can use anonymous function syntax (\\(x) recommended over ~ .x).\n\ndf_miss |&gt; \n  summarize(\n    across(a:d, \\(x) median(x, na.rm = TRUE)),\n    n = n()\n  )\n\n# A tibble: 1 × 5\n       a      b      c     d     n\n   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 -0.703 -0.265 -0.522 0.413     5\n\n\nIf you want to supply more than one function in the call to across(), use a named list:\n\ndf_miss |&gt; \n  summarize(\n    across(a:d, list(\n      median = \\(x) median(x, na.rm = TRUE),\n      n_miss = \\(x) sum(is.na(x))\n    )),\n    n = n()\n  )\n\n# A tibble: 1 × 9\n  a_median a_n_miss b_median b_n_miss c_median c_n_miss d_median d_n_miss     n\n     &lt;dbl&gt;    &lt;int&gt;    &lt;dbl&gt;    &lt;int&gt;    &lt;dbl&gt;    &lt;int&gt;    &lt;dbl&gt;    &lt;int&gt; &lt;int&gt;\n1   -0.703        1   -0.265        1   -0.522        2    0.413        0     5\n\n#&gt; # A tibble: 1 × 9\n#&gt;   a_median a_n_miss b_median b_n_miss c_median c_n_miss d_median d_n_miss\n#&gt;      &lt;dbl&gt;    &lt;int&gt;    &lt;dbl&gt;    &lt;int&gt;    &lt;dbl&gt;    &lt;int&gt;    &lt;dbl&gt;    &lt;int&gt;\n#&gt; 1   -0.703        1   -0.265        1   -0.522        2    0.413        0\n#&gt; # ℹ 1 more variable: n &lt;int&gt;\n\nColumn names are specified with the .names argument.\n\ndf_miss |&gt; \n  summarize(\n    across(\n      a:d,\n      list(\n        median = \\(x) median(x, na.rm = TRUE),\n        n_miss = \\(x) sum(is.na(x))\n      ),\n      .names = \"{.fn}_{.col}\"\n    ),\n    n = n(),\n  )\n\n# A tibble: 1 × 9\n  median_a n_miss_a median_b n_miss_b median_c n_miss_c median_d n_miss_d     n\n     &lt;dbl&gt;    &lt;int&gt;    &lt;dbl&gt;    &lt;int&gt;    &lt;dbl&gt;    &lt;int&gt;    &lt;dbl&gt;    &lt;int&gt; &lt;int&gt;\n1   -0.703        1   -0.265        1   -0.522        2    0.413        0     5\n\n#&gt; # A tibble: 1 × 9\n#&gt;   median_a n_miss_a median_b n_miss_b median_c n_miss_c median_d n_miss_d\n#&gt;      &lt;dbl&gt;    &lt;int&gt;    &lt;dbl&gt;    &lt;int&gt;    &lt;dbl&gt;    &lt;int&gt;    &lt;dbl&gt;    &lt;int&gt;\n#&gt; 1   -0.703        1   -0.265        1   -0.522        2    0.413        0\n#&gt; # ℹ 1 more variable: n &lt;int&gt;\n\n.names is especially helpful when calling mutate() within across() to prevent overwriting columns, since the output of across() is given the same name as the inputs.\n\ndf_miss |&gt; \n  mutate(\n    across(a:d, \\(x) coalesce(x, 0))\n  )\n\n# A tibble: 5 × 4\n         a      b      c        d\n     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n1 -0.00557 -0.283 -1.86  -0.783  \n2  0.255   -0.247 -0.522 -0.00289\n3 -1.40    -0.554  0.512  0.413  \n4 -2.44    -0.244  0      0.724  \n5  0        0      0      2.35   \n\n#&gt; # A tibble: 5 × 4\n#&gt;          a      b      c        d\n#&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1 -0.00557 -0.283 -1.86  -0.783  \n#&gt; 2  0.255   -0.247 -0.522 -0.00289\n#&gt; 3 -1.40    -0.554  0.512  0.413  \n#&gt; 4 -2.44    -0.244  0      0.724  \n#&gt; 5  0        0      0      2.35\n\nUse the .names argument to instead create new columns.\n\ndf_miss |&gt; \n  mutate(\n    across(a:d, \\(x) coalesce(x, 0), .names = \"{.col}_na_zero\")\n  )\n\n# A tibble: 5 × 8\n         a      b      c        d a_na_zero b_na_zero c_na_zero d_na_zero\n     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 -0.00557 -0.283 -1.86  -0.783    -0.00557    -0.283    -1.86   -0.783  \n2  0.255   -0.247 -0.522 -0.00289   0.255      -0.247    -0.522  -0.00289\n3 -1.40    -0.554  0.512  0.413    -1.40       -0.554     0.512   0.413  \n4 -2.44    -0.244 NA      0.724    -2.44       -0.244     0       0.724  \n5 NA       NA     NA      2.35      0           0         0       2.35   \n\n#&gt; # A tibble: 5 × 8\n#&gt;          a      b      c        d a_na_zero b_na_zero c_na_zero d_na_zero\n#&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 -0.00557 -0.283 -1.86  -0.783    -0.00557    -0.283    -1.86   -0.783  \n#&gt; 2  0.255   -0.247 -0.522 -0.00289   0.255      -0.247    -0.522  -0.00289\n#&gt; 3 -1.40    -0.554  0.512  0.413    -1.40       -0.554     0.512   0.413  \n#&gt; 4 -2.44    -0.244 NA      0.724    -2.44       -0.244     0       0.724  \n#&gt; 5 NA       NA     NA      2.35      0           0         0       2.35\n\nFiltering. Two variants of across() are provided for use inside of filter(): if_any() and if_all()\n\n# same as df_miss |&gt; filter(is.na(a) | is.na(b) | is.na(c) | is.na(d))\ndf_miss |&gt; filter(if_any(a:d, is.na))\n\n# A tibble: 2 × 4\n      a      b     c     d\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 -2.44 -0.244    NA 0.724\n2 NA    NA        NA 2.35 \n\n#&gt; # A tibble: 2 × 4\n#&gt;       a      b     c     d\n#&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 -2.44 -0.244    NA 0.724\n#&gt; 2 NA    NA        NA 2.35\n\n# same as df_miss |&gt; filter(is.na(a) & is.na(b) & is.na(c) & is.na(d))\ndf_miss |&gt; filter(if_all(a:d, is.na))\n\n# A tibble: 0 × 4\n# ℹ 4 variables: a &lt;dbl&gt;, b &lt;dbl&gt;, c &lt;dbl&gt;, d &lt;dbl&gt;\n\n#&gt; # A tibble: 0 × 4\n#&gt; # ℹ 4 variables: a &lt;dbl&gt;, b &lt;dbl&gt;, c &lt;dbl&gt;, d &lt;dbl&gt;\n\nacross() can also be quite useful inside of functions.\nA helper function to expand all date columns into year, month, and day columns:\n\nexpand_dates &lt;- function(df) {\n  df |&gt; \n    mutate(\n      across(where(is.Date), list(year = year, month = month, day = mday))\n    )\n}\n\ndf_date &lt;- tibble(\n  name = c(\"Amy\", \"Bob\"),\n  date = ymd(c(\"2009-08-03\", \"2010-01-16\"))\n)\n\ndf_date |&gt; \n  expand_dates()\n\n# A tibble: 2 × 5\n  name  date       date_year date_month date_day\n  &lt;chr&gt; &lt;date&gt;         &lt;dbl&gt;      &lt;dbl&gt;    &lt;int&gt;\n1 Amy   2009-08-03      2009          8        3\n2 Bob   2010-01-16      2010          1       16\n\n#&gt; # A tibble: 2 × 5\n#&gt;   name  date       date_year date_month date_day\n#&gt;   &lt;chr&gt; &lt;date&gt;         &lt;dbl&gt;      &lt;dbl&gt;    &lt;int&gt;\n#&gt; 1 Amy   2009-08-03      2009          8        3\n#&gt; 2 Bob   2010-01-16      2010          1       16\n\nDon’t forget to use{ } when selecting columns inside of a function, since across() uses tidy-select.\n\nsummarize_means &lt;- function(df, summary_vars = where(is.numeric)) {\n  df |&gt; \n    summarize(\n      across({{ summary_vars }}, \\(x) mean(x, na.rm = TRUE)),\n      n = n(),\n      .groups = \"drop\"\n    )\n}\n\ndiamonds |&gt; \n  group_by(cut) |&gt; \n  summarize_means()\n\n# A tibble: 5 × 9\n  cut       carat depth table price     x     y     z     n\n  &lt;ord&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 Fair      1.05   64.0  59.1 4359.  6.25  6.18  3.98  1610\n2 Good      0.849  62.4  58.7 3929.  5.84  5.85  3.64  4906\n3 Very Good 0.806  61.8  58.0 3982.  5.74  5.77  3.56 12082\n4 Premium   0.892  61.3  58.7 4584.  5.97  5.94  3.65 13791\n5 Ideal     0.703  61.7  56.0 3458.  5.51  5.52  3.40 21551\n\n#&gt; # A tibble: 5 × 9\n#&gt;   cut       carat depth table price     x     y     z     n\n#&gt;   &lt;ord&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n#&gt; 1 Fair      1.05   64.0  59.1 4359.  6.25  6.18  3.98  1610\n#&gt; 2 Good      0.849  62.4  58.7 3929.  5.84  5.85  3.64  4906\n#&gt; 3 Very Good 0.806  61.8  58.0 3982.  5.74  5.77  3.56 12082\n#&gt; 4 Premium   0.892  61.3  58.7 4584.  5.97  5.94  3.65 13791\n#&gt; 5 Ideal     0.703  61.7  56.0 3458.  5.51  5.52  3.40 21551\n\ndiamonds |&gt; \n  group_by(cut) |&gt; \n  summarize_means(c(carat, x:z))\n\n# A tibble: 5 × 6\n  cut       carat     x     y     z     n\n  &lt;ord&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 Fair      1.05   6.25  6.18  3.98  1610\n2 Good      0.849  5.84  5.85  3.64  4906\n3 Very Good 0.806  5.74  5.77  3.56 12082\n4 Premium   0.892  5.97  5.94  3.65 13791\n5 Ideal     0.703  5.51  5.52  3.40 21551\n\n#&gt; # A tibble: 5 × 6\n#&gt;   cut       carat     x     y     z     n\n#&gt;   &lt;ord&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n#&gt; 1 Fair      1.05   6.25  6.18  3.98  1610\n#&gt; 2 Good      0.849  5.84  5.85  3.64  4906\n#&gt; 3 Very Good 0.806  5.74  5.77  3.56 12082\n#&gt; 4 Premium   0.892  5.97  5.94  3.65 13791\n#&gt; 5 Ideal     0.703  5.51  5.52  3.40 21551\n\nThere is a connection between across() and pivot_longer().\n\ndf |&gt; \n  summarize(across(a:d, list(median = median, mean = mean)))\n\n# A tibble: 1 × 8\n  a_median  a_mean b_median  b_mean c_median  c_mean d_median d_mean\n     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n1   -0.246 -0.0426    0.155 -0.0656   0.0480 -0.0297   -0.193 -0.200\n\n#&gt; # A tibble: 1 × 8\n#&gt;   a_median  a_mean b_median  b_mean c_median  c_mean d_median d_mean\n#&gt;      &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1   -0.246 -0.0426    0.155 -0.0656   0.0480 -0.0297   -0.193 -0.200\n\nCompare this to:\n\nlong &lt;- df |&gt; \n  pivot_longer(a:d) |&gt; \n  group_by(name) |&gt; \n  summarize(\n    median = median(value),\n    mean = mean(value)\n  )\nlong\n\n# A tibble: 4 × 3\n  name   median    mean\n  &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 a     -0.246  -0.0426\n2 b      0.155  -0.0656\n3 c      0.0480 -0.0297\n4 d     -0.193  -0.200 \n\n#&gt; # A tibble: 4 × 3\n#&gt;   name   median    mean\n#&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1 a     -0.246  -0.0426\n#&gt; 2 b      0.155  -0.0656\n#&gt; 3 c      0.0480 -0.0297\n#&gt; 4 d     -0.193  -0.200\n\nAnd to get it back to the structure across() gives you, pivot_wider()\n\nlong |&gt; \n  pivot_wider(\n    names_from = name,\n    values_from = c(median, mean),\n    names_vary = \"slowest\",\n    names_glue = \"{name}_{.value}\"\n  )\n\n# A tibble: 1 × 8\n  a_median  a_mean b_median  b_mean c_median  c_mean d_median d_mean\n     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n1   -0.246 -0.0426    0.155 -0.0656   0.0480 -0.0297   -0.193 -0.200\n\n#&gt; # A tibble: 1 × 8\n#&gt;   a_median  a_mean b_median  b_mean c_median  c_mean d_median d_mean\n#&gt;      &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1   -0.246 -0.0426    0.155 -0.0656   0.0480 -0.0297   -0.193 -0.200\n\nThis is useful because occasionally you won’t be able to solve a problem with across(), such as groups of columns that you want to compute with simultaneously. Here is an example of a weighted mean:\n\nset.seed(1014)\ndf_paired &lt;- tibble(\n  a_val = rnorm(10),\n  a_wts = runif(10),\n  b_val = rnorm(10),\n  b_wts = runif(10),\n  c_val = rnorm(10),\n  c_wts = runif(10),\n  d_val = rnorm(10),\n  d_wts = runif(10)\n)\n\nYou can do this with pivot_longer().\n\ndf_long &lt;- df_paired |&gt; \n  pivot_longer(\n    everything(), \n    names_to = c(\"group\", \".value\"), \n    names_sep = \"_\"\n  )\ndf_long\n\n# A tibble: 40 × 3\n   group     val   wts\n   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1 a     -1.40   0.290\n 2 b     -1.86   0.461\n 3 c      0.935  0.528\n 4 d      2.76   0.709\n 5 a      0.255  0.678\n 6 b     -0.522  0.315\n 7 c      0.176  0.601\n 8 d      0.0465 0.874\n 9 a     -2.44   0.735\n10 b     -0.0526 0.175\n# ℹ 30 more rows\n\n#&gt; # A tibble: 40 × 3\n#&gt;   group    val   wts\n#&gt;   &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 a     -1.40  0.290\n#&gt; 2 b     -1.86  0.461\n#&gt; 3 c      0.935 0.528\n#&gt; 4 d      2.76  0.709\n#&gt; 5 a      0.255 0.678\n#&gt; 6 b     -0.522 0.315\n#&gt; # ℹ 34 more rows\n\ndf_long |&gt; \n  group_by(group) |&gt; \n  summarize(mean = weighted.mean(val, wts))\n\n# A tibble: 4 × 2\n  group    mean\n  &lt;chr&gt;   &lt;dbl&gt;\n1 a     -0.207 \n2 b     -0.237 \n3 c      0.0208\n4 d      0.0655\n\n#&gt; # A tibble: 4 × 2\n#&gt;   group    mean\n#&gt;   &lt;chr&gt;   &lt;dbl&gt;\n#&gt; 1 a     -0.207 \n#&gt; 2 b     -0.237 \n#&gt; 3 c      0.0208\n#&gt; 4 d      0.0655\n\nfind missing values after grouping on columns you choose.\n\nshow_missing &lt;- function(df, group_vars, summary_vars = everything()) {\n  df |&gt; \n    group_by(pick({{ group_vars }})) |&gt; \n    summarize(\n      across({{ summary_vars }}, \\(x) sum(is.na(x))),\n      .groups = \"drop\"\n    ) |&gt;\n    select(where(\\(x) any(x &gt; 0)))\n}\nnycflights13::flights |&gt; show_missing(c(year, month, day))\n\n# A tibble: 365 × 9\n    year month   day dep_time dep_delay arr_time arr_delay tailnum air_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;     &lt;int&gt;    &lt;int&gt;     &lt;int&gt;   &lt;int&gt;    &lt;int&gt;\n 1  2013     1     1        4         4        5        11       0       11\n 2  2013     1     2        8         8       10        15       2       15\n 3  2013     1     3       10        10       10        14       2       14\n 4  2013     1     4        6         6        6         7       2        7\n 5  2013     1     5        3         3        3         3       1        3\n 6  2013     1     6        1         1        1         3       0        3\n 7  2013     1     7        3         3        3         3       1        3\n 8  2013     1     8        4         4        4         7       1        7\n 9  2013     1     9        5         5        7         9       2        9\n10  2013     1    10        3         3        3         3       2        3\n# ℹ 355 more rows"
  },
  {
    "objectID": "chapter_26_notes.html#reading-multiple-files",
    "href": "chapter_26_notes.html#reading-multiple-files",
    "title": "R for Data Science, 2nd Edition - Chapter 26 Notes",
    "section": "",
    "text": "The basic pattern is to use list.files(), then purrr:map(), then bind the rows together across elements of the list with purrr:list_bind().\n\n# \n# paths &lt;- list.files(\"data/gapminder\", pattern = \"[.]xlsx$\", full.names = TRUE)\n# paths\n#&gt;  [1] \"data/gapminder/1952.xlsx\" \"data/gapminder/1957.xlsx\"\n#&gt;  [3] \"data/gapminder/1962.xlsx\" \"data/gapminder/1967.xlsx\"\n#&gt;  [5] \"data/gapminder/1972.xlsx\" \"data/gapminder/1977.xlsx\"\n#&gt;  [7] \"data/gapminder/1982.xlsx\" \"data/gapminder/1987.xlsx\"\n#&gt;  [9] \"data/gapminder/1992.xlsx\" \"data/gapminder/1997.xlsx\"\n#&gt; [11] \"data/gapminder/2002.xlsx\" \"data/gapminder/2007.xlsx\"\n\nNow use map().\n\n# files &lt;- map(paths, readxl::read_excel)\n# length(files)\n# #&gt; [1] 12\n# \n# files[[1]]\n#&gt; # A tibble: 142 × 5\n#&gt;   country     continent lifeExp      pop gdpPercap\n#&gt;   &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 Afghanistan Asia         28.8  8425333      779.\n#&gt; 2 Albania     Europe       55.2  1282697     1601.\n#&gt; 3 Algeria     Africa       43.1  9279525     2449.\n#&gt; 4 Angola      Africa       30.0  4232095     3521.\n#&gt; 5 Argentina   Americas     62.5 17876956     5911.\n#&gt; 6 Australia   Oceania      69.1  8691212    10040.\n#&gt; # ℹ 136 more rows\n\nNow combine elements of the list with purrr::list_rbind().\n\n# list_rbind(files)\n# #&gt; # A tibble: 1,704 × 5\n# #&gt;   country     continent lifeExp      pop gdpPercap\n# #&gt;   &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n# #&gt; 1 Afghanistan Asia         28.8  8425333      779.\n# #&gt; 2 Albania     Europe       55.2  1282697     1601.\n# #&gt; 3 Algeria     Africa       43.1  9279525     2449.\n# #&gt; 4 Angola      Africa       30.0  4232095     3521.\n# #&gt; 5 Argentina   Americas     62.5 17876956     5911.\n# #&gt; 6 Australia   Oceania      69.1  8691212    10040.\n# #&gt; # ℹ 1,698 more rows\n\n# Or we could do this:\n\n# paths |&gt; \n#   map(readxl::read_excel) |&gt; \n#   list_rbind()\n\nAdding additional arguments to the function being applied to each element of the list can be accomplished with an anonymous function.\n\n# paths |&gt; \n#   map(\\(path) readxl::read_excel(path, n_max = 1)) |&gt; \n#   list_rbind()\n# #&gt; # A tibble: 12 × 5\n# #&gt;   country     continent lifeExp      pop gdpPercap\n# #&gt;   &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n# #&gt; 1 Afghanistan Asia         28.8  8425333      779.\n# #&gt; 2 Afghanistan Asia         30.3  9240934      821.\n# #&gt; 3 Afghanistan Asia         32.0 10267083      853.\n# #&gt; 4 Afghanistan Asia         34.0 11537966      836.\n# #&gt; 5 Afghanistan Asia         36.1 13079460      740.\n# #&gt; 6 Afghanistan Asia         38.4 14880372      786.\n# #&gt; # ℹ 6 more rows\n\nWhat if variables are in the file names?\n\n# paths |&gt; set_names(basename) \n# #&gt;                  1952.xlsx                  1957.xlsx \n# #&gt; \"data/gapminder/1952.xlsx\" \"data/gapminder/1957.xlsx\" \n# #&gt;                  1962.xlsx                  1967.xlsx \n# #&gt; \"data/gapminder/1962.xlsx\" \"data/gapminder/1967.xlsx\" \n# #&gt;                  1972.xlsx                  1977.xlsx \n# #&gt; \"data/gapminder/1972.xlsx\" \"data/gapminder/1977.xlsx\" \n# #&gt;                  1982.xlsx                  1987.xlsx \n# #&gt; \"data/gapminder/1982.xlsx\" \"data/gapminder/1987.xlsx\" \n# #&gt;                  1992.xlsx                  1997.xlsx \n# #&gt; \"data/gapminder/1992.xlsx\" \"data/gapminder/1997.xlsx\" \n# #&gt;                  2002.xlsx                  2007.xlsx \n# #&gt; \"data/gapminder/2002.xlsx\" \"data/gapminder/2007.xlsx\"\n# \n# files &lt;- paths |&gt; \n#   set_names(basename) |&gt; \n#   map(readxl::read_excel)\n\n\n# paths |&gt; \n#   set_names(basename) |&gt; \n#   map(readxl::read_excel) |&gt; \n#   list_rbind(names_to = \"year\") |&gt; \n#   mutate(year = parse_number(year))\n# #&gt; # A tibble: 1,704 × 6\n# #&gt;    year country     continent lifeExp      pop gdpPercap\n# #&gt;   &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n# #&gt; 1  1952 Afghanistan Asia         28.8  8425333      779.\n# #&gt; 2  1952 Albania     Europe       55.2  1282697     1601.\n# #&gt; 3  1952 Algeria     Africa       43.1  9279525     2449.\n# #&gt; 4  1952 Angola      Africa       30.0  4232095     3521.\n# #&gt; 5  1952 Argentina   Americas     62.5 17876956     5911.\n# #&gt; 6  1952 Australia   Oceania      69.1  8691212    10040.\n# #&gt; # ℹ 1,698 more rows\n\nIn cases where there is more than one variable in the file name:\n\n# paths |&gt; \n#   set_names() |&gt; \n#   map(readxl::read_excel) |&gt; \n#   list_rbind(names_to = \"year\") |&gt; \n#   separate_wider_delim(year, delim = \"/\", names = c(NA, \"dir\", \"file\")) |&gt; \n#   separate_wider_delim(file, delim = \".\", names = c(\"file\", \"ext\"))\n# #&gt; # A tibble: 1,704 × 8\n# #&gt;   dir       file  ext   country     continent lifeExp      pop gdpPercap\n# #&gt;   &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n# #&gt; 1 gapminder 1952  xlsx  Afghanistan Asia         28.8  8425333      779.\n# #&gt; 2 gapminder 1952  xlsx  Albania     Europe       55.2  1282697     1601.\n# #&gt; 3 gapminder 1952  xlsx  Algeria     Africa       43.1  9279525     2449.\n# #&gt; 4 gapminder 1952  xlsx  Angola      Africa       30.0  4232095     3521.\n# #&gt; 5 gapminder 1952  xlsx  Argentina   Americas     62.5 17876956     5911.\n# #&gt; 6 gapminder 1952  xlsx  Australia   Oceania      69.1  8691212    10040.\n# #&gt; # ℹ 1,698 more rows\n\nThis all works if the data you get are already tidy, but in many cases this is not true. Then, it is useful to explore the structure of the data you have been given.\n\ndf_types &lt;- function(df) {\n  tibble(\n    col_name = names(df), \n    col_type = map_chr(df, vctrs::vec_ptype_full),\n    n_miss = map_int(df, \\(x) sum(is.na(x)))\n  )\n}\n\ndf_types(gapminder::gapminder)\n\n# A tibble: 6 × 3\n  col_name  col_type      n_miss\n  &lt;chr&gt;     &lt;chr&gt;          &lt;int&gt;\n1 country   factor&lt;39935&gt;      0\n2 continent factor&lt;be586&gt;      0\n3 year      integer            0\n4 lifeExp   double             0\n5 pop       integer            0\n6 gdpPercap double             0\n\n#&gt; # A tibble: 6 × 3\n#&gt;   col_name  col_type  n_miss\n#&gt;   &lt;chr&gt;     &lt;chr&gt;      &lt;int&gt;\n#&gt; 1 year      double         0\n#&gt; 2 country   character      0\n#&gt; 3 continent character      0\n#&gt; 4 lifeExp   double         0\n#&gt; 5 pop       double         0\n#&gt; 6 gdpPercap double         0\n\nTwo functions that may prove useful in modifying heterogenous files are map_if() and map_at().\nHandle failures in calls to map() with possibly().\n\n# \n# files &lt;- paths |&gt; \n#   map(possibly(\\(path) readxl::read_excel(path), NULL))\n# \n# data &lt;- files |&gt; list_rbind()\n# \n# # Now figure out which files failed.\n# \n# failed &lt;- map_vec(files, is.null)\n# paths[failed]\n# #&gt; character(0)"
  },
  {
    "objectID": "chapter_26_notes.html#saving-multiple-outputs",
    "href": "chapter_26_notes.html#saving-multiple-outputs",
    "title": "R for Data Science, 2nd Edition - Chapter 26 Notes",
    "section": "",
    "text": "Saving multiple data frames into one database.\nSaving multiple data frames into multiple .csv files.\nSaving multiple plots to multiple .png files.\n\nWriting to a database\nThe following would work if we just had .csv files.\n\n# con &lt;- DBI::dbConnect(duckdb::duckdb())\n# duckdb::duckdb_read_csv(con, \"gapminder\", paths)\n\nWe actually have .xlsx files, so we’ll have to do this by hand. First, make a template.\n\n# template &lt;- readxl::read_excel(paths[[1]])\n# template$year &lt;- 1952\n# template\n# #&gt; # A tibble: 142 × 6\n# #&gt;   country     continent lifeExp      pop gdpPercap  year\n# #&gt;   &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n# #&gt; 1 Afghanistan Asia         28.8  8425333      779.  1952\n# #&gt; 2 Albania     Europe       55.2  1282697     1601.  1952\n# #&gt; 3 Algeria     Africa       43.1  9279525     2449.  1952\n# #&gt; 4 Angola      Africa       30.0  4232095     3521.  1952\n# #&gt; 5 Argentina   Americas     62.5 17876956     5911.  1952\n# #&gt; 6 Australia   Oceania      69.1  8691212    10040.  1952\n# #&gt; # ℹ 136 more rows\n\nNow connect to the database, and create a database table with the template.\n\n# \n# con &lt;- DBI::dbConnect(duckdb::duckdb())\n# DBI::dbCreateTable(con, \"gapminder\", template)\n\nNow the table is created with the correct variable types, but no data are in the table.\n\n# con |&gt; tbl(\"gapminder\")\n# #&gt; # Source:   table&lt;gapminder&gt; [?? x 6]\n# #&gt; # Database: DuckDB 1.4.4 [unknown@Linux 6.11.0-1018-azure:R 4.5.2/:memory:]\n# #&gt; # ℹ 6 variables: country &lt;chr&gt;, continent &lt;chr&gt;, lifeExp &lt;dbl&gt;, pop &lt;dbl&gt;,\n# #&gt; #   gdpPercap &lt;dbl&gt;, year &lt;dbl&gt;\n\nNow write a function that will read in files and append them to the database table:\n\n# append_file &lt;- function(path) {\n#   df &lt;- readxl::read_excel(path)\n#   df$year &lt;- parse_number(basename(path))\n#   \n#   DBI::dbAppendTable(con, \"gapminder\", df)\n# }\n\nWe don’t actually need the output of this function. So it’s a good time to use walk().\n\n# paths |&gt; walk(append_file)\n\n# con |&gt; \n#   tbl(\"gapminder\") |&gt; \n#   count(year)\n# #&gt; # Source:   SQL [?? x 2]\n# #&gt; # Database: DuckDB 1.4.4 [unknown@Linux 6.11.0-1018-azure:R 4.5.2/:memory:]\n# #&gt;    year     n\n# #&gt;   &lt;dbl&gt; &lt;dbl&gt;\n# #&gt; 1  1952   142\n# #&gt; 2  1957   142\n# #&gt; 3  1962   142\n# #&gt; 4  1972   142\n# #&gt; 5  1982   142\n# #&gt; 6  1992   142\n# #&gt; # ℹ more rows\n\nWriting csv files. Here, they use group_nest to create a tibble per group.\n\nby_clarity &lt;- diamonds |&gt; \n  group_nest(clarity)\n\nby_clarity\n\n# A tibble: 8 × 2\n  clarity               data\n  &lt;ord&gt;   &lt;list&lt;tibble[,9]&gt;&gt;\n1 I1               [741 × 9]\n2 SI2            [9,194 × 9]\n3 SI1           [13,065 × 9]\n4 VS2           [12,258 × 9]\n5 VS1            [8,171 × 9]\n6 VVS2           [5,066 × 9]\n7 VVS1           [3,655 × 9]\n8 IF             [1,790 × 9]\n\n#&gt; # A tibble: 8 × 2\n#&gt;   clarity               data\n#&gt;   &lt;ord&gt;   &lt;list&lt;tibble[,9]&gt;&gt;\n#&gt; 1 I1               [741 × 9]\n#&gt; 2 SI2            [9,194 × 9]\n#&gt; 3 SI1           [13,065 × 9]\n#&gt; 4 VS2           [12,258 × 9]\n#&gt; 5 VS1            [8,171 × 9]\n#&gt; 6 VVS2           [5,066 × 9]\n#&gt; # ℹ 2 more rows\n\n\nby_clarity$data[[1]]\n\n# A tibble: 741 × 9\n   carat cut       color depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.32 Premium   E      60.9    58   345  4.38  4.42  2.68\n 2  1.17 Very Good J      60.2    61  2774  6.83  6.9   4.13\n 3  1.01 Premium   F      61.8    60  2781  6.39  6.36  3.94\n 4  1.01 Fair      E      64.5    58  2788  6.29  6.21  4.03\n 5  0.96 Ideal     F      60.7    55  2801  6.37  6.41  3.88\n 6  1.04 Premium   G      62.2    58  2801  6.46  6.41  4   \n 7  1    Fair      G      66.4    59  2808  6.16  6.09  4.07\n 8  1.2  Fair      F      64.6    56  2809  6.73  6.66  4.33\n 9  0.43 Very Good E      58.4    62   555  4.94  5     2.9 \n10  1.02 Premium   G      60.3    58  2815  6.55  6.5   3.94\n# ℹ 731 more rows\n\n#&gt; # A tibble: 741 × 9\n#&gt;   carat cut       color depth table price     x     y     z\n#&gt;   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1  0.32 Premium   E      60.9    58   345  4.38  4.42  2.68\n#&gt; 2  1.17 Very Good J      60.2    61  2774  6.83  6.9   4.13\n#&gt; 3  1.01 Premium   F      61.8    60  2781  6.39  6.36  3.94\n#&gt; 4  1.01 Fair      E      64.5    58  2788  6.29  6.21  4.03\n#&gt; 5  0.96 Ideal     F      60.7    55  2801  6.37  6.41  3.88\n#&gt; 6  1.04 Premium   G      62.2    58  2801  6.46  6.41  4   \n#&gt; # ℹ 735 more rows\n\nNow add a column with the name of the output file.\n\nby_clarity &lt;- by_clarity |&gt; \n  mutate(path = str_glue(\"diamonds-{clarity}.csv\"))\n\nby_clarity\n\n# A tibble: 8 × 3\n  clarity               data path             \n  &lt;ord&gt;   &lt;list&lt;tibble[,9]&gt;&gt; &lt;glue&gt;           \n1 I1               [741 × 9] diamonds-I1.csv  \n2 SI2            [9,194 × 9] diamonds-SI2.csv \n3 SI1           [13,065 × 9] diamonds-SI1.csv \n4 VS2           [12,258 × 9] diamonds-VS2.csv \n5 VS1            [8,171 × 9] diamonds-VS1.csv \n6 VVS2           [5,066 × 9] diamonds-VVS2.csv\n7 VVS1           [3,655 × 9] diamonds-VVS1.csv\n8 IF             [1,790 × 9] diamonds-IF.csv  \n\n#&gt; # A tibble: 8 × 3\n#&gt;   clarity               data path             \n#&gt;   &lt;ord&gt;   &lt;list&lt;tibble[,9]&gt;&gt; &lt;glue&gt;           \n#&gt; 1 I1               [741 × 9] diamonds-I1.csv  \n#&gt; 2 SI2            [9,194 × 9] diamonds-SI2.csv \n#&gt; 3 SI1           [13,065 × 9] diamonds-SI1.csv \n#&gt; 4 VS2           [12,258 × 9] diamonds-VS2.csv \n#&gt; 5 VS1            [8,171 × 9] diamonds-VS1.csv \n#&gt; 6 VVS2           [5,066 × 9] diamonds-VVS2.csv\n#&gt; # ℹ 2 more rows\n\nTo write each of these files to disk, we need to vary two arguments, the object and the path. That would be a good time to use map2(), but again, we care more about the side effect than the returned value, so use walk2() instead.\n\n# walk2(by_clarity$data, by_clarity$path, write_csv)\n\nSaving plots. Use the same basic approach.\n\ncarat_histogram &lt;- function(df) {\n  ggplot(df, aes(x = carat)) + geom_histogram(binwidth = 0.1)  \n}\n\ncarat_histogram(by_clarity$data[[1]])\n\n\n\n\n\n\n\n\n\nby_clarity &lt;- by_clarity |&gt; \n  mutate(\n    plot = map(data, carat_histogram),\n    path = str_glue(\"clarity-{clarity}.png\")\n  )\n\nNow use walk2() again:\n\n# walk2(\n#   by_clarity$path,\n#   by_clarity$plot,\n#   \\(path, plot) ggsave(path, plot, width = 6, height = 6)\n# )\n\n\nby_clarity\n\n# A tibble: 8 × 4\n  clarity               data path             plot      \n  &lt;ord&gt;   &lt;list&lt;tibble[,9]&gt;&gt; &lt;glue&gt;           &lt;list&gt;    \n1 I1               [741 × 9] clarity-I1.png   &lt;ggplt2::&gt;\n2 SI2            [9,194 × 9] clarity-SI2.png  &lt;ggplt2::&gt;\n3 SI1           [13,065 × 9] clarity-SI1.png  &lt;ggplt2::&gt;\n4 VS2           [12,258 × 9] clarity-VS2.png  &lt;ggplt2::&gt;\n5 VS1            [8,171 × 9] clarity-VS1.png  &lt;ggplt2::&gt;\n6 VVS2           [5,066 × 9] clarity-VVS2.png &lt;ggplt2::&gt;\n7 VVS1           [3,655 × 9] clarity-VVS1.png &lt;ggplt2::&gt;\n8 IF             [1,790 × 9] clarity-IF.png   &lt;ggplt2::&gt;"
  },
  {
    "objectID": "chapter_18_notes.html",
    "href": "chapter_18_notes.html",
    "title": "R for Data Science, 2nd Edition - Chapter 18 Notes",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.6\n✔ forcats   1.0.1     ✔ stringr   1.6.0\n✔ ggplot2   4.0.1     ✔ tibble    3.3.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.2\n✔ purrr     1.2.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(nycflights13)\n\n\n\nCreating or eliminating missing values.\n\ntreatment &lt;- tribble(\n  ~person,           ~treatment, ~response,\n  \"Derrick Whitmore\", 1,         7,\n  NA,                 2,         10,\n  NA,                 3,         NA,\n  \"Katherine Burke\",  1,         4\n)\n\nYou can fill in missing values with tidyr::fill().\n\ntreatment |&gt;\n  fill(everything())\n\n# A tibble: 4 × 3\n  person           treatment response\n  &lt;chr&gt;                &lt;dbl&gt;    &lt;dbl&gt;\n1 Derrick Whitmore         1        7\n2 Derrick Whitmore         2       10\n3 Derrick Whitmore         3       10\n4 Katherine Burke          1        4\n\n#&gt; # A tibble: 4 × 3\n#&gt;   person           treatment response\n#&gt;   &lt;chr&gt;                &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1 Derrick Whitmore         1        7\n#&gt; 2 Derrick Whitmore         2       10\n#&gt; 3 Derrick Whitmore         3       10\n#&gt; 4 Katherine Burke          1        4\n\nThe default behavior of fill() is to use last observation carried forward, but this can be altered using the .direction argument.\nIf missingness can be specified by a number, you can use dplyr::coalesce() to replace NA with the releveant number:\n\nx &lt;- c(1, 4, 5, 7, NA)\ncoalesce(x, 0)\n\n[1] 1 4 5 7 0\n\n#&gt; [1] 1 4 5 7 0\n\nGoing the other direciton, from some special value like 999 to NA can be handled upon reading data into R. If you can’t do that, use na_if().\n\nx &lt;- c(1, 4, 5, 7, -99)\nna_if(x, -99)\n\n[1]  1  4  5  7 NA\n\n#&gt; [1]  1  4  5  7 NA\n\n\n\n\nImplicit refers to the entire row of data being removed from the data frame, rather than explicitly filling each column in a given row with NA.\n\nstocks &lt;- tibble(\n  year  = c(2020, 2020, 2020, 2020, 2021, 2021, 2021),\n  qtr   = c(   1,    2,    3,    4,    2,    3,    4),\n  price = c(1.88, 0.59, 0.35,   NA, 0.92, 0.17, 2.66)\n)\n\nThis dataset has two missing observations:\nThe price in the fourth quarter of 2020 is explicitly missing, because its value is NA.\nThe price for the first quarter of 2021 is implicitly missing, because it simply does not appear in the dataset.\nMoving between implicit and explicit missingness may be necessary.\nPivoting. Pivoting wider makes implicit missingness explicit.\n\nstocks |&gt;\n  pivot_wider(\n    names_from = qtr, \n    values_from = price\n  )\n\n# A tibble: 2 × 5\n   year   `1`   `2`   `3`   `4`\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  2020  1.88  0.59  0.35 NA   \n2  2021 NA     0.92  0.17  2.66\n\n#&gt; # A tibble: 2 × 5\n#&gt;    year   `1`   `2`   `3`   `4`\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1  2020  1.88  0.59  0.35 NA   \n#&gt; 2  2021 NA     0.92  0.17  2.66\n\nComplete. tidyr::complete() generates explicit missing values by defining the combination of rows that should exist. If we want all combinations of year and quarter to exist:\n\nstocks |&gt;\n  complete(year, qtr)\n\n# A tibble: 8 × 3\n   year   qtr price\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  2020     1  1.88\n2  2020     2  0.59\n3  2020     3  0.35\n4  2020     4 NA   \n5  2021     1 NA   \n6  2021     2  0.92\n7  2021     3  0.17\n8  2021     4  2.66\n\n#&gt; # A tibble: 8 × 3\n#&gt;    year   qtr price\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1  2020     1  1.88\n#&gt; 2  2020     2  0.59\n#&gt; 3  2020     3  0.35\n#&gt; 4  2020     4 NA   \n#&gt; 5  2021     1 NA   \n#&gt; 6  2021     2  0.92\n#&gt; # ℹ 2 more rows\n\nYou can also use complete() on an individual variable:\n\nstocks |&gt;\n  complete(year = 2019:2021, qtr)\n\n# A tibble: 12 × 3\n    year   qtr price\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  2019     1 NA   \n 2  2019     2 NA   \n 3  2019     3 NA   \n 4  2019     4 NA   \n 5  2020     1  1.88\n 6  2020     2  0.59\n 7  2020     3  0.35\n 8  2020     4 NA   \n 9  2021     1 NA   \n10  2021     2  0.92\n11  2021     3  0.17\n12  2021     4  2.66\n\n#&gt; # A tibble: 12 × 3\n#&gt;    year   qtr price\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1  2019     1 NA   \n#&gt; 2  2019     2 NA   \n#&gt; 3  2019     3 NA   \n#&gt; 4  2019     4 NA   \n#&gt; 5  2020     1  1.88\n#&gt; 6  2020     2  0.59\n#&gt; # ℹ 6 more rows\n\nYou can also use joins to reveal implicitly missing observations.\ndplyr::anti_join(x,y) selects only the rows in x that don’t have a match in y.\n\nflights |&gt; \n  distinct(faa = dest) |&gt; \n  anti_join(airports)\n\nJoining with `by = join_by(faa)`\n\n\n# A tibble: 4 × 1\n  faa  \n  &lt;chr&gt;\n1 BQN  \n2 SJU  \n3 STT  \n4 PSE  \n\n#&gt; Joining with `by = join_by(faa)`\n#&gt; # A tibble: 4 × 1\n#&gt;   faa  \n#&gt;   &lt;chr&gt;\n#&gt; 1 BQN  \n#&gt; 2 SJU  \n#&gt; 3 STT  \n#&gt; 4 PSE\n\n\nflights |&gt; \n  distinct(tailnum) |&gt; \n  anti_join(planes)\n\nJoining with `by = join_by(tailnum)`\n\n\n# A tibble: 722 × 1\n   tailnum\n   &lt;chr&gt;  \n 1 N3ALAA \n 2 N3DUAA \n 3 N542MQ \n 4 N730MQ \n 5 N9EAMQ \n 6 N532UA \n 7 N3EMAA \n 8 N518MQ \n 9 N3BAAA \n10 N3CYAA \n# ℹ 712 more rows\n\n#&gt; Joining with `by = join_by(tailnum)`\n#&gt; # A tibble: 722 × 1\n#&gt;   tailnum\n#&gt;   &lt;chr&gt;  \n#&gt; 1 N3ALAA \n#&gt; 2 N3DUAA \n#&gt; 3 N542MQ \n#&gt; 4 N730MQ \n#&gt; 5 N9EAMQ \n#&gt; 6 N532UA \n#&gt; # ℹ 716 more rows\n\n\n\n\nA type of missingness is an empty group, or a factor with a level containing zero observations.\n\nhealth &lt;- tibble(\n  name   = c(\"Ikaia\", \"Oletta\", \"Leriah\", \"Dashay\", \"Tresaun\"),\n  smoker = factor(c(\"no\", \"no\", \"no\", \"no\", \"no\"), levels = c(\"yes\", \"no\")),\n  age    = c(34, 88, 75, 47, 56),\n)\n\ncount() automatically removes the level with 0 observations:\n\nhealth |&gt; count(smoker)\n\n# A tibble: 1 × 2\n  smoker     n\n  &lt;fct&gt;  &lt;int&gt;\n1 no         5\n\n#&gt; # A tibble: 1 × 2\n#&gt;   smoker     n\n#&gt;   &lt;fct&gt;  &lt;int&gt;\n#&gt; 1 no         5\n\nbut we can keep it with .drop = FALSE.\n\nhealth |&gt; count(smoker, .drop = FALSE)\n\n# A tibble: 2 × 2\n  smoker     n\n  &lt;fct&gt;  &lt;int&gt;\n1 yes        0\n2 no         5\n\n#&gt; # A tibble: 2 × 2\n#&gt;   smoker     n\n#&gt;   &lt;fct&gt;  &lt;int&gt;\n#&gt; 1 yes        0\n#&gt; 2 no         5\n\nThe same behavior applies to ggplot2’s discrete axes:\n\nggplot(health, aes(x = smoker)) +\n  geom_bar() +\n  scale_x_discrete()\n\n\n\n\n\n\n\nggplot(health, aes(x = smoker)) +\n  geom_bar() +\n  scale_x_discrete(drop = FALSE)\n\n\n\n\n\n\n\n\nThe same behavior applies to group_by().\n\nhealth |&gt; \n  group_by(smoker, .drop = FALSE) |&gt; \n  summarize(\n    n = n(),\n    mean_age = mean(age),\n    min_age = min(age),\n    max_age = max(age),\n    sd_age = sd(age)\n  )\n\nWarning: There were 2 warnings in `summarize()`.\nThe first warning was:\nℹ In argument: `min_age = min(age)`.\nℹ In group 1: `smoker = yes`.\nCaused by warning in `min()`:\n! no non-missing arguments to min; returning Inf\nℹ Run `dplyr::last_dplyr_warnings()` to see the 1 remaining warning.\n\n\n# A tibble: 2 × 6\n  smoker     n mean_age min_age max_age sd_age\n  &lt;fct&gt;  &lt;int&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1 yes        0      NaN     Inf    -Inf   NA  \n2 no         5       60      34      88   21.6\n\n#&gt; # A tibble: 2 × 6\n#&gt;   smoker     n mean_age min_age max_age sd_age\n#&gt;   &lt;fct&gt;  &lt;int&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 yes        0      NaN     Inf    -Inf   NA  \n#&gt; 2 no         5       60      34      88   21.6"
  },
  {
    "objectID": "chapter_18_notes.html#explicit-missing-values",
    "href": "chapter_18_notes.html#explicit-missing-values",
    "title": "R for Data Science, 2nd Edition - Chapter 18 Notes",
    "section": "",
    "text": "Creating or eliminating missing values.\n\ntreatment &lt;- tribble(\n  ~person,           ~treatment, ~response,\n  \"Derrick Whitmore\", 1,         7,\n  NA,                 2,         10,\n  NA,                 3,         NA,\n  \"Katherine Burke\",  1,         4\n)\n\nYou can fill in missing values with tidyr::fill().\n\ntreatment |&gt;\n  fill(everything())\n\n# A tibble: 4 × 3\n  person           treatment response\n  &lt;chr&gt;                &lt;dbl&gt;    &lt;dbl&gt;\n1 Derrick Whitmore         1        7\n2 Derrick Whitmore         2       10\n3 Derrick Whitmore         3       10\n4 Katherine Burke          1        4\n\n#&gt; # A tibble: 4 × 3\n#&gt;   person           treatment response\n#&gt;   &lt;chr&gt;                &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1 Derrick Whitmore         1        7\n#&gt; 2 Derrick Whitmore         2       10\n#&gt; 3 Derrick Whitmore         3       10\n#&gt; 4 Katherine Burke          1        4\n\nThe default behavior of fill() is to use last observation carried forward, but this can be altered using the .direction argument.\nIf missingness can be specified by a number, you can use dplyr::coalesce() to replace NA with the releveant number:\n\nx &lt;- c(1, 4, 5, 7, NA)\ncoalesce(x, 0)\n\n[1] 1 4 5 7 0\n\n#&gt; [1] 1 4 5 7 0\n\nGoing the other direciton, from some special value like 999 to NA can be handled upon reading data into R. If you can’t do that, use na_if().\n\nx &lt;- c(1, 4, 5, 7, -99)\nna_if(x, -99)\n\n[1]  1  4  5  7 NA\n\n#&gt; [1]  1  4  5  7 NA"
  },
  {
    "objectID": "chapter_18_notes.html#implicit-missing-values",
    "href": "chapter_18_notes.html#implicit-missing-values",
    "title": "R for Data Science, 2nd Edition - Chapter 18 Notes",
    "section": "",
    "text": "Implicit refers to the entire row of data being removed from the data frame, rather than explicitly filling each column in a given row with NA.\n\nstocks &lt;- tibble(\n  year  = c(2020, 2020, 2020, 2020, 2021, 2021, 2021),\n  qtr   = c(   1,    2,    3,    4,    2,    3,    4),\n  price = c(1.88, 0.59, 0.35,   NA, 0.92, 0.17, 2.66)\n)\n\nThis dataset has two missing observations:\nThe price in the fourth quarter of 2020 is explicitly missing, because its value is NA.\nThe price for the first quarter of 2021 is implicitly missing, because it simply does not appear in the dataset.\nMoving between implicit and explicit missingness may be necessary.\nPivoting. Pivoting wider makes implicit missingness explicit.\n\nstocks |&gt;\n  pivot_wider(\n    names_from = qtr, \n    values_from = price\n  )\n\n# A tibble: 2 × 5\n   year   `1`   `2`   `3`   `4`\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  2020  1.88  0.59  0.35 NA   \n2  2021 NA     0.92  0.17  2.66\n\n#&gt; # A tibble: 2 × 5\n#&gt;    year   `1`   `2`   `3`   `4`\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1  2020  1.88  0.59  0.35 NA   \n#&gt; 2  2021 NA     0.92  0.17  2.66\n\nComplete. tidyr::complete() generates explicit missing values by defining the combination of rows that should exist. If we want all combinations of year and quarter to exist:\n\nstocks |&gt;\n  complete(year, qtr)\n\n# A tibble: 8 × 3\n   year   qtr price\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  2020     1  1.88\n2  2020     2  0.59\n3  2020     3  0.35\n4  2020     4 NA   \n5  2021     1 NA   \n6  2021     2  0.92\n7  2021     3  0.17\n8  2021     4  2.66\n\n#&gt; # A tibble: 8 × 3\n#&gt;    year   qtr price\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1  2020     1  1.88\n#&gt; 2  2020     2  0.59\n#&gt; 3  2020     3  0.35\n#&gt; 4  2020     4 NA   \n#&gt; 5  2021     1 NA   \n#&gt; 6  2021     2  0.92\n#&gt; # ℹ 2 more rows\n\nYou can also use complete() on an individual variable:\n\nstocks |&gt;\n  complete(year = 2019:2021, qtr)\n\n# A tibble: 12 × 3\n    year   qtr price\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  2019     1 NA   \n 2  2019     2 NA   \n 3  2019     3 NA   \n 4  2019     4 NA   \n 5  2020     1  1.88\n 6  2020     2  0.59\n 7  2020     3  0.35\n 8  2020     4 NA   \n 9  2021     1 NA   \n10  2021     2  0.92\n11  2021     3  0.17\n12  2021     4  2.66\n\n#&gt; # A tibble: 12 × 3\n#&gt;    year   qtr price\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1  2019     1 NA   \n#&gt; 2  2019     2 NA   \n#&gt; 3  2019     3 NA   \n#&gt; 4  2019     4 NA   \n#&gt; 5  2020     1  1.88\n#&gt; 6  2020     2  0.59\n#&gt; # ℹ 6 more rows\n\nYou can also use joins to reveal implicitly missing observations.\ndplyr::anti_join(x,y) selects only the rows in x that don’t have a match in y.\n\nflights |&gt; \n  distinct(faa = dest) |&gt; \n  anti_join(airports)\n\nJoining with `by = join_by(faa)`\n\n\n# A tibble: 4 × 1\n  faa  \n  &lt;chr&gt;\n1 BQN  \n2 SJU  \n3 STT  \n4 PSE  \n\n#&gt; Joining with `by = join_by(faa)`\n#&gt; # A tibble: 4 × 1\n#&gt;   faa  \n#&gt;   &lt;chr&gt;\n#&gt; 1 BQN  \n#&gt; 2 SJU  \n#&gt; 3 STT  \n#&gt; 4 PSE\n\n\nflights |&gt; \n  distinct(tailnum) |&gt; \n  anti_join(planes)\n\nJoining with `by = join_by(tailnum)`\n\n\n# A tibble: 722 × 1\n   tailnum\n   &lt;chr&gt;  \n 1 N3ALAA \n 2 N3DUAA \n 3 N542MQ \n 4 N730MQ \n 5 N9EAMQ \n 6 N532UA \n 7 N3EMAA \n 8 N518MQ \n 9 N3BAAA \n10 N3CYAA \n# ℹ 712 more rows\n\n#&gt; Joining with `by = join_by(tailnum)`\n#&gt; # A tibble: 722 × 1\n#&gt;   tailnum\n#&gt;   &lt;chr&gt;  \n#&gt; 1 N3ALAA \n#&gt; 2 N3DUAA \n#&gt; 3 N542MQ \n#&gt; 4 N730MQ \n#&gt; 5 N9EAMQ \n#&gt; 6 N532UA \n#&gt; # ℹ 716 more rows"
  },
  {
    "objectID": "chapter_18_notes.html#factors-and-empty-groups",
    "href": "chapter_18_notes.html#factors-and-empty-groups",
    "title": "R for Data Science, 2nd Edition - Chapter 18 Notes",
    "section": "",
    "text": "A type of missingness is an empty group, or a factor with a level containing zero observations.\n\nhealth &lt;- tibble(\n  name   = c(\"Ikaia\", \"Oletta\", \"Leriah\", \"Dashay\", \"Tresaun\"),\n  smoker = factor(c(\"no\", \"no\", \"no\", \"no\", \"no\"), levels = c(\"yes\", \"no\")),\n  age    = c(34, 88, 75, 47, 56),\n)\n\ncount() automatically removes the level with 0 observations:\n\nhealth |&gt; count(smoker)\n\n# A tibble: 1 × 2\n  smoker     n\n  &lt;fct&gt;  &lt;int&gt;\n1 no         5\n\n#&gt; # A tibble: 1 × 2\n#&gt;   smoker     n\n#&gt;   &lt;fct&gt;  &lt;int&gt;\n#&gt; 1 no         5\n\nbut we can keep it with .drop = FALSE.\n\nhealth |&gt; count(smoker, .drop = FALSE)\n\n# A tibble: 2 × 2\n  smoker     n\n  &lt;fct&gt;  &lt;int&gt;\n1 yes        0\n2 no         5\n\n#&gt; # A tibble: 2 × 2\n#&gt;   smoker     n\n#&gt;   &lt;fct&gt;  &lt;int&gt;\n#&gt; 1 yes        0\n#&gt; 2 no         5\n\nThe same behavior applies to ggplot2’s discrete axes:\n\nggplot(health, aes(x = smoker)) +\n  geom_bar() +\n  scale_x_discrete()\n\n\n\n\n\n\n\nggplot(health, aes(x = smoker)) +\n  geom_bar() +\n  scale_x_discrete(drop = FALSE)\n\n\n\n\n\n\n\n\nThe same behavior applies to group_by().\n\nhealth |&gt; \n  group_by(smoker, .drop = FALSE) |&gt; \n  summarize(\n    n = n(),\n    mean_age = mean(age),\n    min_age = min(age),\n    max_age = max(age),\n    sd_age = sd(age)\n  )\n\nWarning: There were 2 warnings in `summarize()`.\nThe first warning was:\nℹ In argument: `min_age = min(age)`.\nℹ In group 1: `smoker = yes`.\nCaused by warning in `min()`:\n! no non-missing arguments to min; returning Inf\nℹ Run `dplyr::last_dplyr_warnings()` to see the 1 remaining warning.\n\n\n# A tibble: 2 × 6\n  smoker     n mean_age min_age max_age sd_age\n  &lt;fct&gt;  &lt;int&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1 yes        0      NaN     Inf    -Inf   NA  \n2 no         5       60      34      88   21.6\n\n#&gt; # A tibble: 2 × 6\n#&gt;   smoker     n mean_age min_age max_age sd_age\n#&gt;   &lt;fct&gt;  &lt;int&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 yes        0      NaN     Inf    -Inf   NA  \n#&gt; 2 no         5       60      34      88   21.6"
  },
  {
    "objectID": "chapter_19_notes.html",
    "href": "chapter_19_notes.html",
    "title": "R for Data Science, 2nd Edition - Chapter 19 Notes",
    "section": "",
    "text": "There are two main types of joins: mutating and filtering\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.6\n✔ forcats   1.0.1     ✔ stringr   1.6.0\n✔ ggplot2   4.0.1     ✔ tibble    3.3.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.2\n✔ purrr     1.2.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(nycflights13)\n\n\n\nPrimary and foreign keys. Primary keys uniquely identify an observation. A foreign key a variable or set of variables that corresponds to a primary key in another table.\ncheck that your primary key is unique:\n\nplanes |&gt; \n  count(tailnum) |&gt; \n  filter(n &gt; 1)\n\n# A tibble: 0 × 2\n# ℹ 2 variables: tailnum &lt;chr&gt;, n &lt;int&gt;\n\n#&gt; # A tibble: 0 × 2\n#&gt; # ℹ 2 variables: tailnum &lt;chr&gt;, n &lt;int&gt;\n\nweather |&gt; \n  count(time_hour, origin) |&gt; \n  filter(n &gt; 1)\n\n# A tibble: 0 × 3\n# ℹ 3 variables: time_hour &lt;dttm&gt;, origin &lt;chr&gt;, n &lt;int&gt;\n\n#&gt; # A tibble: 0 × 3\n#&gt; # ℹ 3 variables: time_hour &lt;dttm&gt;, origin &lt;chr&gt;, n &lt;int&gt;\n\nShould probably also check for missing values on primary keys:\n\nplanes |&gt; \n  filter(is.na(tailnum))\n\n# A tibble: 0 × 9\n# ℹ 9 variables: tailnum &lt;chr&gt;, year &lt;int&gt;, type &lt;chr&gt;, manufacturer &lt;chr&gt;,\n#   model &lt;chr&gt;, engines &lt;int&gt;, seats &lt;int&gt;, speed &lt;int&gt;, engine &lt;chr&gt;\n\n#&gt; # A tibble: 0 × 9\n#&gt; # ℹ 9 variables: tailnum &lt;chr&gt;, year &lt;int&gt;, type &lt;chr&gt;, manufacturer &lt;chr&gt;,\n#&gt; #   model &lt;chr&gt;, engines &lt;int&gt;, seats &lt;int&gt;, speed &lt;int&gt;, engine &lt;chr&gt;\n\nweather |&gt; \n  filter(is.na(time_hour) | is.na(origin))\n\n# A tibble: 0 × 15\n# ℹ 15 variables: origin &lt;chr&gt;, year &lt;int&gt;, month &lt;int&gt;, day &lt;int&gt;, hour &lt;int&gt;,\n#   temp &lt;dbl&gt;, dewp &lt;dbl&gt;, humid &lt;dbl&gt;, wind_dir &lt;dbl&gt;, wind_speed &lt;dbl&gt;,\n#   wind_gust &lt;dbl&gt;, precip &lt;dbl&gt;, pressure &lt;dbl&gt;, visib &lt;dbl&gt;,\n#   time_hour &lt;dttm&gt;\n\n#&gt; # A tibble: 0 × 15\n#&gt; # ℹ 15 variables: origin &lt;chr&gt;, year &lt;int&gt;, month &lt;int&gt;, day &lt;int&gt;,\n#&gt; #   hour &lt;int&gt;, temp &lt;dbl&gt;, dewp &lt;dbl&gt;, humid &lt;dbl&gt;, wind_dir &lt;dbl&gt;, …\n\nfor the flights data frame, a compound primary key can be formed like so:\n\nflights |&gt; \n  count(time_hour, carrier, flight) |&gt; \n  filter(n &gt; 1)\n\n# A tibble: 0 × 4\n# ℹ 4 variables: time_hour &lt;dttm&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, n &lt;int&gt;\n\n#&gt; # A tibble: 0 × 4\n#&gt; # ℹ 4 variables: time_hour &lt;dttm&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, n &lt;int&gt;\n\nIf you don’t have a clear primary key, you can use a surrogate:\n\nflights2 &lt;- flights |&gt; \n  mutate(id = row_number(), .before = 1)\nflights2\n\n# A tibble: 336,776 × 20\n      id  year month   day dep_time sched_dep_time dep_delay arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;\n 1     1  2013     1     1      517            515         2      830\n 2     2  2013     1     1      533            529         4      850\n 3     3  2013     1     1      542            540         2      923\n 4     4  2013     1     1      544            545        -1     1004\n 5     5  2013     1     1      554            600        -6      812\n 6     6  2013     1     1      554            558        -4      740\n 7     7  2013     1     1      555            600        -5      913\n 8     8  2013     1     1      557            600        -3      709\n 9     9  2013     1     1      557            600        -3      838\n10    10  2013     1     1      558            600        -2      753\n# ℹ 336,766 more rows\n# ℹ 12 more variables: sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;,\n#   flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;,\n#   distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n#&gt; # A tibble: 336,776 × 20\n#&gt;      id  year month   day dep_time sched_dep_time dep_delay arr_time\n#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;\n#&gt; 1     1  2013     1     1      517            515         2      830\n#&gt; 2     2  2013     1     1      533            529         4      850\n#&gt; 3     3  2013     1     1      542            540         2      923\n#&gt; 4     4  2013     1     1      544            545        -1     1004\n#&gt; 5     5  2013     1     1      554            600        -6      812\n#&gt; 6     6  2013     1     1      554            558        -4      740\n#&gt; # ℹ 336,770 more rows\n#&gt; # ℹ 12 more variables: sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, …\n\nMutating joins.\nMake a more narrow data frame first.\n\nflights2 &lt;- flights |&gt; \n  select(year, time_hour, origin, dest, tailnum, carrier)\nflights2\n\n# A tibble: 336,776 × 6\n    year time_hour           origin dest  tailnum carrier\n   &lt;int&gt; &lt;dttm&gt;              &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;  \n 1  2013 2013-01-01 05:00:00 EWR    IAH   N14228  UA     \n 2  2013 2013-01-01 05:00:00 LGA    IAH   N24211  UA     \n 3  2013 2013-01-01 05:00:00 JFK    MIA   N619AA  AA     \n 4  2013 2013-01-01 05:00:00 JFK    BQN   N804JB  B6     \n 5  2013 2013-01-01 06:00:00 LGA    ATL   N668DN  DL     \n 6  2013 2013-01-01 05:00:00 EWR    ORD   N39463  UA     \n 7  2013 2013-01-01 06:00:00 EWR    FLL   N516JB  B6     \n 8  2013 2013-01-01 06:00:00 LGA    IAD   N829AS  EV     \n 9  2013 2013-01-01 06:00:00 JFK    MCO   N593JB  B6     \n10  2013 2013-01-01 06:00:00 LGA    ORD   N3ALAA  AA     \n# ℹ 336,766 more rows\n\n#&gt; # A tibble: 336,776 × 6\n#&gt;    year time_hour           origin dest  tailnum carrier\n#&gt;   &lt;int&gt; &lt;dttm&gt;              &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;  \n#&gt; 1  2013 2013-01-01 05:00:00 EWR    IAH   N14228  UA     \n#&gt; 2  2013 2013-01-01 05:00:00 LGA    IAH   N24211  UA     \n#&gt; 3  2013 2013-01-01 05:00:00 JFK    MIA   N619AA  AA     \n#&gt; 4  2013 2013-01-01 05:00:00 JFK    BQN   N804JB  B6     \n#&gt; 5  2013 2013-01-01 06:00:00 LGA    ATL   N668DN  DL     \n#&gt; 6  2013 2013-01-01 05:00:00 EWR    ORD   N39463  UA     \n#&gt; # ℹ 336,770 more rows\n\nadd airline information to flights\n\nflights2 |&gt;\n  left_join(airlines)\n\nJoining with `by = join_by(carrier)`\n\n\n# A tibble: 336,776 × 7\n    year time_hour           origin dest  tailnum carrier name                  \n   &lt;int&gt; &lt;dttm&gt;              &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;                 \n 1  2013 2013-01-01 05:00:00 EWR    IAH   N14228  UA      United Air Lines Inc. \n 2  2013 2013-01-01 05:00:00 LGA    IAH   N24211  UA      United Air Lines Inc. \n 3  2013 2013-01-01 05:00:00 JFK    MIA   N619AA  AA      American Airlines Inc.\n 4  2013 2013-01-01 05:00:00 JFK    BQN   N804JB  B6      JetBlue Airways       \n 5  2013 2013-01-01 06:00:00 LGA    ATL   N668DN  DL      Delta Air Lines Inc.  \n 6  2013 2013-01-01 05:00:00 EWR    ORD   N39463  UA      United Air Lines Inc. \n 7  2013 2013-01-01 06:00:00 EWR    FLL   N516JB  B6      JetBlue Airways       \n 8  2013 2013-01-01 06:00:00 LGA    IAD   N829AS  EV      ExpressJet Airlines I…\n 9  2013 2013-01-01 06:00:00 JFK    MCO   N593JB  B6      JetBlue Airways       \n10  2013 2013-01-01 06:00:00 LGA    ORD   N3ALAA  AA      American Airlines Inc.\n# ℹ 336,766 more rows\n\n#&gt; Joining with `by = join_by(carrier)`\n#&gt; # A tibble: 336,776 × 7\n#&gt;    year time_hour           origin dest  tailnum carrier name                \n#&gt;   &lt;int&gt; &lt;dttm&gt;              &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;               \n#&gt; 1  2013 2013-01-01 05:00:00 EWR    IAH   N14228  UA      United Air Lines In…\n#&gt; 2  2013 2013-01-01 05:00:00 LGA    IAH   N24211  UA      United Air Lines In…\n#&gt; 3  2013 2013-01-01 05:00:00 JFK    MIA   N619AA  AA      American Airlines I…\n#&gt; 4  2013 2013-01-01 05:00:00 JFK    BQN   N804JB  B6      JetBlue Airways     \n#&gt; 5  2013 2013-01-01 06:00:00 LGA    ATL   N668DN  DL      Delta Air Lines Inc.\n#&gt; 6  2013 2013-01-01 05:00:00 EWR    ORD   N39463  UA      United Air Lines In…\n#&gt; # ℹ 336,770 more rows\n\nFind weather when each flight departed. Note the use of select within left_join():\n\nflights2 |&gt; \n  left_join(weather |&gt; select(origin, time_hour, temp, wind_speed))\n\nJoining with `by = join_by(time_hour, origin)`\n\n\n# A tibble: 336,776 × 8\n    year time_hour           origin dest  tailnum carrier  temp wind_speed\n   &lt;int&gt; &lt;dttm&gt;              &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n 1  2013 2013-01-01 05:00:00 EWR    IAH   N14228  UA       39.0       12.7\n 2  2013 2013-01-01 05:00:00 LGA    IAH   N24211  UA       39.9       15.0\n 3  2013 2013-01-01 05:00:00 JFK    MIA   N619AA  AA       39.0       15.0\n 4  2013 2013-01-01 05:00:00 JFK    BQN   N804JB  B6       39.0       15.0\n 5  2013 2013-01-01 06:00:00 LGA    ATL   N668DN  DL       39.9       16.1\n 6  2013 2013-01-01 05:00:00 EWR    ORD   N39463  UA       39.0       12.7\n 7  2013 2013-01-01 06:00:00 EWR    FLL   N516JB  B6       37.9       11.5\n 8  2013 2013-01-01 06:00:00 LGA    IAD   N829AS  EV       39.9       16.1\n 9  2013 2013-01-01 06:00:00 JFK    MCO   N593JB  B6       37.9       13.8\n10  2013 2013-01-01 06:00:00 LGA    ORD   N3ALAA  AA       39.9       16.1\n# ℹ 336,766 more rows\n\n#&gt; Joining with `by = join_by(time_hour, origin)`\n#&gt; # A tibble: 336,776 × 8\n#&gt;    year time_hour           origin dest  tailnum carrier  temp wind_speed\n#&gt;   &lt;int&gt; &lt;dttm&gt;              &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n#&gt; 1  2013 2013-01-01 05:00:00 EWR    IAH   N14228  UA       39.0       12.7\n#&gt; 2  2013 2013-01-01 05:00:00 LGA    IAH   N24211  UA       39.9       15.0\n#&gt; 3  2013 2013-01-01 05:00:00 JFK    MIA   N619AA  AA       39.0       15.0\n#&gt; 4  2013 2013-01-01 05:00:00 JFK    BQN   N804JB  B6       39.0       15.0\n#&gt; 5  2013 2013-01-01 06:00:00 LGA    ATL   N668DN  DL       39.9       16.1\n#&gt; 6  2013 2013-01-01 05:00:00 EWR    ORD   N39463  UA       39.0       12.7\n#&gt; # ℹ 336,770 more rows\n\nWhen no match is found for a row in x, the new variables get NA in the new columns.\n\nflights2 |&gt; \n  filter(tailnum == \"N3ALAA\") |&gt; \n  left_join(planes |&gt; select(tailnum, type, engines, seats))\n\nJoining with `by = join_by(tailnum)`\n\n\n# A tibble: 63 × 9\n    year time_hour           origin dest  tailnum carrier type  engines seats\n   &lt;int&gt; &lt;dttm&gt;              &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;int&gt; &lt;int&gt;\n 1  2013 2013-01-01 06:00:00 LGA    ORD   N3ALAA  AA      &lt;NA&gt;       NA    NA\n 2  2013 2013-01-02 18:00:00 LGA    ORD   N3ALAA  AA      &lt;NA&gt;       NA    NA\n 3  2013 2013-01-03 06:00:00 LGA    ORD   N3ALAA  AA      &lt;NA&gt;       NA    NA\n 4  2013 2013-01-07 19:00:00 LGA    ORD   N3ALAA  AA      &lt;NA&gt;       NA    NA\n 5  2013 2013-01-08 17:00:00 JFK    ORD   N3ALAA  AA      &lt;NA&gt;       NA    NA\n 6  2013 2013-01-16 06:00:00 LGA    ORD   N3ALAA  AA      &lt;NA&gt;       NA    NA\n 7  2013 2013-01-20 18:00:00 LGA    ORD   N3ALAA  AA      &lt;NA&gt;       NA    NA\n 8  2013 2013-01-22 17:00:00 JFK    ORD   N3ALAA  AA      &lt;NA&gt;       NA    NA\n 9  2013 2013-10-11 06:00:00 EWR    MIA   N3ALAA  AA      &lt;NA&gt;       NA    NA\n10  2013 2013-10-14 08:00:00 JFK    BOS   N3ALAA  AA      &lt;NA&gt;       NA    NA\n# ℹ 53 more rows\n\n#&gt; Joining with `by = join_by(tailnum)`\n#&gt; # A tibble: 63 × 9\n#&gt;    year time_hour           origin dest  tailnum carrier type  engines seats\n#&gt;   &lt;int&gt; &lt;dttm&gt;              &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;int&gt; &lt;int&gt;\n#&gt; 1  2013 2013-01-01 06:00:00 LGA    ORD   N3ALAA  AA      &lt;NA&gt;       NA    NA\n#&gt; 2  2013 2013-01-02 18:00:00 LGA    ORD   N3ALAA  AA      &lt;NA&gt;       NA    NA\n#&gt; 3  2013 2013-01-03 06:00:00 LGA    ORD   N3ALAA  AA      &lt;NA&gt;       NA    NA\n#&gt; 4  2013 2013-01-07 19:00:00 LGA    ORD   N3ALAA  AA      &lt;NA&gt;       NA    NA\n#&gt; 5  2013 2013-01-08 17:00:00 JFK    ORD   N3ALAA  AA      &lt;NA&gt;       NA    NA\n#&gt; 6  2013 2013-01-16 06:00:00 LGA    ORD   N3ALAA  AA      &lt;NA&gt;       NA    NA\n#&gt; # ℹ 57 more rows\n\nBy default, left_join() uses all columns that appear in both data frames as the join key. But you can and should specify the joining keys:\n\nflights2 |&gt; \n  left_join(planes, join_by(tailnum))\n\n# A tibble: 336,776 × 14\n   year.x time_hour           origin dest  tailnum carrier year.y type          \n    &lt;int&gt; &lt;dttm&gt;              &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;    &lt;int&gt; &lt;chr&gt;         \n 1   2013 2013-01-01 05:00:00 EWR    IAH   N14228  UA        1999 Fixed wing mu…\n 2   2013 2013-01-01 05:00:00 LGA    IAH   N24211  UA        1998 Fixed wing mu…\n 3   2013 2013-01-01 05:00:00 JFK    MIA   N619AA  AA        1990 Fixed wing mu…\n 4   2013 2013-01-01 05:00:00 JFK    BQN   N804JB  B6        2012 Fixed wing mu…\n 5   2013 2013-01-01 06:00:00 LGA    ATL   N668DN  DL        1991 Fixed wing mu…\n 6   2013 2013-01-01 05:00:00 EWR    ORD   N39463  UA        2012 Fixed wing mu…\n 7   2013 2013-01-01 06:00:00 EWR    FLL   N516JB  B6        2000 Fixed wing mu…\n 8   2013 2013-01-01 06:00:00 LGA    IAD   N829AS  EV        1998 Fixed wing mu…\n 9   2013 2013-01-01 06:00:00 JFK    MCO   N593JB  B6        2004 Fixed wing mu…\n10   2013 2013-01-01 06:00:00 LGA    ORD   N3ALAA  AA          NA &lt;NA&gt;          \n# ℹ 336,766 more rows\n# ℹ 6 more variables: manufacturer &lt;chr&gt;, model &lt;chr&gt;, engines &lt;int&gt;,\n#   seats &lt;int&gt;, speed &lt;int&gt;, engine &lt;chr&gt;\n\n#&gt; # A tibble: 336,776 × 14\n#&gt;   year.x time_hour           origin dest  tailnum carrier year.y\n#&gt;    &lt;int&gt; &lt;dttm&gt;              &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;    &lt;int&gt;\n#&gt; 1   2013 2013-01-01 05:00:00 EWR    IAH   N14228  UA        1999\n#&gt; 2   2013 2013-01-01 05:00:00 LGA    IAH   N24211  UA        1998\n#&gt; 3   2013 2013-01-01 05:00:00 JFK    MIA   N619AA  AA        1990\n#&gt; 4   2013 2013-01-01 05:00:00 JFK    BQN   N804JB  B6        2012\n#&gt; 5   2013 2013-01-01 06:00:00 LGA    ATL   N668DN  DL        1991\n#&gt; 6   2013 2013-01-01 05:00:00 EWR    ORD   N39463  UA        2012\n#&gt; # ℹ 336,770 more rows\n#&gt; # ℹ 7 more variables: type &lt;chr&gt;, manufacturer &lt;chr&gt;, model &lt;chr&gt;, …\n\nThis is short for join_by(tailnum == tailnum). That’s fine is the same variables have the same names across data frames. If the names are different, then you need to specify the equality of variables across different columns names in the join_by() argument.\n\nflights2 |&gt; \n  left_join(airports, join_by(dest == faa))\n\n# A tibble: 336,776 × 13\n    year time_hour           origin dest  tailnum carrier name         lat   lon\n   &lt;int&gt; &lt;dttm&gt;              &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;\n 1  2013 2013-01-01 05:00:00 EWR    IAH   N14228  UA      George Bu…  30.0 -95.3\n 2  2013 2013-01-01 05:00:00 LGA    IAH   N24211  UA      George Bu…  30.0 -95.3\n 3  2013 2013-01-01 05:00:00 JFK    MIA   N619AA  AA      Miami Intl  25.8 -80.3\n 4  2013 2013-01-01 05:00:00 JFK    BQN   N804JB  B6      &lt;NA&gt;        NA    NA  \n 5  2013 2013-01-01 06:00:00 LGA    ATL   N668DN  DL      Hartsfiel…  33.6 -84.4\n 6  2013 2013-01-01 05:00:00 EWR    ORD   N39463  UA      Chicago O…  42.0 -87.9\n 7  2013 2013-01-01 06:00:00 EWR    FLL   N516JB  B6      Fort Laud…  26.1 -80.2\n 8  2013 2013-01-01 06:00:00 LGA    IAD   N829AS  EV      Washingto…  38.9 -77.5\n 9  2013 2013-01-01 06:00:00 JFK    MCO   N593JB  B6      Orlando I…  28.4 -81.3\n10  2013 2013-01-01 06:00:00 LGA    ORD   N3ALAA  AA      Chicago O…  42.0 -87.9\n# ℹ 336,766 more rows\n# ℹ 4 more variables: alt &lt;dbl&gt;, tz &lt;dbl&gt;, dst &lt;chr&gt;, tzone &lt;chr&gt;\n\n#&gt; # A tibble: 336,776 × 13\n#&gt;    year time_hour           origin dest  tailnum carrier name                \n#&gt;   &lt;int&gt; &lt;dttm&gt;              &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;               \n#&gt; 1  2013 2013-01-01 05:00:00 EWR    IAH   N14228  UA      George Bush Interco…\n#&gt; 2  2013 2013-01-01 05:00:00 LGA    IAH   N24211  UA      George Bush Interco…\n#&gt; 3  2013 2013-01-01 05:00:00 JFK    MIA   N619AA  AA      Miami Intl          \n#&gt; 4  2013 2013-01-01 05:00:00 JFK    BQN   N804JB  B6      &lt;NA&gt;                \n#&gt; 5  2013 2013-01-01 06:00:00 LGA    ATL   N668DN  DL      Hartsfield Jackson …\n#&gt; 6  2013 2013-01-01 05:00:00 EWR    ORD   N39463  UA      Chicago Ohare Intl  \n#&gt; # ℹ 336,770 more rows\n#&gt; # ℹ 6 more variables: lat &lt;dbl&gt;, lon &lt;dbl&gt;, alt &lt;dbl&gt;, tz &lt;dbl&gt;, …\n\nflights2 |&gt; \n  left_join(airports, join_by(origin == faa))\n\n# A tibble: 336,776 × 13\n    year time_hour           origin dest  tailnum carrier name         lat   lon\n   &lt;int&gt; &lt;dttm&gt;              &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;\n 1  2013 2013-01-01 05:00:00 EWR    IAH   N14228  UA      Newark Li…  40.7 -74.2\n 2  2013 2013-01-01 05:00:00 LGA    IAH   N24211  UA      La Guardia  40.8 -73.9\n 3  2013 2013-01-01 05:00:00 JFK    MIA   N619AA  AA      John F Ke…  40.6 -73.8\n 4  2013 2013-01-01 05:00:00 JFK    BQN   N804JB  B6      John F Ke…  40.6 -73.8\n 5  2013 2013-01-01 06:00:00 LGA    ATL   N668DN  DL      La Guardia  40.8 -73.9\n 6  2013 2013-01-01 05:00:00 EWR    ORD   N39463  UA      Newark Li…  40.7 -74.2\n 7  2013 2013-01-01 06:00:00 EWR    FLL   N516JB  B6      Newark Li…  40.7 -74.2\n 8  2013 2013-01-01 06:00:00 LGA    IAD   N829AS  EV      La Guardia  40.8 -73.9\n 9  2013 2013-01-01 06:00:00 JFK    MCO   N593JB  B6      John F Ke…  40.6 -73.8\n10  2013 2013-01-01 06:00:00 LGA    ORD   N3ALAA  AA      La Guardia  40.8 -73.9\n# ℹ 336,766 more rows\n# ℹ 4 more variables: alt &lt;dbl&gt;, tz &lt;dbl&gt;, dst &lt;chr&gt;, tzone &lt;chr&gt;\n\n#&gt; # A tibble: 336,776 × 13\n#&gt;    year time_hour           origin dest  tailnum carrier name               \n#&gt;   &lt;int&gt; &lt;dttm&gt;              &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;              \n#&gt; 1  2013 2013-01-01 05:00:00 EWR    IAH   N14228  UA      Newark Liberty Intl\n#&gt; 2  2013 2013-01-01 05:00:00 LGA    IAH   N24211  UA      La Guardia         \n#&gt; 3  2013 2013-01-01 05:00:00 JFK    MIA   N619AA  AA      John F Kennedy Intl\n#&gt; 4  2013 2013-01-01 05:00:00 JFK    BQN   N804JB  B6      John F Kennedy Intl\n#&gt; 5  2013 2013-01-01 06:00:00 LGA    ATL   N668DN  DL      La Guardia         \n#&gt; 6  2013 2013-01-01 05:00:00 EWR    ORD   N39463  UA      Newark Liberty Intl\n#&gt; # ℹ 336,770 more rows\n#&gt; # ℹ 6 more variables: lat &lt;dbl&gt;, lon &lt;dbl&gt;, alt &lt;dbl&gt;, tz &lt;dbl&gt;, …\n\nOlder code may use by with a character vector to specify join keys. Prefer join_by().\nFiltering joins. There are two types: semi-joins and anti-joins.\nSemi-joins keep all rows in x that have a match in y. Show just origin airports:\n\nairports |&gt; \n  semi_join(flights2, join_by(faa == origin))\n\n# A tibble: 3 × 8\n  faa   name                  lat   lon   alt    tz dst   tzone           \n  &lt;chr&gt; &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;           \n1 EWR   Newark Liberty Intl  40.7 -74.2    18    -5 A     America/New_York\n2 JFK   John F Kennedy Intl  40.6 -73.8    13    -5 A     America/New_York\n3 LGA   La Guardia           40.8 -73.9    22    -5 A     America/New_York\n\n#&gt; # A tibble: 3 × 8\n#&gt;   faa   name                  lat   lon   alt    tz dst   tzone           \n#&gt;   &lt;chr&gt; &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;           \n#&gt; 1 EWR   Newark Liberty Intl  40.7 -74.2    18    -5 A     America/New_York\n#&gt; 2 JFK   John F Kennedy Intl  40.6 -73.8    13    -5 A     America/New_York\n#&gt; 3 LGA   La Guardia           40.8 -73.9    22    -5 A     America/New_York\n\nAnti-joins are the opposite: they return all rows in x that don’t have a match in y. These are useful for identifying implicit missing values.\n\nflights2 |&gt; \n  anti_join(airports, join_by(dest == faa)) |&gt; \n  distinct(dest)\n\n# A tibble: 4 × 1\n  dest \n  &lt;chr&gt;\n1 BQN  \n2 SJU  \n3 STT  \n4 PSE  \n\n#&gt; # A tibble: 4 × 1\n#&gt;   dest \n#&gt;   &lt;chr&gt;\n#&gt; 1 BQN  \n#&gt; 2 SJU  \n#&gt; 3 STT  \n#&gt; 4 PSE\n\nFind which tailnum() are missing from planes:\n\nflights2 |&gt;\n  anti_join(planes, join_by(tailnum)) |&gt; \n  distinct(tailnum)\n\n# A tibble: 722 × 1\n   tailnum\n   &lt;chr&gt;  \n 1 N3ALAA \n 2 N3DUAA \n 3 N542MQ \n 4 N730MQ \n 5 N9EAMQ \n 6 N532UA \n 7 N3EMAA \n 8 N518MQ \n 9 N3BAAA \n10 N3CYAA \n# ℹ 712 more rows\n\n#&gt; # A tibble: 722 × 1\n#&gt;   tailnum\n#&gt;   &lt;chr&gt;  \n#&gt; 1 N3ALAA \n#&gt; 2 N3DUAA \n#&gt; 3 N542MQ \n#&gt; 4 N730MQ \n#&gt; 5 N9EAMQ \n#&gt; 6 N532UA \n#&gt; # ℹ 716 more rows\n\nExercises 19.3.4 look especially useful to try out.\n\n\n\n\n\n\nThese are joins without a strict requirement of equality of keys. This makes keeping both keys used in the join more useful, since they may be slightly different. We could have done this in the earlier equi joins above:\n\n# x |&gt; inner_join(y, join_by(key == key), keep = TRUE)\n# #&gt; # A tibble: 2 × 4\n# #&gt;   key.x val_x key.y val_y\n# #&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;\n# #&gt; 1     1 x1        1 y1   \n# #&gt; 2     2 x2        2 y2\n\nYou can, for example, do matches like x$key &gt; y$key.\n\nCross joins match every pair of rows.\nInequality joins use &lt;, &lt;=, &gt;, and &gt;= instead of ==.\nRolling joins are similar to inequality joins but only find the closest match.\nOverlap joins are a special type of inequality join designed to work with ranges.\n\nCross joins are the cartesian product of rows:\n\ndf &lt;- tibble(name = c(\"John\", \"Simon\", \"Tracy\", \"Max\"))\ndf |&gt; cross_join(df)\n\n# A tibble: 16 × 2\n   name.x name.y\n   &lt;chr&gt;  &lt;chr&gt; \n 1 John   John  \n 2 John   Simon \n 3 John   Tracy \n 4 John   Max   \n 5 Simon  John  \n 6 Simon  Simon \n 7 Simon  Tracy \n 8 Simon  Max   \n 9 Tracy  John  \n10 Tracy  Simon \n11 Tracy  Tracy \n12 Tracy  Max   \n13 Max    John  \n14 Max    Simon \n15 Max    Tracy \n16 Max    Max   \n\n#&gt; # A tibble: 16 × 2\n#&gt;   name.x name.y\n#&gt;   &lt;chr&gt;  &lt;chr&gt; \n#&gt; 1 John   John  \n#&gt; 2 John   Simon \n#&gt; 3 John   Tracy \n#&gt; 4 John   Max   \n#&gt; 5 Simon  John  \n#&gt; 6 Simon  Simon \n#&gt; # ℹ 10 more rows\n\nInequality joins can be used to generate all combinations, instead of all permutations:\n\ndf &lt;- tibble(id = 1:4, name = c(\"John\", \"Simon\", \"Tracy\", \"Max\"))\n\ndf |&gt; inner_join(df, join_by(id &lt; id))\n\n# A tibble: 6 × 4\n   id.x name.x  id.y name.y\n  &lt;int&gt; &lt;chr&gt;  &lt;int&gt; &lt;chr&gt; \n1     1 John       2 Simon \n2     1 John       3 Tracy \n3     1 John       4 Max   \n4     2 Simon      3 Tracy \n5     2 Simon      4 Max   \n6     3 Tracy      4 Max   \n\n#&gt; # A tibble: 6 × 4\n#&gt;    id.x name.x  id.y name.y\n#&gt;   &lt;int&gt; &lt;chr&gt;  &lt;int&gt; &lt;chr&gt; \n#&gt; 1     1 John       2 Simon \n#&gt; 2     1 John       3 Tracy \n#&gt; 3     1 John       4 Max   \n#&gt; 4     2 Simon      3 Tracy \n#&gt; 5     2 Simon      4 Max   \n#&gt; 6     3 Tracy      4 Max\n\nRolling joins are similar to inequality joins, but only give you the closest row that satisfies the inequality:\nRolling joins are particularly useful when you have two tables of dates that don’t perfectly line up and you want to find (e.g.) the closest date in table 1 that comes before (or after) some date in table 2.\nQuarterly party planning for employee birthdays:\n\nparties &lt;- tibble(\n  q = 1:4,\n  party = ymd(c(\"2022-01-10\", \"2022-04-04\", \"2022-07-11\", \"2022-10-03\"))\n)\n\n\nset.seed(123)\nemployees &lt;- tibble(\n  name = sample(babynames::babynames$name, 100),\n  birthday = ymd(\"2022-01-01\") + (sample(365, 100, replace = TRUE) - 1)\n)\nemployees\n\n# A tibble: 100 × 2\n   name     birthday  \n   &lt;chr&gt;    &lt;date&gt;    \n 1 Kemba    2022-01-22\n 2 Orean    2022-06-26\n 3 Kirstyn  2022-02-11\n 4 Amparo   2022-11-11\n 5 Belen    2022-03-25\n 6 Rayshaun 2022-01-11\n 7 Brazil   2022-05-01\n 8 Chaston  2022-10-29\n 9 Reyn     2022-03-26\n10 Ogechi   2022-12-31\n# ℹ 90 more rows\n\n#&gt; # A tibble: 100 × 2\n#&gt;   name     birthday  \n#&gt;   &lt;chr&gt;    &lt;date&gt;    \n#&gt; 1 Kemba    2022-01-22\n#&gt; 2 Orean    2022-06-26\n#&gt; 3 Kirstyn  2022-02-11\n#&gt; 4 Amparo   2022-11-11\n#&gt; 5 Belen    2022-03-25\n#&gt; 6 Rayshaun 2022-01-11\n#&gt; # ℹ 94 more rows\n\nFor each employee we want to find the last party date that comes before (or on) their birthday. We can express that with a rolling join. The closest() function takes this from an inequality join to a rolling join:\n\nemployees |&gt; \n  left_join(parties, join_by(closest(birthday &gt;= party)))\n\n# A tibble: 100 × 4\n   name     birthday       q party     \n   &lt;chr&gt;    &lt;date&gt;     &lt;int&gt; &lt;date&gt;    \n 1 Kemba    2022-01-22     1 2022-01-10\n 2 Orean    2022-06-26     2 2022-04-04\n 3 Kirstyn  2022-02-11     1 2022-01-10\n 4 Amparo   2022-11-11     4 2022-10-03\n 5 Belen    2022-03-25     1 2022-01-10\n 6 Rayshaun 2022-01-11     1 2022-01-10\n 7 Brazil   2022-05-01     2 2022-04-04\n 8 Chaston  2022-10-29     4 2022-10-03\n 9 Reyn     2022-03-26     1 2022-01-10\n10 Ogechi   2022-12-31     4 2022-10-03\n# ℹ 90 more rows\n\n#&gt; # A tibble: 100 × 4\n#&gt;   name     birthday       q party     \n#&gt;   &lt;chr&gt;    &lt;date&gt;     &lt;int&gt; &lt;date&gt;    \n#&gt; 1 Kemba    2022-01-22     1 2022-01-10\n#&gt; 2 Orean    2022-06-26     2 2022-04-04\n#&gt; 3 Kirstyn  2022-02-11     1 2022-01-10\n#&gt; 4 Amparo   2022-11-11     4 2022-10-03\n#&gt; 5 Belen    2022-03-25     1 2022-01-10\n#&gt; 6 Rayshaun 2022-01-11     1 2022-01-10\n#&gt; # ℹ 94 more rows\n\nThe problem with this is that folks with birthdays before January 10 don’t get a party.\nOverlap joins. There are three helpers:\n\nbetween(x, y_lower, y_upper) is short for x &gt;= y_lower, x &lt;= y_upper.\nwithin(x_lower, x_upper, y_lower, y_upper) is short for x_lower &gt;= y_lower, x_upper &lt;= y_upper.\noverlaps(x_lower, x_upper, y_lower, y_upper) is short for x_lower &lt;= y_upper, x_upper &gt;= y_lower.\n\n\nparties &lt;- tibble(\n  q = 1:4,\n  party = ymd(c(\"2022-01-10\", \"2022-04-04\", \"2022-07-11\", \"2022-10-03\")),\n  start = ymd(c(\"2022-01-01\", \"2022-04-04\", \"2022-07-11\", \"2022-10-03\")),\n  end = ymd(c(\"2022-04-03\", \"2022-07-11\", \"2022-10-02\", \"2022-12-31\"))\n)\nparties\n\n# A tibble: 4 × 4\n      q party      start      end       \n  &lt;int&gt; &lt;date&gt;     &lt;date&gt;     &lt;date&gt;    \n1     1 2022-01-10 2022-01-01 2022-04-03\n2     2 2022-04-04 2022-04-04 2022-07-11\n3     3 2022-07-11 2022-07-11 2022-10-02\n4     4 2022-10-03 2022-10-03 2022-12-31\n\n#&gt; # A tibble: 4 × 4\n#&gt;       q party      start      end       \n#&gt;   &lt;int&gt; &lt;date&gt;     &lt;date&gt;     &lt;date&gt;    \n#&gt; 1     1 2022-01-10 2022-01-01 2022-04-03\n#&gt; 2     2 2022-04-04 2022-04-04 2022-07-11\n#&gt; 3     3 2022-07-11 2022-07-11 2022-10-02\n#&gt; 4     4 2022-10-03 2022-10-03 2022-12-31\n\nUse a self-join to check the data entry quality:\n\nparties |&gt; \n  inner_join(parties, join_by(overlaps(start, end, start, end), q &lt; q)) |&gt; \n  select(start.x, end.x, start.y, end.y)\n\n# A tibble: 1 × 4\n  start.x    end.x      start.y    end.y     \n  &lt;date&gt;     &lt;date&gt;     &lt;date&gt;     &lt;date&gt;    \n1 2022-04-04 2022-07-11 2022-07-11 2022-10-02\n\n#&gt; # A tibble: 1 × 4\n#&gt;   start.x    end.x      start.y    end.y     \n#&gt;   &lt;date&gt;     &lt;date&gt;     &lt;date&gt;     &lt;date&gt;    \n#&gt; 1 2022-04-04 2022-07-11 2022-07-11 2022-10-02\n\nThere is an overlap. Fix:\n\nparties &lt;- tibble(\n  q = 1:4,\n  party = ymd(c(\"2022-01-10\", \"2022-04-04\", \"2022-07-11\", \"2022-10-03\")),\n  start = ymd(c(\"2022-01-01\", \"2022-04-04\", \"2022-07-11\", \"2022-10-03\")),\n  end = ymd(c(\"2022-04-03\", \"2022-07-10\", \"2022-10-02\", \"2022-12-31\"))\n)\n\nNow assign a party\n\nemployees |&gt; \n  inner_join(parties, join_by(between(birthday, start, end)), unmatched = \"error\")\n\n# A tibble: 100 × 6\n   name     birthday       q party      start      end       \n   &lt;chr&gt;    &lt;date&gt;     &lt;int&gt; &lt;date&gt;     &lt;date&gt;     &lt;date&gt;    \n 1 Kemba    2022-01-22     1 2022-01-10 2022-01-01 2022-04-03\n 2 Orean    2022-06-26     2 2022-04-04 2022-04-04 2022-07-10\n 3 Kirstyn  2022-02-11     1 2022-01-10 2022-01-01 2022-04-03\n 4 Amparo   2022-11-11     4 2022-10-03 2022-10-03 2022-12-31\n 5 Belen    2022-03-25     1 2022-01-10 2022-01-01 2022-04-03\n 6 Rayshaun 2022-01-11     1 2022-01-10 2022-01-01 2022-04-03\n 7 Brazil   2022-05-01     2 2022-04-04 2022-04-04 2022-07-10\n 8 Chaston  2022-10-29     4 2022-10-03 2022-10-03 2022-12-31\n 9 Reyn     2022-03-26     1 2022-01-10 2022-01-01 2022-04-03\n10 Ogechi   2022-12-31     4 2022-10-03 2022-10-03 2022-12-31\n# ℹ 90 more rows\n\n#&gt; # A tibble: 100 × 6\n#&gt;   name     birthday       q party      start      end       \n#&gt;   &lt;chr&gt;    &lt;date&gt;     &lt;int&gt; &lt;date&gt;     &lt;date&gt;     &lt;date&gt;    \n#&gt; 1 Kemba    2022-01-22     1 2022-01-10 2022-01-01 2022-04-03\n#&gt; 2 Orean    2022-06-26     2 2022-04-04 2022-04-04 2022-07-10\n#&gt; 3 Kirstyn  2022-02-11     1 2022-01-10 2022-01-01 2022-04-03\n#&gt; 4 Amparo   2022-11-11     4 2022-10-03 2022-10-03 2022-12-31\n#&gt; 5 Belen    2022-03-25     1 2022-01-10 2022-01-01 2022-04-03\n#&gt; 6 Rayshaun 2022-01-11     1 2022-01-10 2022-01-01 2022-04-03\n#&gt; # ℹ 94 more rows"
  },
  {
    "objectID": "chapter_19_notes.html#keys",
    "href": "chapter_19_notes.html#keys",
    "title": "R for Data Science, 2nd Edition - Chapter 19 Notes",
    "section": "",
    "text": "Primary and foreign keys. Primary keys uniquely identify an observation. A foreign key a variable or set of variables that corresponds to a primary key in another table.\ncheck that your primary key is unique:\n\nplanes |&gt; \n  count(tailnum) |&gt; \n  filter(n &gt; 1)\n\n# A tibble: 0 × 2\n# ℹ 2 variables: tailnum &lt;chr&gt;, n &lt;int&gt;\n\n#&gt; # A tibble: 0 × 2\n#&gt; # ℹ 2 variables: tailnum &lt;chr&gt;, n &lt;int&gt;\n\nweather |&gt; \n  count(time_hour, origin) |&gt; \n  filter(n &gt; 1)\n\n# A tibble: 0 × 3\n# ℹ 3 variables: time_hour &lt;dttm&gt;, origin &lt;chr&gt;, n &lt;int&gt;\n\n#&gt; # A tibble: 0 × 3\n#&gt; # ℹ 3 variables: time_hour &lt;dttm&gt;, origin &lt;chr&gt;, n &lt;int&gt;\n\nShould probably also check for missing values on primary keys:\n\nplanes |&gt; \n  filter(is.na(tailnum))\n\n# A tibble: 0 × 9\n# ℹ 9 variables: tailnum &lt;chr&gt;, year &lt;int&gt;, type &lt;chr&gt;, manufacturer &lt;chr&gt;,\n#   model &lt;chr&gt;, engines &lt;int&gt;, seats &lt;int&gt;, speed &lt;int&gt;, engine &lt;chr&gt;\n\n#&gt; # A tibble: 0 × 9\n#&gt; # ℹ 9 variables: tailnum &lt;chr&gt;, year &lt;int&gt;, type &lt;chr&gt;, manufacturer &lt;chr&gt;,\n#&gt; #   model &lt;chr&gt;, engines &lt;int&gt;, seats &lt;int&gt;, speed &lt;int&gt;, engine &lt;chr&gt;\n\nweather |&gt; \n  filter(is.na(time_hour) | is.na(origin))\n\n# A tibble: 0 × 15\n# ℹ 15 variables: origin &lt;chr&gt;, year &lt;int&gt;, month &lt;int&gt;, day &lt;int&gt;, hour &lt;int&gt;,\n#   temp &lt;dbl&gt;, dewp &lt;dbl&gt;, humid &lt;dbl&gt;, wind_dir &lt;dbl&gt;, wind_speed &lt;dbl&gt;,\n#   wind_gust &lt;dbl&gt;, precip &lt;dbl&gt;, pressure &lt;dbl&gt;, visib &lt;dbl&gt;,\n#   time_hour &lt;dttm&gt;\n\n#&gt; # A tibble: 0 × 15\n#&gt; # ℹ 15 variables: origin &lt;chr&gt;, year &lt;int&gt;, month &lt;int&gt;, day &lt;int&gt;,\n#&gt; #   hour &lt;int&gt;, temp &lt;dbl&gt;, dewp &lt;dbl&gt;, humid &lt;dbl&gt;, wind_dir &lt;dbl&gt;, …\n\nfor the flights data frame, a compound primary key can be formed like so:\n\nflights |&gt; \n  count(time_hour, carrier, flight) |&gt; \n  filter(n &gt; 1)\n\n# A tibble: 0 × 4\n# ℹ 4 variables: time_hour &lt;dttm&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, n &lt;int&gt;\n\n#&gt; # A tibble: 0 × 4\n#&gt; # ℹ 4 variables: time_hour &lt;dttm&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, n &lt;int&gt;\n\nIf you don’t have a clear primary key, you can use a surrogate:\n\nflights2 &lt;- flights |&gt; \n  mutate(id = row_number(), .before = 1)\nflights2\n\n# A tibble: 336,776 × 20\n      id  year month   day dep_time sched_dep_time dep_delay arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;\n 1     1  2013     1     1      517            515         2      830\n 2     2  2013     1     1      533            529         4      850\n 3     3  2013     1     1      542            540         2      923\n 4     4  2013     1     1      544            545        -1     1004\n 5     5  2013     1     1      554            600        -6      812\n 6     6  2013     1     1      554            558        -4      740\n 7     7  2013     1     1      555            600        -5      913\n 8     8  2013     1     1      557            600        -3      709\n 9     9  2013     1     1      557            600        -3      838\n10    10  2013     1     1      558            600        -2      753\n# ℹ 336,766 more rows\n# ℹ 12 more variables: sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;,\n#   flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;,\n#   distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n#&gt; # A tibble: 336,776 × 20\n#&gt;      id  year month   day dep_time sched_dep_time dep_delay arr_time\n#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;\n#&gt; 1     1  2013     1     1      517            515         2      830\n#&gt; 2     2  2013     1     1      533            529         4      850\n#&gt; 3     3  2013     1     1      542            540         2      923\n#&gt; 4     4  2013     1     1      544            545        -1     1004\n#&gt; 5     5  2013     1     1      554            600        -6      812\n#&gt; 6     6  2013     1     1      554            558        -4      740\n#&gt; # ℹ 336,770 more rows\n#&gt; # ℹ 12 more variables: sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, …\n\nMutating joins.\nMake a more narrow data frame first.\n\nflights2 &lt;- flights |&gt; \n  select(year, time_hour, origin, dest, tailnum, carrier)\nflights2\n\n# A tibble: 336,776 × 6\n    year time_hour           origin dest  tailnum carrier\n   &lt;int&gt; &lt;dttm&gt;              &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;  \n 1  2013 2013-01-01 05:00:00 EWR    IAH   N14228  UA     \n 2  2013 2013-01-01 05:00:00 LGA    IAH   N24211  UA     \n 3  2013 2013-01-01 05:00:00 JFK    MIA   N619AA  AA     \n 4  2013 2013-01-01 05:00:00 JFK    BQN   N804JB  B6     \n 5  2013 2013-01-01 06:00:00 LGA    ATL   N668DN  DL     \n 6  2013 2013-01-01 05:00:00 EWR    ORD   N39463  UA     \n 7  2013 2013-01-01 06:00:00 EWR    FLL   N516JB  B6     \n 8  2013 2013-01-01 06:00:00 LGA    IAD   N829AS  EV     \n 9  2013 2013-01-01 06:00:00 JFK    MCO   N593JB  B6     \n10  2013 2013-01-01 06:00:00 LGA    ORD   N3ALAA  AA     \n# ℹ 336,766 more rows\n\n#&gt; # A tibble: 336,776 × 6\n#&gt;    year time_hour           origin dest  tailnum carrier\n#&gt;   &lt;int&gt; &lt;dttm&gt;              &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;  \n#&gt; 1  2013 2013-01-01 05:00:00 EWR    IAH   N14228  UA     \n#&gt; 2  2013 2013-01-01 05:00:00 LGA    IAH   N24211  UA     \n#&gt; 3  2013 2013-01-01 05:00:00 JFK    MIA   N619AA  AA     \n#&gt; 4  2013 2013-01-01 05:00:00 JFK    BQN   N804JB  B6     \n#&gt; 5  2013 2013-01-01 06:00:00 LGA    ATL   N668DN  DL     \n#&gt; 6  2013 2013-01-01 05:00:00 EWR    ORD   N39463  UA     \n#&gt; # ℹ 336,770 more rows\n\nadd airline information to flights\n\nflights2 |&gt;\n  left_join(airlines)\n\nJoining with `by = join_by(carrier)`\n\n\n# A tibble: 336,776 × 7\n    year time_hour           origin dest  tailnum carrier name                  \n   &lt;int&gt; &lt;dttm&gt;              &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;                 \n 1  2013 2013-01-01 05:00:00 EWR    IAH   N14228  UA      United Air Lines Inc. \n 2  2013 2013-01-01 05:00:00 LGA    IAH   N24211  UA      United Air Lines Inc. \n 3  2013 2013-01-01 05:00:00 JFK    MIA   N619AA  AA      American Airlines Inc.\n 4  2013 2013-01-01 05:00:00 JFK    BQN   N804JB  B6      JetBlue Airways       \n 5  2013 2013-01-01 06:00:00 LGA    ATL   N668DN  DL      Delta Air Lines Inc.  \n 6  2013 2013-01-01 05:00:00 EWR    ORD   N39463  UA      United Air Lines Inc. \n 7  2013 2013-01-01 06:00:00 EWR    FLL   N516JB  B6      JetBlue Airways       \n 8  2013 2013-01-01 06:00:00 LGA    IAD   N829AS  EV      ExpressJet Airlines I…\n 9  2013 2013-01-01 06:00:00 JFK    MCO   N593JB  B6      JetBlue Airways       \n10  2013 2013-01-01 06:00:00 LGA    ORD   N3ALAA  AA      American Airlines Inc.\n# ℹ 336,766 more rows\n\n#&gt; Joining with `by = join_by(carrier)`\n#&gt; # A tibble: 336,776 × 7\n#&gt;    year time_hour           origin dest  tailnum carrier name                \n#&gt;   &lt;int&gt; &lt;dttm&gt;              &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;               \n#&gt; 1  2013 2013-01-01 05:00:00 EWR    IAH   N14228  UA      United Air Lines In…\n#&gt; 2  2013 2013-01-01 05:00:00 LGA    IAH   N24211  UA      United Air Lines In…\n#&gt; 3  2013 2013-01-01 05:00:00 JFK    MIA   N619AA  AA      American Airlines I…\n#&gt; 4  2013 2013-01-01 05:00:00 JFK    BQN   N804JB  B6      JetBlue Airways     \n#&gt; 5  2013 2013-01-01 06:00:00 LGA    ATL   N668DN  DL      Delta Air Lines Inc.\n#&gt; 6  2013 2013-01-01 05:00:00 EWR    ORD   N39463  UA      United Air Lines In…\n#&gt; # ℹ 336,770 more rows\n\nFind weather when each flight departed. Note the use of select within left_join():\n\nflights2 |&gt; \n  left_join(weather |&gt; select(origin, time_hour, temp, wind_speed))\n\nJoining with `by = join_by(time_hour, origin)`\n\n\n# A tibble: 336,776 × 8\n    year time_hour           origin dest  tailnum carrier  temp wind_speed\n   &lt;int&gt; &lt;dttm&gt;              &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n 1  2013 2013-01-01 05:00:00 EWR    IAH   N14228  UA       39.0       12.7\n 2  2013 2013-01-01 05:00:00 LGA    IAH   N24211  UA       39.9       15.0\n 3  2013 2013-01-01 05:00:00 JFK    MIA   N619AA  AA       39.0       15.0\n 4  2013 2013-01-01 05:00:00 JFK    BQN   N804JB  B6       39.0       15.0\n 5  2013 2013-01-01 06:00:00 LGA    ATL   N668DN  DL       39.9       16.1\n 6  2013 2013-01-01 05:00:00 EWR    ORD   N39463  UA       39.0       12.7\n 7  2013 2013-01-01 06:00:00 EWR    FLL   N516JB  B6       37.9       11.5\n 8  2013 2013-01-01 06:00:00 LGA    IAD   N829AS  EV       39.9       16.1\n 9  2013 2013-01-01 06:00:00 JFK    MCO   N593JB  B6       37.9       13.8\n10  2013 2013-01-01 06:00:00 LGA    ORD   N3ALAA  AA       39.9       16.1\n# ℹ 336,766 more rows\n\n#&gt; Joining with `by = join_by(time_hour, origin)`\n#&gt; # A tibble: 336,776 × 8\n#&gt;    year time_hour           origin dest  tailnum carrier  temp wind_speed\n#&gt;   &lt;int&gt; &lt;dttm&gt;              &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n#&gt; 1  2013 2013-01-01 05:00:00 EWR    IAH   N14228  UA       39.0       12.7\n#&gt; 2  2013 2013-01-01 05:00:00 LGA    IAH   N24211  UA       39.9       15.0\n#&gt; 3  2013 2013-01-01 05:00:00 JFK    MIA   N619AA  AA       39.0       15.0\n#&gt; 4  2013 2013-01-01 05:00:00 JFK    BQN   N804JB  B6       39.0       15.0\n#&gt; 5  2013 2013-01-01 06:00:00 LGA    ATL   N668DN  DL       39.9       16.1\n#&gt; 6  2013 2013-01-01 05:00:00 EWR    ORD   N39463  UA       39.0       12.7\n#&gt; # ℹ 336,770 more rows\n\nWhen no match is found for a row in x, the new variables get NA in the new columns.\n\nflights2 |&gt; \n  filter(tailnum == \"N3ALAA\") |&gt; \n  left_join(planes |&gt; select(tailnum, type, engines, seats))\n\nJoining with `by = join_by(tailnum)`\n\n\n# A tibble: 63 × 9\n    year time_hour           origin dest  tailnum carrier type  engines seats\n   &lt;int&gt; &lt;dttm&gt;              &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;int&gt; &lt;int&gt;\n 1  2013 2013-01-01 06:00:00 LGA    ORD   N3ALAA  AA      &lt;NA&gt;       NA    NA\n 2  2013 2013-01-02 18:00:00 LGA    ORD   N3ALAA  AA      &lt;NA&gt;       NA    NA\n 3  2013 2013-01-03 06:00:00 LGA    ORD   N3ALAA  AA      &lt;NA&gt;       NA    NA\n 4  2013 2013-01-07 19:00:00 LGA    ORD   N3ALAA  AA      &lt;NA&gt;       NA    NA\n 5  2013 2013-01-08 17:00:00 JFK    ORD   N3ALAA  AA      &lt;NA&gt;       NA    NA\n 6  2013 2013-01-16 06:00:00 LGA    ORD   N3ALAA  AA      &lt;NA&gt;       NA    NA\n 7  2013 2013-01-20 18:00:00 LGA    ORD   N3ALAA  AA      &lt;NA&gt;       NA    NA\n 8  2013 2013-01-22 17:00:00 JFK    ORD   N3ALAA  AA      &lt;NA&gt;       NA    NA\n 9  2013 2013-10-11 06:00:00 EWR    MIA   N3ALAA  AA      &lt;NA&gt;       NA    NA\n10  2013 2013-10-14 08:00:00 JFK    BOS   N3ALAA  AA      &lt;NA&gt;       NA    NA\n# ℹ 53 more rows\n\n#&gt; Joining with `by = join_by(tailnum)`\n#&gt; # A tibble: 63 × 9\n#&gt;    year time_hour           origin dest  tailnum carrier type  engines seats\n#&gt;   &lt;int&gt; &lt;dttm&gt;              &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;int&gt; &lt;int&gt;\n#&gt; 1  2013 2013-01-01 06:00:00 LGA    ORD   N3ALAA  AA      &lt;NA&gt;       NA    NA\n#&gt; 2  2013 2013-01-02 18:00:00 LGA    ORD   N3ALAA  AA      &lt;NA&gt;       NA    NA\n#&gt; 3  2013 2013-01-03 06:00:00 LGA    ORD   N3ALAA  AA      &lt;NA&gt;       NA    NA\n#&gt; 4  2013 2013-01-07 19:00:00 LGA    ORD   N3ALAA  AA      &lt;NA&gt;       NA    NA\n#&gt; 5  2013 2013-01-08 17:00:00 JFK    ORD   N3ALAA  AA      &lt;NA&gt;       NA    NA\n#&gt; 6  2013 2013-01-16 06:00:00 LGA    ORD   N3ALAA  AA      &lt;NA&gt;       NA    NA\n#&gt; # ℹ 57 more rows\n\nBy default, left_join() uses all columns that appear in both data frames as the join key. But you can and should specify the joining keys:\n\nflights2 |&gt; \n  left_join(planes, join_by(tailnum))\n\n# A tibble: 336,776 × 14\n   year.x time_hour           origin dest  tailnum carrier year.y type          \n    &lt;int&gt; &lt;dttm&gt;              &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;    &lt;int&gt; &lt;chr&gt;         \n 1   2013 2013-01-01 05:00:00 EWR    IAH   N14228  UA        1999 Fixed wing mu…\n 2   2013 2013-01-01 05:00:00 LGA    IAH   N24211  UA        1998 Fixed wing mu…\n 3   2013 2013-01-01 05:00:00 JFK    MIA   N619AA  AA        1990 Fixed wing mu…\n 4   2013 2013-01-01 05:00:00 JFK    BQN   N804JB  B6        2012 Fixed wing mu…\n 5   2013 2013-01-01 06:00:00 LGA    ATL   N668DN  DL        1991 Fixed wing mu…\n 6   2013 2013-01-01 05:00:00 EWR    ORD   N39463  UA        2012 Fixed wing mu…\n 7   2013 2013-01-01 06:00:00 EWR    FLL   N516JB  B6        2000 Fixed wing mu…\n 8   2013 2013-01-01 06:00:00 LGA    IAD   N829AS  EV        1998 Fixed wing mu…\n 9   2013 2013-01-01 06:00:00 JFK    MCO   N593JB  B6        2004 Fixed wing mu…\n10   2013 2013-01-01 06:00:00 LGA    ORD   N3ALAA  AA          NA &lt;NA&gt;          \n# ℹ 336,766 more rows\n# ℹ 6 more variables: manufacturer &lt;chr&gt;, model &lt;chr&gt;, engines &lt;int&gt;,\n#   seats &lt;int&gt;, speed &lt;int&gt;, engine &lt;chr&gt;\n\n#&gt; # A tibble: 336,776 × 14\n#&gt;   year.x time_hour           origin dest  tailnum carrier year.y\n#&gt;    &lt;int&gt; &lt;dttm&gt;              &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;    &lt;int&gt;\n#&gt; 1   2013 2013-01-01 05:00:00 EWR    IAH   N14228  UA        1999\n#&gt; 2   2013 2013-01-01 05:00:00 LGA    IAH   N24211  UA        1998\n#&gt; 3   2013 2013-01-01 05:00:00 JFK    MIA   N619AA  AA        1990\n#&gt; 4   2013 2013-01-01 05:00:00 JFK    BQN   N804JB  B6        2012\n#&gt; 5   2013 2013-01-01 06:00:00 LGA    ATL   N668DN  DL        1991\n#&gt; 6   2013 2013-01-01 05:00:00 EWR    ORD   N39463  UA        2012\n#&gt; # ℹ 336,770 more rows\n#&gt; # ℹ 7 more variables: type &lt;chr&gt;, manufacturer &lt;chr&gt;, model &lt;chr&gt;, …\n\nThis is short for join_by(tailnum == tailnum). That’s fine is the same variables have the same names across data frames. If the names are different, then you need to specify the equality of variables across different columns names in the join_by() argument.\n\nflights2 |&gt; \n  left_join(airports, join_by(dest == faa))\n\n# A tibble: 336,776 × 13\n    year time_hour           origin dest  tailnum carrier name         lat   lon\n   &lt;int&gt; &lt;dttm&gt;              &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;\n 1  2013 2013-01-01 05:00:00 EWR    IAH   N14228  UA      George Bu…  30.0 -95.3\n 2  2013 2013-01-01 05:00:00 LGA    IAH   N24211  UA      George Bu…  30.0 -95.3\n 3  2013 2013-01-01 05:00:00 JFK    MIA   N619AA  AA      Miami Intl  25.8 -80.3\n 4  2013 2013-01-01 05:00:00 JFK    BQN   N804JB  B6      &lt;NA&gt;        NA    NA  \n 5  2013 2013-01-01 06:00:00 LGA    ATL   N668DN  DL      Hartsfiel…  33.6 -84.4\n 6  2013 2013-01-01 05:00:00 EWR    ORD   N39463  UA      Chicago O…  42.0 -87.9\n 7  2013 2013-01-01 06:00:00 EWR    FLL   N516JB  B6      Fort Laud…  26.1 -80.2\n 8  2013 2013-01-01 06:00:00 LGA    IAD   N829AS  EV      Washingto…  38.9 -77.5\n 9  2013 2013-01-01 06:00:00 JFK    MCO   N593JB  B6      Orlando I…  28.4 -81.3\n10  2013 2013-01-01 06:00:00 LGA    ORD   N3ALAA  AA      Chicago O…  42.0 -87.9\n# ℹ 336,766 more rows\n# ℹ 4 more variables: alt &lt;dbl&gt;, tz &lt;dbl&gt;, dst &lt;chr&gt;, tzone &lt;chr&gt;\n\n#&gt; # A tibble: 336,776 × 13\n#&gt;    year time_hour           origin dest  tailnum carrier name                \n#&gt;   &lt;int&gt; &lt;dttm&gt;              &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;               \n#&gt; 1  2013 2013-01-01 05:00:00 EWR    IAH   N14228  UA      George Bush Interco…\n#&gt; 2  2013 2013-01-01 05:00:00 LGA    IAH   N24211  UA      George Bush Interco…\n#&gt; 3  2013 2013-01-01 05:00:00 JFK    MIA   N619AA  AA      Miami Intl          \n#&gt; 4  2013 2013-01-01 05:00:00 JFK    BQN   N804JB  B6      &lt;NA&gt;                \n#&gt; 5  2013 2013-01-01 06:00:00 LGA    ATL   N668DN  DL      Hartsfield Jackson …\n#&gt; 6  2013 2013-01-01 05:00:00 EWR    ORD   N39463  UA      Chicago Ohare Intl  \n#&gt; # ℹ 336,770 more rows\n#&gt; # ℹ 6 more variables: lat &lt;dbl&gt;, lon &lt;dbl&gt;, alt &lt;dbl&gt;, tz &lt;dbl&gt;, …\n\nflights2 |&gt; \n  left_join(airports, join_by(origin == faa))\n\n# A tibble: 336,776 × 13\n    year time_hour           origin dest  tailnum carrier name         lat   lon\n   &lt;int&gt; &lt;dttm&gt;              &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;\n 1  2013 2013-01-01 05:00:00 EWR    IAH   N14228  UA      Newark Li…  40.7 -74.2\n 2  2013 2013-01-01 05:00:00 LGA    IAH   N24211  UA      La Guardia  40.8 -73.9\n 3  2013 2013-01-01 05:00:00 JFK    MIA   N619AA  AA      John F Ke…  40.6 -73.8\n 4  2013 2013-01-01 05:00:00 JFK    BQN   N804JB  B6      John F Ke…  40.6 -73.8\n 5  2013 2013-01-01 06:00:00 LGA    ATL   N668DN  DL      La Guardia  40.8 -73.9\n 6  2013 2013-01-01 05:00:00 EWR    ORD   N39463  UA      Newark Li…  40.7 -74.2\n 7  2013 2013-01-01 06:00:00 EWR    FLL   N516JB  B6      Newark Li…  40.7 -74.2\n 8  2013 2013-01-01 06:00:00 LGA    IAD   N829AS  EV      La Guardia  40.8 -73.9\n 9  2013 2013-01-01 06:00:00 JFK    MCO   N593JB  B6      John F Ke…  40.6 -73.8\n10  2013 2013-01-01 06:00:00 LGA    ORD   N3ALAA  AA      La Guardia  40.8 -73.9\n# ℹ 336,766 more rows\n# ℹ 4 more variables: alt &lt;dbl&gt;, tz &lt;dbl&gt;, dst &lt;chr&gt;, tzone &lt;chr&gt;\n\n#&gt; # A tibble: 336,776 × 13\n#&gt;    year time_hour           origin dest  tailnum carrier name               \n#&gt;   &lt;int&gt; &lt;dttm&gt;              &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;              \n#&gt; 1  2013 2013-01-01 05:00:00 EWR    IAH   N14228  UA      Newark Liberty Intl\n#&gt; 2  2013 2013-01-01 05:00:00 LGA    IAH   N24211  UA      La Guardia         \n#&gt; 3  2013 2013-01-01 05:00:00 JFK    MIA   N619AA  AA      John F Kennedy Intl\n#&gt; 4  2013 2013-01-01 05:00:00 JFK    BQN   N804JB  B6      John F Kennedy Intl\n#&gt; 5  2013 2013-01-01 06:00:00 LGA    ATL   N668DN  DL      La Guardia         \n#&gt; 6  2013 2013-01-01 05:00:00 EWR    ORD   N39463  UA      Newark Liberty Intl\n#&gt; # ℹ 336,770 more rows\n#&gt; # ℹ 6 more variables: lat &lt;dbl&gt;, lon &lt;dbl&gt;, alt &lt;dbl&gt;, tz &lt;dbl&gt;, …\n\nOlder code may use by with a character vector to specify join keys. Prefer join_by().\nFiltering joins. There are two types: semi-joins and anti-joins.\nSemi-joins keep all rows in x that have a match in y. Show just origin airports:\n\nairports |&gt; \n  semi_join(flights2, join_by(faa == origin))\n\n# A tibble: 3 × 8\n  faa   name                  lat   lon   alt    tz dst   tzone           \n  &lt;chr&gt; &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;           \n1 EWR   Newark Liberty Intl  40.7 -74.2    18    -5 A     America/New_York\n2 JFK   John F Kennedy Intl  40.6 -73.8    13    -5 A     America/New_York\n3 LGA   La Guardia           40.8 -73.9    22    -5 A     America/New_York\n\n#&gt; # A tibble: 3 × 8\n#&gt;   faa   name                  lat   lon   alt    tz dst   tzone           \n#&gt;   &lt;chr&gt; &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;           \n#&gt; 1 EWR   Newark Liberty Intl  40.7 -74.2    18    -5 A     America/New_York\n#&gt; 2 JFK   John F Kennedy Intl  40.6 -73.8    13    -5 A     America/New_York\n#&gt; 3 LGA   La Guardia           40.8 -73.9    22    -5 A     America/New_York\n\nAnti-joins are the opposite: they return all rows in x that don’t have a match in y. These are useful for identifying implicit missing values.\n\nflights2 |&gt; \n  anti_join(airports, join_by(dest == faa)) |&gt; \n  distinct(dest)\n\n# A tibble: 4 × 1\n  dest \n  &lt;chr&gt;\n1 BQN  \n2 SJU  \n3 STT  \n4 PSE  \n\n#&gt; # A tibble: 4 × 1\n#&gt;   dest \n#&gt;   &lt;chr&gt;\n#&gt; 1 BQN  \n#&gt; 2 SJU  \n#&gt; 3 STT  \n#&gt; 4 PSE\n\nFind which tailnum() are missing from planes:\n\nflights2 |&gt;\n  anti_join(planes, join_by(tailnum)) |&gt; \n  distinct(tailnum)\n\n# A tibble: 722 × 1\n   tailnum\n   &lt;chr&gt;  \n 1 N3ALAA \n 2 N3DUAA \n 3 N542MQ \n 4 N730MQ \n 5 N9EAMQ \n 6 N532UA \n 7 N3EMAA \n 8 N518MQ \n 9 N3BAAA \n10 N3CYAA \n# ℹ 712 more rows\n\n#&gt; # A tibble: 722 × 1\n#&gt;   tailnum\n#&gt;   &lt;chr&gt;  \n#&gt; 1 N3ALAA \n#&gt; 2 N3DUAA \n#&gt; 3 N542MQ \n#&gt; 4 N730MQ \n#&gt; 5 N9EAMQ \n#&gt; 6 N532UA \n#&gt; # ℹ 716 more rows\n\nExercises 19.3.4 look especially useful to try out."
  },
  {
    "objectID": "chapter_19_notes.html#non-equi-joins",
    "href": "chapter_19_notes.html#non-equi-joins",
    "title": "R for Data Science, 2nd Edition - Chapter 19 Notes",
    "section": "",
    "text": "These are joins without a strict requirement of equality of keys. This makes keeping both keys used in the join more useful, since they may be slightly different. We could have done this in the earlier equi joins above:\n\n# x |&gt; inner_join(y, join_by(key == key), keep = TRUE)\n# #&gt; # A tibble: 2 × 4\n# #&gt;   key.x val_x key.y val_y\n# #&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;\n# #&gt; 1     1 x1        1 y1   \n# #&gt; 2     2 x2        2 y2\n\nYou can, for example, do matches like x$key &gt; y$key.\n\nCross joins match every pair of rows.\nInequality joins use &lt;, &lt;=, &gt;, and &gt;= instead of ==.\nRolling joins are similar to inequality joins but only find the closest match.\nOverlap joins are a special type of inequality join designed to work with ranges.\n\nCross joins are the cartesian product of rows:\n\ndf &lt;- tibble(name = c(\"John\", \"Simon\", \"Tracy\", \"Max\"))\ndf |&gt; cross_join(df)\n\n# A tibble: 16 × 2\n   name.x name.y\n   &lt;chr&gt;  &lt;chr&gt; \n 1 John   John  \n 2 John   Simon \n 3 John   Tracy \n 4 John   Max   \n 5 Simon  John  \n 6 Simon  Simon \n 7 Simon  Tracy \n 8 Simon  Max   \n 9 Tracy  John  \n10 Tracy  Simon \n11 Tracy  Tracy \n12 Tracy  Max   \n13 Max    John  \n14 Max    Simon \n15 Max    Tracy \n16 Max    Max   \n\n#&gt; # A tibble: 16 × 2\n#&gt;   name.x name.y\n#&gt;   &lt;chr&gt;  &lt;chr&gt; \n#&gt; 1 John   John  \n#&gt; 2 John   Simon \n#&gt; 3 John   Tracy \n#&gt; 4 John   Max   \n#&gt; 5 Simon  John  \n#&gt; 6 Simon  Simon \n#&gt; # ℹ 10 more rows\n\nInequality joins can be used to generate all combinations, instead of all permutations:\n\ndf &lt;- tibble(id = 1:4, name = c(\"John\", \"Simon\", \"Tracy\", \"Max\"))\n\ndf |&gt; inner_join(df, join_by(id &lt; id))\n\n# A tibble: 6 × 4\n   id.x name.x  id.y name.y\n  &lt;int&gt; &lt;chr&gt;  &lt;int&gt; &lt;chr&gt; \n1     1 John       2 Simon \n2     1 John       3 Tracy \n3     1 John       4 Max   \n4     2 Simon      3 Tracy \n5     2 Simon      4 Max   \n6     3 Tracy      4 Max   \n\n#&gt; # A tibble: 6 × 4\n#&gt;    id.x name.x  id.y name.y\n#&gt;   &lt;int&gt; &lt;chr&gt;  &lt;int&gt; &lt;chr&gt; \n#&gt; 1     1 John       2 Simon \n#&gt; 2     1 John       3 Tracy \n#&gt; 3     1 John       4 Max   \n#&gt; 4     2 Simon      3 Tracy \n#&gt; 5     2 Simon      4 Max   \n#&gt; 6     3 Tracy      4 Max\n\nRolling joins are similar to inequality joins, but only give you the closest row that satisfies the inequality:\nRolling joins are particularly useful when you have two tables of dates that don’t perfectly line up and you want to find (e.g.) the closest date in table 1 that comes before (or after) some date in table 2.\nQuarterly party planning for employee birthdays:\n\nparties &lt;- tibble(\n  q = 1:4,\n  party = ymd(c(\"2022-01-10\", \"2022-04-04\", \"2022-07-11\", \"2022-10-03\"))\n)\n\n\nset.seed(123)\nemployees &lt;- tibble(\n  name = sample(babynames::babynames$name, 100),\n  birthday = ymd(\"2022-01-01\") + (sample(365, 100, replace = TRUE) - 1)\n)\nemployees\n\n# A tibble: 100 × 2\n   name     birthday  \n   &lt;chr&gt;    &lt;date&gt;    \n 1 Kemba    2022-01-22\n 2 Orean    2022-06-26\n 3 Kirstyn  2022-02-11\n 4 Amparo   2022-11-11\n 5 Belen    2022-03-25\n 6 Rayshaun 2022-01-11\n 7 Brazil   2022-05-01\n 8 Chaston  2022-10-29\n 9 Reyn     2022-03-26\n10 Ogechi   2022-12-31\n# ℹ 90 more rows\n\n#&gt; # A tibble: 100 × 2\n#&gt;   name     birthday  \n#&gt;   &lt;chr&gt;    &lt;date&gt;    \n#&gt; 1 Kemba    2022-01-22\n#&gt; 2 Orean    2022-06-26\n#&gt; 3 Kirstyn  2022-02-11\n#&gt; 4 Amparo   2022-11-11\n#&gt; 5 Belen    2022-03-25\n#&gt; 6 Rayshaun 2022-01-11\n#&gt; # ℹ 94 more rows\n\nFor each employee we want to find the last party date that comes before (or on) their birthday. We can express that with a rolling join. The closest() function takes this from an inequality join to a rolling join:\n\nemployees |&gt; \n  left_join(parties, join_by(closest(birthday &gt;= party)))\n\n# A tibble: 100 × 4\n   name     birthday       q party     \n   &lt;chr&gt;    &lt;date&gt;     &lt;int&gt; &lt;date&gt;    \n 1 Kemba    2022-01-22     1 2022-01-10\n 2 Orean    2022-06-26     2 2022-04-04\n 3 Kirstyn  2022-02-11     1 2022-01-10\n 4 Amparo   2022-11-11     4 2022-10-03\n 5 Belen    2022-03-25     1 2022-01-10\n 6 Rayshaun 2022-01-11     1 2022-01-10\n 7 Brazil   2022-05-01     2 2022-04-04\n 8 Chaston  2022-10-29     4 2022-10-03\n 9 Reyn     2022-03-26     1 2022-01-10\n10 Ogechi   2022-12-31     4 2022-10-03\n# ℹ 90 more rows\n\n#&gt; # A tibble: 100 × 4\n#&gt;   name     birthday       q party     \n#&gt;   &lt;chr&gt;    &lt;date&gt;     &lt;int&gt; &lt;date&gt;    \n#&gt; 1 Kemba    2022-01-22     1 2022-01-10\n#&gt; 2 Orean    2022-06-26     2 2022-04-04\n#&gt; 3 Kirstyn  2022-02-11     1 2022-01-10\n#&gt; 4 Amparo   2022-11-11     4 2022-10-03\n#&gt; 5 Belen    2022-03-25     1 2022-01-10\n#&gt; 6 Rayshaun 2022-01-11     1 2022-01-10\n#&gt; # ℹ 94 more rows\n\nThe problem with this is that folks with birthdays before January 10 don’t get a party.\nOverlap joins. There are three helpers:\n\nbetween(x, y_lower, y_upper) is short for x &gt;= y_lower, x &lt;= y_upper.\nwithin(x_lower, x_upper, y_lower, y_upper) is short for x_lower &gt;= y_lower, x_upper &lt;= y_upper.\noverlaps(x_lower, x_upper, y_lower, y_upper) is short for x_lower &lt;= y_upper, x_upper &gt;= y_lower.\n\n\nparties &lt;- tibble(\n  q = 1:4,\n  party = ymd(c(\"2022-01-10\", \"2022-04-04\", \"2022-07-11\", \"2022-10-03\")),\n  start = ymd(c(\"2022-01-01\", \"2022-04-04\", \"2022-07-11\", \"2022-10-03\")),\n  end = ymd(c(\"2022-04-03\", \"2022-07-11\", \"2022-10-02\", \"2022-12-31\"))\n)\nparties\n\n# A tibble: 4 × 4\n      q party      start      end       \n  &lt;int&gt; &lt;date&gt;     &lt;date&gt;     &lt;date&gt;    \n1     1 2022-01-10 2022-01-01 2022-04-03\n2     2 2022-04-04 2022-04-04 2022-07-11\n3     3 2022-07-11 2022-07-11 2022-10-02\n4     4 2022-10-03 2022-10-03 2022-12-31\n\n#&gt; # A tibble: 4 × 4\n#&gt;       q party      start      end       \n#&gt;   &lt;int&gt; &lt;date&gt;     &lt;date&gt;     &lt;date&gt;    \n#&gt; 1     1 2022-01-10 2022-01-01 2022-04-03\n#&gt; 2     2 2022-04-04 2022-04-04 2022-07-11\n#&gt; 3     3 2022-07-11 2022-07-11 2022-10-02\n#&gt; 4     4 2022-10-03 2022-10-03 2022-12-31\n\nUse a self-join to check the data entry quality:\n\nparties |&gt; \n  inner_join(parties, join_by(overlaps(start, end, start, end), q &lt; q)) |&gt; \n  select(start.x, end.x, start.y, end.y)\n\n# A tibble: 1 × 4\n  start.x    end.x      start.y    end.y     \n  &lt;date&gt;     &lt;date&gt;     &lt;date&gt;     &lt;date&gt;    \n1 2022-04-04 2022-07-11 2022-07-11 2022-10-02\n\n#&gt; # A tibble: 1 × 4\n#&gt;   start.x    end.x      start.y    end.y     \n#&gt;   &lt;date&gt;     &lt;date&gt;     &lt;date&gt;     &lt;date&gt;    \n#&gt; 1 2022-04-04 2022-07-11 2022-07-11 2022-10-02\n\nThere is an overlap. Fix:\n\nparties &lt;- tibble(\n  q = 1:4,\n  party = ymd(c(\"2022-01-10\", \"2022-04-04\", \"2022-07-11\", \"2022-10-03\")),\n  start = ymd(c(\"2022-01-01\", \"2022-04-04\", \"2022-07-11\", \"2022-10-03\")),\n  end = ymd(c(\"2022-04-03\", \"2022-07-10\", \"2022-10-02\", \"2022-12-31\"))\n)\n\nNow assign a party\n\nemployees |&gt; \n  inner_join(parties, join_by(between(birthday, start, end)), unmatched = \"error\")\n\n# A tibble: 100 × 6\n   name     birthday       q party      start      end       \n   &lt;chr&gt;    &lt;date&gt;     &lt;int&gt; &lt;date&gt;     &lt;date&gt;     &lt;date&gt;    \n 1 Kemba    2022-01-22     1 2022-01-10 2022-01-01 2022-04-03\n 2 Orean    2022-06-26     2 2022-04-04 2022-04-04 2022-07-10\n 3 Kirstyn  2022-02-11     1 2022-01-10 2022-01-01 2022-04-03\n 4 Amparo   2022-11-11     4 2022-10-03 2022-10-03 2022-12-31\n 5 Belen    2022-03-25     1 2022-01-10 2022-01-01 2022-04-03\n 6 Rayshaun 2022-01-11     1 2022-01-10 2022-01-01 2022-04-03\n 7 Brazil   2022-05-01     2 2022-04-04 2022-04-04 2022-07-10\n 8 Chaston  2022-10-29     4 2022-10-03 2022-10-03 2022-12-31\n 9 Reyn     2022-03-26     1 2022-01-10 2022-01-01 2022-04-03\n10 Ogechi   2022-12-31     4 2022-10-03 2022-10-03 2022-12-31\n# ℹ 90 more rows\n\n#&gt; # A tibble: 100 × 6\n#&gt;   name     birthday       q party      start      end       \n#&gt;   &lt;chr&gt;    &lt;date&gt;     &lt;int&gt; &lt;date&gt;     &lt;date&gt;     &lt;date&gt;    \n#&gt; 1 Kemba    2022-01-22     1 2022-01-10 2022-01-01 2022-04-03\n#&gt; 2 Orean    2022-06-26     2 2022-04-04 2022-04-04 2022-07-10\n#&gt; 3 Kirstyn  2022-02-11     1 2022-01-10 2022-01-01 2022-04-03\n#&gt; 4 Amparo   2022-11-11     4 2022-10-03 2022-10-03 2022-12-31\n#&gt; 5 Belen    2022-03-25     1 2022-01-10 2022-01-01 2022-04-03\n#&gt; 6 Rayshaun 2022-01-11     1 2022-01-10 2022-01-01 2022-04-03\n#&gt; # ℹ 94 more rows"
  }
]